# 데이터 수집
## 웹 크롤링의 구조와 이해
### 웹 크롤링
Web(거미줄) + Crawling(기어다니다)

-> 인터넷 상의 웹 페이지를 자동으로 탐색하고 데이터를 수집하는 기술

- 웹 크롤링의 특징
  - 자동화
  - 대량 데이터 수집
  - 구조화된 데이터 추출

- **스크래핑**
  - 특정 웹페이지에서 원하는 데이터(텍스트, 이미지, 표 등)를 추출하는 과정

- 웹 데이터 수집 방법
  - 웹 페이지를 가져와서 필요한 정보만 추출하기 (스크래핑)
  - WebDriver를 이용해 웹 브라우저 자동화하기 (스크래핑 or 크롤링)
    - 웹 브라우저를 조작해서 동적인 데이터 가져올 때(ex. 로그인 버튼이 있다던지 등)
  - 제공된 OpenAPI를 이용해 실시간으로 데이터 가져오기

### 웹 페이지에서 데이터 추출
- 웹 페이지는 HTML(hyper text markup language)을 중심으로 이루어져 있음
  - 원하는 주소의 웹 페이지로 들어가 HTML 내용을 가져오고, 그 안에서 원하는 데이터가 어디 있는지 가져오는 직업 -> `parsing`
  - Python에는 `BeautifulSoup` 라이브러리 주로 사용

    ![alt text](image.png)

- 공개된 Open API 사용하기
  - 만약 기관 등의 서버에서 Open API를 제공하고 있다면, 손쉽게 정제된 형태의 데이터를 가져올 수 있음
  - 주로 `XML`이나 `JSON`이라는 데이터 형식으로 인코딩 하고있음

    ![alt text](image-1.png)

### WebDriver
크롬, 파이어폭스, edge등의 웹 브라우저에서 클릭, 텍스트 입력 등의 행동을 코드로 제어 할 수 있게끔 만든 브라우저

- WebDriver로 웹 브라우저 자동화
  - 로그인이 필요한 서비스 등 단순 URL만으로는 접속할 수 없는 세션 유지가 필요한 작업이나, 옵션이나 드롭다운 메뉴를 클릭해야 데이터를 얻을 수 있는 작업에 필요함
    - 꼭 크롤링이 아니더라도, 웹 브라우저의 행동을 자동화하는 다른 작업 (게시판에 자동 글쓰기, 새 메일 자동 읽기 등)에도 사용할 수 있음

    ![alt text](image-2.png)

  - Python에서는 `Selenium` 라이브러리를 사용함

### 기타 고급 작업
아주 드물게 HTML이나 WebDriver만으로는 접근이 불가능한 데이터도 있음 (공인인증서 등 추가 팝업 프로그램을 사용하는 경우, 이미지 내의 텍스트 등)

- 웹 브라우저를 넘어서는 수준의 조작이 필요한 경우
  - Python에서는 `win32api`나 `pyautogui` 등의 라이브러리를 사용
  - 데스크탑 수준에서 마우스 강제이동, 클릭, 키보드 입력 등을 제어할 수 있음
  - 따라서, 거의 모든 작업을 자동화할 수 있음

- 이미지 인식이 필요한 경우 
  - `opencv-python(cv2)`, `DirectPython`, `PyGTK`, `PIL` 등의 라이브러리를 사용
  - 화면 실시간 캡처, 이미지 인식까지 구현할 수 있음

- 크롤링 정보 저장이 필요한 경우
  - `*.txt`, `*.csv` 형태로 저장하거나 혹은 `pickle` 라이브러리를 써서 내보냄
  - 그러나 간혹 엑셀(`*.xlsx`)형태로 가공하여, 셀 크기나 색상 등의 속성, 글꼴 속성 등을 설정하고자 하는 경우가 있음
    - 이 경우 `openpyxl`을 사용하면 python으로 엑셀 문서의 거의 모든 것을 편집할 수 있음
    - 비슷하게 MS Word는 `python-docx`, 아래한글은 `pyHWP`로 편집할 수 있음
    
### 웹 페이지의 구조
웹 페이지는 HTML, CSS, JavaScript의 조합으로 구성됨

- HTML(hypertext markup language)
  - 웹 페이지를 구성하는 필수 언어
  - 제목, 단락, 목록 등과 같은 본문을 위한 구조적 의미를 나타내는 것 뿐만 아니라 링크, 인용과 그 밖의 항목으로 구조적 문서를 만들 수 있는 방법을 제공함
  - 웹 크롤링을 통해 얻고자 하는 데이터는 HTML의 어딘가에 들어 있음
    - 따라서 웹 페이지 내의 어느 정보가 HTML의 어디에 위치하는지를 파악하기 위해, HTML의 구조를 알아야 함
    - 데이터를 뽑아내 원하는 형태로 가공하는 작업을 파싱(parsing)이라 함

- HTMP 기본 구성 요소

  ![alt text](image-4.png)

  - 속성
    - 해당 요소에 추가적인 내용을 담고 싶을 때
    - ex. 글자색 크기, 배경색, 배경이미지, 여백 등
      
      ![alt text](image-5.png)
    - 현재 예시는 가운데 정렬

- HTML 구조
  ```HTML
  <html>  <!-- 해당 프로그램 언어가 HTML임을 알림 -->
    <head>  <!-- 문서 서문의 시작과 끝을 알림 -->
      <title>  <!-- 문서 제목을 나타냄 -->
        test 홈페이지
      </title>
    </head>
    <body>  <!-- 문서 본문 시작을 알림 -->
      환영합니다
    </body>
  </html>
  ```

- HTML 태그 구조

  ![alt text](image-6.png)

  ![alt text](image-7.png)


- CSS(cascading stylesheets)
  - HTML의 요소의 외관을 디자인 함

    ![alt text](image-3.png)

- CSS 기본 문법

  ![alt text](image-8.png)

- CSS 타입선택자
  - 타입 선택자
    - HTML 태그 이름 그대로 선택하는 방식
      ```CSS
      p {
        color : red;
      }
      ```
      ![alt text](image-9.png)
  
  - 아이디 선택자
    - html의 고유한 id 요소를 선택하는 방식
      ```CSS
      #item {
        color : yellow;
      }
      ```
      ![alt text](image-10.png)

  - 클래스 선택자
    - 여러 요소에 동일하게 선택하는 방식
    - 즉, 같은 유형의 데이터를 한번에 수집할 때 사용
      ```CSS
      .item {
        color : yellow;
      }
      ```
      ![alt text](image-11.png)

  - 자식 선택자
    - 중첩된 구조에서 쓸데없는 태그 제외하고 필요한 태그만 가져오고 싶을 때 선택하는 방식
      ```CSS
      body > p {
        color : blue;
      } 
      ```
      ![alt text](image-12.png)

  - 가상 클래스 선택자
    - 순서가 중요한 표 등 특정 몇번째 데이터를 정확히 추출하고자 할 때 사용
      ```CSS
      ul > li:nth-child(3){ /* ul의 자식 li들 중 3번째 자식 선택 */
        
        color : yellow;
      }
      ```
      ![alt text](image-13.png)

### Robots.txt
웹사이트 소유자가 검색 엔진의 웹 크롤러(로봇)가 사이트의 특정 부분을 **크롤링하거나 인덱싱하는 것을 허용하거나 차단하기 위해 사용하는 텍스트 파일**

![alt text](image-14.png)

-> 웹사이트의 최상위 디렉토리에 `/robots.txt` 추가로 입력하면 크롤링 가능한 영역 등을 지정해놓은 규칙이 적힌 txt파일이 보임

-> 웹사이트 소유자가 공개 여부를 제어하고자 할 때 사용


## Beautiful Soup의 활용
- HTTP 통신
  - 웹 브라우저와 웹 서버 사이에 데이터를 주고 받는데 사용되는 통신

    ![alt text](image-15.png)
    - request 라이브러리를 통해 가져온 HTML 문서를 분석하는 것이 BeautifulSoup

### Requests 라이브러리
접근할 웹 페이지의 데이터를 요청/응답 받기 위한 라이브러리

```python
import requests as req

# 예시: 네이버 페이지 요청하기
  # 나: request야, 네이버 메인 페이지 정보 좀~
  # request: 네, 근데 네이버가 어디?
  # 나: www.naver.com
res = req.get("http://www.naver.com")

# 응답 확인
  # <Response [200]> = 데이터 잘 가지고 옴. 통신 성공!
res   # <Response [200]>

# res 변수에서 내용 보기
res.text
```
![alt text](image-16.png)

- 필요 라이브러리 설치
  ```python
  pip install requests
  pip install bs4

  # 위 실행이 안될 경우, pip 경로를 인식 못하고 있는 상태이므로 아래와 같이 시도
  python -m pip install requests
  python -m pip install bs4
  ```
  - requests
    - 웹 통신 라이브러리
  - BeautifulSoup(bs4)
    - HTML 파싱 라이브러리

- GET 메소드와 URL
  - GET 메소드
    - 웹 서버에게 파라미터를 포함해 요청을 보내는 가장 쉬운 방법으로, URL에 파라미터 정보를 담아서 보내는 것
    - 사용자가 서버에게 웹 페이지를 보여 달라고 하는 것을 **요청**이라 하고
    - 서버가 요청에 대한 대답을 담아 HTML문서로 주는 것을 **응답**이라고 함

      ![alt text](image-17.png)
      - 위 예제는 가상의 사이트 www.example.com 의 로그인 페이지에 bigdata라는 id와 123456이라는 비밀번호로 로그인 요청을 보내는 것이다.
      - 요청을 보낼 페이지의 URL과 파라미터 사이는 반드시 `?` 로 구분되어야 한다.
      - 파라미터는 '변수명=값'형태로 나열되며, 변수 사이는 `&` 로 구분해야 한다.

        ![alt text](image-18.png)
  
  - 예시
    - 해당 페이지의 title만 출력
      ```python
      # 1. 원하는 url로 요청 보낸 뒤, 응답 문자열을 html로 받는다
      # 2. html 문자열에서 <title>과 </title>의 위치를 각각 찾아 인덱스를 저장한다
      # 3. html 문자열 중 title 시작과 끝 인덱스 사이의 문자열을 출력한다
      import requests

      url = 'http://mysnu.ac.kr'
      html = requests.get(url).text

      title_begin = html.index('<title>')
      title_end = html.index('</title>')
      title = html[title_begin : title_end]

      print(title)
      ```
      ![alt text](image-19.png)
    
    - 주식 종목코드로 기업 이름 알아내기
      ```python
      # 1. 검색할 종목코드를 code라는 변수로 선언한다
      # 2. GET 방식 파라미터를 포함한 URL에 code를 더하여 requests.get으로 요청한다
      # 3. title을 출력한다 (ex. 코드 005930은 삼성전자)
      import requests

      code = '005930'
      url = 'https://finance.naver.com/item/main.nhn?code='
      html = requests.get(url + code).text

      title_begin = html.index('<title>')
      title_end = html.index('</title>')
      title = html[title_begin : title_end]

      print(title)
      ```
      ![alt text](image-20.png)


### BeautifulSoup
HTML 및 XML 문서를 구문 분석하기 위한 Python 패키지로, 데이터를 쉽게 추출할 수 있도록 도와줌

-> 웹 브라우저가 하는 일과 비슷하게, HTML 소스를 트리 형태로 해석한 뒤 접근할 수 있음

- 예시
  - 학식 정보 가져오기
    ```python
    # 1. 구글에 '서울대학교 식단'을 검색
      # 생활협동조합 식단 페이지(https://snuco,snu.ac.kr/foodmenu)에 접속할 수 있음
    # 2. 이를 GET으로 요청한 뒤, 응답 텍스트를 BeautifulSoup의 인자로 넣고 실행
    import requests
    from bs4 import BeautifulSoup

    url = 'https://snuco.snu.ac.kr/foodmenu'
    html = requests.get(url).text
    bs = BeautifulSoup(html)
    print(bs)
    ```
    ![alt text](image-21.png)
    
- BeautifulSoup 활용
  - 겉보기엔 별 변화가 없지만, 현재 bs 객체에는 이미 HTML의 정보가 BeautifulSoup를 통해 해석되어 있음
    - 예를 들어, 아래처럼 title을 쉽게 출력할 수 있음
      ```python
      print(bs.title)
      ```
      ![alt text](image-22.png)
  
  - 학생회관식당의 점심 메뉴가 HTML코드 상에서 어디에 위치하는지 알기 위해, 우클릭으로 요소 검사를 시행

    ![alt text](image-23.png)

  - 개발자도구로 코드를 보기
    - 점심 메뉴는 `<td>`와 `<p>` 두 가지 태그에 이중으로 포함되어 있음을 알 수 있음

      ![alt text](image-24.png)

    - `<p>` 태그는 다른 곳에서도 많은 용도로 사용되지만, `<td>`는 그중 표 안의 내용으로 한정됨
    - `class = view-field views-field-lunch` 까지 만족하는 태그는 점심메뉴로 한정됨
    - 따라서 이 경우는 `<td>` 가운데 해당 클래스를 만족하는 것을 검색하면 점심 메뉴의 리스트를 얻을 수 있음

      ![alt text](image-25.png)
      ```python
      # <p> 태그로 선택 (모든 정보가 포함됨)
      # 즉, 점심메뉴 뿐만 아니라 온갖 정보들이 섞여나옴
      bs.select('p')
      ```
      ![alt text](image-26.png)

  - 태그의 식별을 위한 속성으로 주로 `id` 와 `class` 가 사용됨
    - id 는 문서 내에서 유일한 존재이며, class 는 중복될 수 있음
    - 따라서 이 경우는 `<td>` 가운데 해당 클래스를 만족하는 것을 검색하면 점심 메뉴의 리스트를 얻을 수 있음
      ![alt text](image-27.png)

      ![alt text](image-28.png)

    - 점심 메뉴의 하위클래스는 lunch이므로, 이를 갖는 `<td>` 태그만 선택하면 됨
      ```python
      bs.select('td.lunch')
      ```
      ![alt text](image-29.png)

    - 이 가운데 학생회관식당 메뉴만 얻으려면, 리스트의 첫 번째 요소를 출력하면 됨
      ```python
      bs.select('td.lunch')[1]
      ```
      ![alt text](image-30.png)

    - 태그 객체 안에 들어있는 텍스트만 원한다면, `.text` 속성을 읽으면 됨
      ```python
      bs.select('td.lunch')[1].text.strip().split('\n')[0].replace('\r', '')
      ```
      ![alt text](image-31.png)


## OpenAPI의 활용
### OpenAPI
오픈 API(Open Application Programming Interfae)는 누구나 사용할 수 있도록 공개된 API를 말함

-> 웹 사이트가 가진 기능을 모두가 이용할 수 있도록 공개한 프로그래밍 인터페이스

-> 네이버 지도, 구글 맵 등이 대표적인 예시

-> 코로나 사태 때 생긴 라이브코로나, 코로나 맵 등의 서비스는 네이버 클라우드 플랫폼의 API 지원을 받고 있음

-> 
  - 대한민국 정부에서는 공공데이터포털을 통해 도로명주소조회서비스, 동네예보정보조회 서비스 등 현재 약 92,771종의 데이터를 제공하고 있음
  - 일반적인 웹 페이지에 GET요청을 보내면 HTML 문서를 응답해주지만, OpenAPI에 GET요청을 보내면 데이터를 정해진 형식의 텍스트로 응답해줌

| 유형 | 설명 | 확장자 예시 |
|---|---|---|
| 텍스트 | 이미지, 표, 동영상 등을 미포함하는 텍스트만 포함된 표준 텍스트 문서<br>윈도우, 리눅스 등에서 제공하는 기본 텍스트 편집 파일 형식| txt, vi 등|
| 문서 | 이미지, 동영상 등 멀티미디어 포함이 가능하고 XML 포맷으로 작성된 문서<br>'문서'의 경우 특정SW를 통해 편집, 열람 등이 가능하고, '텍스트'의 경우 | hwpX, docx 등|
| 이미지 | - 레스터, 벡터 등의 형태를 가진 디지털 이미지 데이터<br>- 레스터: 비트맵 형식의 파일로 배열 형태의 이미지<br>- 벡터: 선이나 도형으로 만들어진 이미지 | jpg, png 등|
| 음성(음향)| 소리, 음성을 담은 디지털 오디오 데이터 | mp3, acc 등|
| 동영상 | 프레임을 활용하여 만들어진 움직이는 이미지와 음성 및 음향의 모음 | mp4, mpg 등 |
| 공간 정보 | 지리정보, 위치정보 등 공간 데이터를 포함하는 비정형 데이터 유형| Shapefile (SHP, SHX, DBF) 등|

- 공공 데이터 포털의 OpenAPI 활용
  - 먼저 공공데이터포털(https://www.data.go.kr/index.do)에 접속한다.
  - 검색창에 관심 주제를 검색한다. 혹은 인기검색어로 검색한다.
  - 분류에서 파일데이터는 다운로드 가능한 과거 데이터, 오픈API는 실시간 상황이 반영되는 API를 의미한다. 오픈 API중 원하는 주제를 선택한다.
  - 적절히 활용 목적을 쓰고, 동의 후 활용신청 버튼을 누른다. (검토단계 없음)
  - 이후 마이페이지>오픈API>인증키방급현황에서 인증키를 확인한다.
  - 오픈API는 서버의 과도한 부담을 막기 위해 인증키를 사용하는 경우가 많다.
  - 동일한 인증키로 과도한 요청이 들어오면, 해당 키의 권한이 일시정지된다.
  - 다시 해당 OpenAPI의 상세페이지로 돌아가면, 요청변수에 무엇을 입력해야 하는지 설명되어있다. 샘플데이터의 입력방식을 참고하여 GET요청을 보내면 된다. 필수 항목은 반드시 입력해야 하고, 옵션 항목은 GET에 포함시키지 않아도 된다. 여기서는 ServiceKey만이 필수요건이고 나머지는 옵션이다.
    > 구분자 ? 와 & 의 위치에 주의하며 URL 변수를 작성한다.
    > Oprional 파라미터는 공란으로 두더라도, 일단 변수명은 써주는게 좋다.
    > python에서 한 행이 너무 길어지면 줄 끝에 \를 삽입해 줄 바꿈 할 수 있다.
    > 응답 텍스트가 길게 나타나면 성공
    - 해당 API 는 결과를 XML형태로 반환해준다.
    - XML도 HTML과 비슷한 문법으로 해석 가능하다.
    > 매 <item>태그마다 연령/성별 계층의 통계가 들어있는 구조
    > API 설명 페이지의 출력결과 변수를 참고하여 원하는 내용만 추출할 수 있다
      - confCase(확진자 수), confCaseRate(확진률), gubun(성/연령 구분)
    > 내용 구성을 확인해보면, 각 <item>마다 연령이나 성별을 기준으로 구별되어 있으며, 하루에 한 번씩 데이터가 추가되고 있다.
    > 현재 연령별 통계를 알고 싶다면, 가장 최근 9개의 <item>을 확인하면 된다.
    > 유일한 태그를 검색하려는 경우 select_one('tag')를 사용하여 선택 가능하다.
    > 좌측의 예제에서는 각 item에 gubun이나 confCase가 하나씩만 있으므로 select_one 을 사용했다.
    > 테스트 결과, 이 API에서 pageNo 나 numOfRows 는 별로 의미가 없는 옵션이라서 생략해도 무방하다.