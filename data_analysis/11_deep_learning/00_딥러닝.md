# 데이터 분석을 위한 딥러닝 기초
## 딥러닝 개념 및 흐름
### 딥러닝(Deep Learning)
데이터를 반복 학습하며 패턴을 찾는 인공지능 기술

-> 영상, 음성, 텍스트 등 다양한 데이터에 활용됨

- 딥러닝과 머신러닝 차이
    - 머신러닝은 **사람이 특징을 설계**해야 함
    - 딥러닝은 특징 추출까지 **모델이 자동으로 수행**함
    - 모두 학습 기반이지만 처리 방식에 차이가 있음

        ![alt text](image.png)

### 딥러닝 모델 구조
- 수능점수 예측 모델

    ![alt text](image-1.png)

    ![alt text](image-2.png)

    ![alt text](image-3.png)

### 모델 학습 방법
1. 순전파
2. Loss 계산
3. 역전파

- 순전파

    ![alt text](image-4.png)
    ![alt text](image-5.png)
    - `w1, w2, ...` : 가중치(weight)
    - 모두 덧셈, 곱셈으로만 이루어져 있음

- Loss 계산
    - 대표적인 활성화 함수
        - `sigmoid` : 출력값을 0과 1 사이로 압축하는 S자 곡선 형태 함수
        - `Hyperbolic tangent` : Sigmoid를 변형한 함수로, 출력 범위가 -1 ~ 1
        - `Rectified Linear Unit(ReLU)` : 음수 입력은 0, 양수 입력은 그대로 출력

        ![alt text](image-6.png)

    - 예시

        ![alt text](image-7.png)
        ![alt text](image-8.png)
        . . .
        - 학생 7명 넣었을 때의 결과

            ![alt text](image-9.png)
        - 모델이 얼마나 예측을 틀렸는지 판단해주는 근거 = `Loss`

    - 대표적인 Loss 함수
        - MSE (Mean Squared Error)

            ![alt text](image-10.png)
        - RMSE (Root Mean Squared Error)

            ![alt text](image-11.png)

- 역전파(Back propagation)
    - 각 노드의 가중치(w)가 최종 예측값에 얼마나 영향을 끼치는지 파악하여, 오차(손실)을 줄이는 방향으로 가중치를 업데이트 하는 것

    - 예시: `(a + b) * c` 식에서 a,b,c가 각각 최종 결과에 끼치는 영향은 얼마일까?
        - ex. `a = 6`, `b = 11`, `c = -3`

            ![alt text](image-12.png)
        
        - f,g 계산하면 `f = 17`, `g = -51`
        - a, b를 계산해서 만들어진 f를 a, b로 편미분하면 수식 f에 a, b가 끼치는 영향이 어느정도인지 알 수 있음

            ![alt text](image-13.png)
            - `f = a + b` 를 a로 편미분
            - `f = a + b` 를 b로 편미분

        - 전체 식에서 a가 끼치는 영향

            ![alt text](image-14.png)
            - `g = f * c`를 f로 편미분하면 f가 g에 미치는 영향은 -3만큼임
            - (g를 f로 편미분) * (f를 a로 편미분) = g를 a로 편미분
            - 즉, $\left(\frac{\partial g}{\partial f}\right) \cdot \left(\frac{\partial f}{\partial a}\right) = \frac{\partial g}{\partial a}$ 성립
            - 왜? 합성함수의 미분법칙인 **연쇄법칙**에 의해 성립함
            - a가 변할 때 g가 변하는 정도는 'a가 f에 끼치는 영향과, f가 g에 끼치는 영향을 곱한 값'으로 표현할 수 있음
            - 따라서 g (전체식)에 a가 미치는 영향을 구할 수 있음 

        - 전체 식에서 b가 끼치는 영향

            ![alt text](image-15.png)
            - $f = a + b$
            - $g = f \cdot c$
            - $\frac{\partial f}{\partial b} = 1$ -> b가 1만큼 변할 때 f도 1만큼 변함
            - $\frac{\partial g}{\partial f} = c$ -> f가 1만큼 변할 때 g는 c만큼 변함
            - $\left(\frac{\partial g}{\partial f}\right) \cdot \left(\frac{\partial f}{\partial b}\right) = \frac{\partial g}{\partial b}$
            - 즉, b가 변하면 g는 상수 c의 크기만큼 영향을 받음
        
        - 전체 식에서 c가 끼치는 영향

            ![alt text](image-16.png)


### 딥러닝 흐름
1. 순전파 과정을 거쳐 예측값 계산
    - 가중치를 연산하여 예측값을 모델이 추론함

        ![alt text](image-17.png)

2. Loss(손실) 계산
    - 예측값을 실제 정답과 비교하여 오차를 확인하고,
    - 손실 함수를 통해 오차를 계산함

        ![alt text](image-18.png)

3. 역전파 기반 가중치 업데이트
    - 각 가중치가 예측값에 얼마나 영향을 끼치는지 계산한 뒤,
    - 오차를 줄이는 방향으로 가중치 업데이트

    ![alt text](image-19.png)
    ![alt text](image-20.png)
    ![alt text](image-21.png)

- 딥러닝 전체 흐름 요약

    ![alt text](image-22.png)

### Epoch, Batch, Iteration
모델은 전체 데이터를 반복적으로 학습하며, 이 과정을 작은 묶음 단위로 나누어 처리함

![alt text](image-23.png)

- `Epoch` : 전체 학습 데이터가 모델을 한 번 통과한 횟수
- `Batch` : 한 번에 처리하는 데이터 묶음 크기
- `Iteration` : 하나의 Batch를 학습하는 횟수 (한 Epoch 안에 여러 Iteration 존재)


## 이미지 데이터 처리
### 이미지 특징
![alt text](image-24.png)
![alt text](image-25.png)

-> 이미지를 확대해보면 사진이 픽셀들로 구성되어있음을 알 수 있음

-> 하나의 픽셀을 찍어보면, 해당 픽셀의 색상 코드를 확인할 수 있음

-> **이미지도 결국 숫자**

### 이미지 학습 과정
![alt text](image-26.png)

- 학습 과정
    1. 입력 단계 
        - 이미지가 수천~수만 개 픽셀로 분해되어 입력층에 전달됨
    2. 특징 추출 단계
        - 각 픽셀 값은 가중치(w)와 곱해지고, 더해지고, 활성화 함수 등을 거치며 특징(feature)을 뽑아냄
        - 단순한 색·선·모양 같은 저수준 특징부터, 층이 깊어질수록 귀 모양, 털 패턴 같은 고수준 특징까지 추출 가능
    3. 출력 단계
        - 마지막 층에서 추출된 특징들을 종합해 "사진이 강아지일 확률"을 계산함
        - 소프트맥스(Softmax) 같은 함수를 사용해 여러 클래스 중 확률로 변환함
    4. 학습(역전파)
        - 예측 확률과 실제 정답(라벨)의 차이를 **오차(loss)**로 계산
        - 이 오차를 줄이기 위해 **연쇄법칙(Chain Rule)**을 이용한 역전파로 가중치들을 조정
        - 이 과정을 반복하면서 네트워크가 점점 더 정확하게 학습

- 이미지의 특징
    - 이미지는 작은 픽셀들의 집합
    - 각 픽셀은 색(RGB)이나 밝기 같은 정보를 숫자로 표현할 수 있음

    - 픽셀 -> 입력값
        - 딥러닝 모델은 숫자를 다루기 때문에, 이미지를 그대로 모델에 넣을 수는 없음
        - 보통은 이미지를 **1차원 벡터(1열로 나열된 숫자 리스트)**로 바꿔서 입력값으로 사용함
    
    - MLP(다층 퍼셉트론)과 한계
        - 이렇게 변환된 픽셀 벡터를 **MLP(다층 퍼셉트론)**에 넣어 학습할 수 있음
        - MLP는 픽셀 간의 공간적 구조(예: 눈, 귀 위치 관계)를 고려하지 않고 **모든 픽셀을 독립적으로 처리**함
        - 따라서 단순 패턴 학습에는 괜찮지만, 이미지의 위치적/지역적 특징을 잡는 데에는 비효율적
        - 아래 그림과 같이 이미지를 해체시켜 1차원 벡터로 바꾸면 이미지의 특성이 깨짐을 볼 수 있음 -> 이미지가 조금만 바뀌어도 예측 잘 못함

            *59p - 66p* 글자빼고 그림만
    
    - 그래서 나온 것이 **CNN (합성곱 신경망)**
        - CNN은 이미지의 공간 구조를 고려해 **필터(커널)**로 국소 특징을 추출하고, 이를 쌓아가며 복잡한 패턴을 학습함
        - 즉, 이미지의 특징을 유지한 채로 압축하자

            ![alt text](image-27.png)

            *69 - 77* 글자빼고 그림만

### Convolutional Neural Network (CNN)
이미지 및 영상 인식을 위한 딥러닝의 기본 모델

![alt text](image-28.png)

-> 커널(필터)를 이용해 원본 이미지의 특징을 뽑아내고, 새로운 레이어(Feature map)를 만듦

- 커널
    - 커널은 이미지에서 특정 특징이 있는지 없는지를 수치적으로 판단하는 역할을 함
    - 이미지의 일부분과 커널을 곱하고 합산했을 때,
        - 해당 특징이 잘 맞으면 **큰 값**
        - 해당되지 않으면 **작은 값**
    - 이 차이가 피처맵(Feature map)에 기록됨
    - 예시: 고양이

        ![alt text](image-29.png)
        - 꼬리를 감지하는 커널로 꼬리 탐색

            ![alt text](image-30.png)

            ![alt text](image-31.png)
        - 꼬리를 감지하는 커널로 귀 탐색

            ![alt text](image-32.png)
        - 꼬리를 감지하는 커널을 적용하면 꼬리 부분에서는 큰 값이 나오고, 귀 부분에서는 작은 값이 나옴
        - 반대로 귀를 감지하는 커널을 적용하면 귀 영역에서 큰 값이 나오게 됨
    - 여러 개의 커널을 사용하면, 각 커널이 서로 다른 특징을 잘 잡아내고, 최종적으로 이미지 구분에 필요한 다양한 특징들을 뽑아낼 수 있음

        ![alt text](image-33.png)
        - 이렇게 여러 개의 커널을 동시에 적용하면, 각각의 커널이 자기 역할에 맞는 피처맵을 생성
        - 즉, 원본 이미지가 '귀 피처맵, 꼬리 피처맵..' 같은 형태로 분해되어 저장되는 것

- CNN (합성곱 신경망) 전체 구조

    ![alt text](image-34.png)
    1. Input Image (입력 이미지)
        - 원본 이미지를 픽셀 값(RGB 등)으로 신경망에 입력
        - ex. 강아지 사진
    2. Convolution Layer (합성곱 층)
        - 여러 개의 **커널(필터)**을 이용해 이미지의 국소 영역을 훑으며 특징을 추출
        - ex. 직선, 곡선, 귀 모양, 꼬리 등 다양한 패턴을 감지
        - 출력은 여러 개의 피처맵(Feature map)으로 나타남
    3. ReLU Layer (활성화 함수 층)
        - 합성곱 연산 결과에 **비선형성(Non-linearity)**을 추가
        - ReLU(Rectified Linear Unit)는 음수 값을 0으로 바꿔주고 양수는 그대로 전달 -> 계산 단순 & 기울기 소실 문제 완화
        - 신경망이 단순 선형모델을 넘어서 복잡한 패턴을 학습할 수 있음
    4. Pooling Layer (풀링 층)
        - 피처맵의 크기를 줄여 연산량을 감소시키고, 동시에 중요한 특징을 유지
        - 대표 방식:
            - `Max Pooling`: 영역 내 최댓값 선택 -> 가장 강한 특징만 남김
            - `Average Pooling`: 영역 내 평균값 선택 -> 전체적 특징 유지
    5. Fully Connected Layer (완전 연결층)
        - 풀링된 특징들을 1차원 벡터로 펼쳐서 전통적인 신경망(MLP) 구조에 연결
        - 이미지 전체의 추출된 특징들을 종합해 **최종 분류를 수행**함
        - ex. "Dog" vs "Not Dog" 같은 클래스 확률 계산

    - CNN 구조 예시 (자동차 분류)

        ![alt text](image-35.png)

### 예시: Fashion MNIST 데이터셋
- 데이터셋 알아보기
    - 60000개의 학습 데이터, 10000개의 테스트 데이터
    - 10개의 카테고리 -> 각 카테고리별 7000개의 이미지들로 구성

        ![alt text](image-36.png)

- TrainSet (학습데이터) / ValidationSet (검증데이터) / TestSet (테스트데이터)
    - 검증 데이터는 무엇인가?
        - ex. 시험기간

            ![alt text](image-37.png)
            - 수많은 문제들로 단련하고, 모의고사랑 기출문제로 실력 점검하며 반복적으로 공부하다가 최종적으로 테스트를 봄
        - 즉, 검증 데이터는 **학습 과정 중 모델의 성능을 점검하고 튜닝하기 위해 사용하는 데이터**

    - 전체 데이터 구분하기

        ![alt text](image-38.png)
        - `batch` : 모델이 한 번에 학습하는 데이터의 양
        - `epoch` : 모델이 학습 데이터를 몇 번 학습할 것인지
        - 배치를 12로 설정했을 때

            ![alt text](image-39.png)
        - ex. 1000개의 학습 데이터가 있을 때, 배치는 50, 에폭은 10으로 설정해서 모델을 만들었다면
            1. 총 몇 번의 배치가 반복될까? `(1000 / 50) * 10 = 200번`
            2. 각 데이터는 모델이 몇 번 확인할까? `10번`

    - 1 에폭 학습한 모델이 잘 학습했는지 평가할 때 **검증 데이터**를 사용함
        - 즉, 학습 도중 각 에폭이 끝날때마다 '검증셋'을 통해 평가
        - 에폭을 10으로 설정한 모델이라면, 10 에폭 학습한 최종 모델을 평가할 때 '테스트셋' 사용함

- 코드

정규화를 통해 0 ~ 1 사이의 실수로 바꿔줌
(모델이 좀 더 안정적이고 빠르게 학습 하는 데 도움을 줌)


## 시퀀스 데이터 처리

