{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05564eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain + RAG 예시 코드 (뉴스 QA, 검색 청크 출력)\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 문서 로더, 텍스트 분할기(청킹), 벡터스토어(FAISS)\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# OpenAI 임베딩 및 LLM\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# 대화형 메시지 포맷(시스템 지시 / 사용자 메시지)\n",
    "from langchain.schema import HumanMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80b6837",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "OPENAI_API_KEY가 설정되지 않았습니다. .env 파일을 확인하세요.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY가 설정되지 않았습니다. .env 파일을 확인하세요.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: OPENAI_API_KEY가 설정되지 않았습니다. .env 파일을 확인하세요."
     ]
    }
   ],
   "source": [
    "# 0. API Key 설정 (.env에서 불러오기)\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY가 설정되지 않았습니다. .env 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948142f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 텍스트 파일 로드\n",
    "def load_news_data(txt_file_path):\n",
    "    \"\"\"\n",
    "    텍스트 파일을 LangChain의 Document 형태로 로드합니다.\n",
    "    - TextLoader는 파일을 읽어서 Document 리스트로 반환합니다.\n",
    "    - 각 Document에는 'page_content'(텍스트)와 'metadata'(부가정보)가 포함됩니다.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(txt_file_path):\n",
    "        raise FileNotFoundError(f\"파일을 찾을 수 없습니다: {txt_file_path}\")\n",
    "\n",
    "    loader = TextLoader(txt_file_path, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"로드 완료: {len(documents)}개 문서\")\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b63130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 문서 분할 (청킹)\n",
    "def split_documents(documents, chunk_size = 200, chunk_overlap = 50):\n",
    "    \"\"\"\n",
    "    긴 문서를 작은 청크로 분할합니다.\n",
    "    - chunk_size: 한 청크의 최대 길이 (문자 수 기준).\n",
    "    - chunk_overlap: 청크 간 겹치는 부분의 길이. 문맥 단절을 방지합니다.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    splits = splitter.split_documents(documents)\n",
    "    print(f\"분할 완료: {len(splits)}개 청크 생성\")\n",
    "\n",
    "    # 디버깅용: 앞의 몇 개 청크 미리보기\n",
    "    preview_n = min(3, len(splits))\n",
    "    for i in range(preview_n):\n",
    "        print(f\"\\n[청크 미리보기 {i+1}]\")\n",
    "        print(splits[i].page_content[:200].strip(), \"...\")\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 벡터 DB 구축 (임베딩 → FAISS 인덱스)\n",
    "def store_in_vector_db(splits):\n",
    "    \"\"\"\n",
    "    텍스트 청크를 임베딩(벡터화)한 후, FAISS를 이용해 빠른 유사도 검색이 가능한 DB를 만듭니다.\n",
    "    - OpenAIEmbeddings: OpenAI의 임베딩 모델을 사용합니다.\n",
    "    - FAISS: 대규모 벡터 검색에 최적화된 라이브러리입니다.\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(api_key=api_key, model=\"text-embedding-3-small\")\n",
    "    vector_store = FAISS.from_documents(splits, embeddings)\n",
    "    print(\"벡터 DB 생성 완료\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 문서 검색\n",
    "def retrieve_similar_docs(query_text, vector_store, k = 2):\n",
    "    \"\"\"\n",
    "    사용자의 질문을 벡터화하여, 의미적으로 가장 유사한 청크 k개를 검색합니다.\n",
    "    \"\"\"\n",
    "    docs = vector_store.similarity_search(query_text, k=k)\n",
    "\n",
    "    # 검색된 청크 출력\n",
    "    print(\"\\n[검색된 청크]\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        print(f\"\\n--- 청크 {i} ---\")\n",
    "        print(d.page_content.strip())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cee8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 답변 생성 (검색된 청크 기반)\n",
    "def generate_answer(query_text, docs):\n",
    "    \"\"\"\n",
    "    검색된 청크를 LLM에 제공하여 답변을 생성합니다.\n",
    "    - SystemMessage: 모델의 역할과 규칙 정의 (예: 뉴스 분석 전문가).\n",
    "    - HumanMessage: 실제 질문과 검색된 청크를 함께 전달.\n",
    "    - LLM은 \"주어진 청크\"만 근거로 답변합니다.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=api_key)\n",
    "\n",
    "    # 검색된 청크들을 하나의 텍스트로 합칩니다.\n",
    "    docs_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    system_message = SystemMessage(\n",
    "        content=\"너는 뉴스 분석 전문가야. 반드시 주어진 문서 내용만 바탕으로 답변하고, 문서에 없는 내용은 '잘 모르겠습니다'라고 말해.\"\n",
    "    )\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"질문: {query_text}\\n\\n[참고 문서]\\n{docs_text}\"\n",
    "    )\n",
    "\n",
    "    response = llm.invoke([system_message, human_message])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 실행\n",
    "txt_file_path = \"./news.txt\"  # 분석할 뉴스 기사 파일\n",
    "\n",
    "queries = [\n",
    "    \"정부가 발표한 총 예산 규모는 얼마인가?\",\n",
    "    \"농촌 지역 교통격차 해소를 위해 얼마가 투입되었나?\",\n",
    "    \"친환경 버스 보급과 관련된 목표는 무엇인가?\",\n",
    "]\n",
    "\n",
    "# Step 1. 뉴스 데이터 로드\n",
    "documents = load_news_data(txt_file_path)\n",
    "\n",
    "# Step 2. 문서 분할 (청킹)\n",
    "splits = split_documents(documents, chunk_size=200, chunk_overlap=50)\n",
    "\n",
    "# Step 3. 벡터 DB 구축\n",
    "vector_store = store_in_vector_db(splits)\n",
    "\n",
    "# Step 4. 질문별 검색 및 답변 생성\n",
    "for q in queries:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"질문:\", q)\n",
    "\n",
    "    similar_docs = retrieve_similar_docs(q, vector_store, k=2)\n",
    "    answer = generate_answer(q, similar_docs)\n",
    "\n",
    "    print(\"\\n답변:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
