## ğŸ”¹ NumPy ì£¼ìš” í•¨ìˆ˜ ì •ë¦¬
### `np.arange()`
ì¼ì •í•œ ê°„ê²©ì˜ ìˆ«ì ë°°ì—´ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

- í˜•íƒœ
  ```python
  np.arange(start, stop, step)
  ```
  - start: ì‹œì‘ê°’ (ê¸°ë³¸ê°’ 0)
  - stop: ëê°’ (í¬í•¨X)
  - step: ì¦ê°€ ê°„ã…‡ê²© (ê¸°ë³¸ê°’ 1)

- ì˜ˆì‹œ
  ```python
  np.arange(5)          # array([0, 1, 2, 3, 4])
  np.arange(2, 10, 2)   # array([2, 4, 6, 8])
  ```

### `.reshape()`
ë°°ì—´ì˜ í˜•íƒœ(ì°¨ì›) ë¥¼ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜

- í˜•íƒœ
  ```python
  array.reshape(rows, cols)
  ```

- ì˜ˆì‹œ
  ```python
  arr = np.arange(6)          # [0 1 2 3 4 5]
  arr.reshape(2, 3)           # [[0 1 2]
                              #  [3 4 5]]
  ```

- í™œìš©
  - scikit-learn ëª¨ë¸(LinearRegression)ì€ 2ì°¨ì› ì…ë ¥ë§Œ í—ˆìš©í•˜ë¯€ë¡œ
  - `np.arange(len(data)).reshape(-1, 1)` í˜•íƒœë¡œ ë³€í™˜í•´ì•¼ í•¨
  - (ì¦‰, `(n,)` -> `(n, 1)` í˜•íƒœë¡œ ë³€í™˜)

### `np.array()`
Python ë¦¬ìŠ¤íŠ¸(list)ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜

- ì˜ˆì‹œ
  ```python
  data = [10, 20, 30]
  np.array(data)  # array([10, 20, 30])
  ```

- í™œìš©
  - sklearn ëª¨ë¸ì˜ ì…ë ¥ì€ ë°˜ë“œì‹œ NumPy ë°°ì—´ í˜•íƒœì—¬ì•¼ í•¨
  - ex. `np.array([[len(weekdays_data) - 1]])` 
    - "ë§ˆì§€ë§‰ ë‚ ì˜ ì¸ë±ìŠ¤(ì˜ˆ: [[30]])"ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜í•œ ê²ƒ


## ğŸ”¹ ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)
ì—¬ëŸ¬ ê°œì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree) ë¥¼ í•™ìŠµì‹œì¼œ, ê·¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ íˆ¬í‘œ(Voting) ë˜ëŠ” í‰ê· (Avg.) í•˜ëŠ” ì•™ìƒë¸” í•™ìŠµ(Ensemble Learning) ê¸°ë²•

-> ë¶„ë¥˜(Classification)ì™€ íšŒê·€(Regression) ë¬¸ì œ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥

-> ê³¼ì í•©(Overfitting)ì— ê°•í•˜ê³ , ë³€ìˆ˜ ì¤‘ìš”ë„(Feature Importance)ë¥¼ í•´ì„í•˜ê¸° ì‰¬ì›€

### ğŸŒ² ë™ì‘ ì›ë¦¬
1. Bootstrap Sampling (ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§)
    - ì›ë³¸ ë°ì´í„°ì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•˜ë©° ëœë¤í•˜ê²Œ ìƒ˜í”Œë§
    - ê° íŠ¸ë¦¬(Tree)ëŠ” ë‹¤ë¥¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµë¨

2. Random Feature Selection (ë¬´ì‘ìœ„ íŠ¹ì„± ì„ íƒ)
    - ê° ë…¸ë“œë¥¼ ë¶„í• í•  ë•Œ ì „ì²´ íŠ¹ì„±ì´ ì•„ë‹Œ ì¼ë¶€ íŠ¹ì„±ë§Œ ë¬´ì‘ìœ„ë¡œ ê³ ë ¤
    - íŠ¸ë¦¬ ê°„ ë‹¤ì–‘ì„±ì„ ë†’ì—¬ ê³¼ì í•© ë°©ì§€

3. Ensemble (ì•™ìƒë¸”)
    - ì—¬ëŸ¬ íŠ¸ë¦¬ì˜ ê²°ê³¼ë¥¼ **í‰ê· (íšŒê·€)** ë˜ëŠ” **íˆ¬í‘œ(ë¶„ë¥˜)** ë¡œ ê²°í•©

### ì½”ë“œ ì˜ˆì‹œ
```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# 1. ë…ë¦½ë³€ìˆ˜(X): ì‹œê°„ëŒ€ë³„ êµí†µëŸ‰ / ì¢…ì†ë³€ìˆ˜(y): í˜¼ì¡ ì—¬ë¶€
X = df.loc[:, '0ì‹œ':'23ì‹œ']
y = df['í˜¼ì¡']

# 2. ë°ì´í„° ë¶„ë¦¬ (í›ˆë ¨ 70%, í…ŒìŠ¤íŠ¸ 30%)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 3. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# 4. ì˜ˆì¸¡
predicted = model.predict(X_test)

# 5. ì •í™•ë„ í‰ê°€
accuracy = model.score(X_test, y_test)
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •í™•ë„: {accuracy:.2f}")

# í…ŒìŠ¤íŠ¸ ì…‹ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸
X_first = X_test.iloc[[0]]
date_first = df.loc[0, 'ì¼ì']

# ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ë¹„êµ
actual = y_test.iloc[0]
pred = model.predict(X_first)[0]

print(f"ì„ íƒëœ ë‚ ì§œ: {date_first}")
print(f"ì‹¤ì œ í˜¼ì¡ ì—¬ë¶€: {'í˜¼ì¡' if actual else 'ë¹„í˜¼ì¡'}")
print(f"ì˜ˆì¸¡ í˜¼ì¡ ì—¬ë¶€: {'í˜¼ì¡' if pred else 'ë¹„í˜¼ì¡'}")
```
- RandomForestClassifier
  - ì£¼ìš” íŒŒë¼ë¯¸í„°
    - `n_estimators` : ìƒì„±í•  íŠ¸ë¦¬(Decision Tree) ê°œìˆ˜ (ê¸°ë³¸ê°’ 100)
    - `max_depth` : íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ ì œí•œ (ê¸°ë³¸ê°’ None(ìë™))
    - `min_samples_split` : ë…¸ë“œë¥¼ ë¶„í• í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’ 2)
    - `min_samples_leaf` : ë¦¬í”„ ë…¸ë“œì— ìˆì–´ì•¼ í•˜ëŠ” ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’ 1)
    - `max_features` : ê° íŠ¸ë¦¬ ë¶„í•  ì‹œ ê³ ë ¤í•  íŠ¹ì„±ì˜ ê°œìˆ˜ (ê¸°ë³¸ê°’ "sqrt" (ë¶„ë¥˜ì¼ ë•Œ))
    - `random_state` : ëœë¤ ì‹œë“œ ê³ ì • (ê¸°ë³¸ê°’ None)

### ì •ë¦¬
| í•­ëª©     | ì„¤ëª…                                      |
| ------ | --------------------------------------- |
| ëª¨ë¸ ìœ í˜•  | ì•™ìƒë¸” ê¸°ë°˜ ì§€ë„í•™ìŠµ(ë¶„ë¥˜/íšŒê·€ ëª¨ë‘ ê°€ëŠ¥)                |
| í•™ìŠµ êµ¬ì¡°  | ë‹¤ìˆ˜ì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree) ê²°í•©            |
| íŠ¹ì§• ì„ íƒ  | ê° íŠ¸ë¦¬ë§ˆë‹¤ ëœë¤í•œ íŠ¹ì„±(feature) ì¡°í•© ì‚¬ìš©            |
| ì˜ˆì¸¡ ë°©ì‹  | ë¶„ë¥˜: ë‹¤ìˆ˜ê²° íˆ¬í‘œ(Voting)<br>íšŒê·€: í‰ê· (Averaging) |
| í•´ì„ ê°€ëŠ¥ì„± | ê° ë³€ìˆ˜ë³„ `feature_importances_`ë¡œ ì¤‘ìš”ë„ í™•ì¸ ê°€ëŠ¥ |


## ğŸ”¹ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (í‘œì¤€í™”, Standardization)
ê° íŠ¹ì„±(feature)ì˜ ê°’ì´ ì„œë¡œ ë‹¤ë¥¸ ë²”ìœ„ë‚˜ ë‹¨ìœ„ë¥¼ ê°€ì§ˆ ë•Œ, í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë§ì¶”ëŠ” ì •ê·œí™” ê¸°ë²•

-> ëª¨ë¸ì´ **ëª¨ë“  íŠ¹ì„±ì„ ë™ì¼í•œ ê¸°ì¤€ì—ì„œ í•™ìŠµ**í•˜ë„ë¡ ë§Œë“¤ì–´ì¤Œ

-> íŠ¹íˆ **ê±°ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜(KNN, SVM, PCA ë“±)** ì—ì„œ í•„ìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©

### í•„ìš”í•œ ì´ìœ 
| ìƒí™©                 | ë¬¸ì œì                            | í‘œì¤€í™” íš¨ê³¼        |
| ------------------ | ----------------------------- | ------------- |
| íŠ¹ì„±ë§ˆë‹¤ ë‹¨ìœ„ê°€ ë‹¤ë¦„        | ì˜ˆ: êµí†µëŸ‰(ë‹¨ìœ„: ëŒ€) vs ì†ë„(ë‹¨ìœ„: km/h) | ë‹¨ìœ„ ì°¨ì´ ì œê±°      |
| í•œ íŠ¹ì„±ì˜ ê°’ ë²”ìœ„ê°€ ë§¤ìš° í¼   | ëª¨ë¸ì´ í° ê°’ì— ë” ë¯¼ê°í•´ì§               | ìŠ¤ì¼€ì¼ì„ ë™ì¼í•˜ê²Œ ë§ì¶°ì¤Œ |
| ê±°ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ (KNN ë“±) | ê±°ë¦¬ ê³„ì‚° ì‹œ í° ê°’ì´ ì§€ë°°ì               | ëª¨ë“  ì¶•ì—ì„œ ê· í˜• ìœ ì§€  |

### âœ… ì˜ˆì‹œ: KNN ëª¨ë¸ í•™ìŠµ ì „ í‘œì¤€í™”
```python
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# 1. ë°ì´í„° ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 2. StandardScaler ê°ì²´ ìƒì„±
scaler = StandardScaler()

# 3. í›ˆë ¨ ë°ì´í„°ë¡œ ê¸°ì¤€(í‰ê· , í‘œì¤€í¸ì°¨) í•™ìŠµ í›„ ë³€í™˜
X_train_scaled = scaler.fit_transform(X_train)

# 4. ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜
X_test_scaled = scaler.transform(X_test)

# 5. KNN ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)
```
- `fit()` : í›ˆë ¨ ë°ì´í„°ì˜ í‰ê· (Î¼)ê³¼ í‘œì¤€í¸ì°¨(Ïƒ) ê³„ì‚° -> ìŠ¤ì¼€ì¼ ê¸°ì¤€ í•™ìŠµ
- `transform()` : ê³„ì‚°ëœ Î¼, Ïƒë¡œ ë°ì´í„° ë³€í™˜ -> ì‹¤ì œ í‘œì¤€í™” ì ìš©
- `fit_transform()` : `fit()` + `transform()`ì„ í•œ ë²ˆì— ìˆ˜í–‰(í›ˆë ¨ ë°ì´í„°ìš©)


## ğŸ”¹ ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
### í˜¼ë™ í–‰ë ¬ (Confusion Matrix)
**ì‹¤ì œ í´ë˜ìŠ¤ì™€ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¹„êµ**í•˜ì—¬, ëª¨ë¸ì´ ì–´ë–¤ ì˜¤ë¥˜ë¥¼ ì£¼ë¡œ ë²”í–ˆëŠ”ì§€ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆëŠ” í‘œ

|    ì‹¤ì œ â†“ / ì˜ˆì¸¡ â†’   |               Positive (1)              |               Negative (0)              |
| :--------------: | :-------------------------------------: | :-------------------------------------: |
| **Positive (1)** |  **TP (True Positive)**<br>ì‹¤ì œ 1 â†’ ì˜ˆì¸¡ë„ 1 | **FN (False Negative)**<br>ì‹¤ì œ 1 â†’ ì˜ˆì¸¡ì€ 0 |
| **Negative (0)** | **FP (False Positive)**<br>ì‹¤ì œ 0 â†’ ì˜ˆì¸¡ì€ 1 |  **TN (True Negative)**<br>ì‹¤ì œ 0 â†’ ì˜ˆì¸¡ë„ 0 |

- ì½”ë“œ
  ```python
  from sklearn.metrics import confusion_matrix

  conf_matrix = confusion_matrix(y_test, y_pred)
  print(conf_matrix)
  ```

### ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ (Precision, Recall, F1-score)
| ì§€í‘œ   | ìˆ˜ì‹             | ì˜ë¯¸       | ì¢‹ì€ ëª¨ë¸ ê¸°ì¤€    |
| ------ | ---------------- | ------------ | ----------- |
| **ì •ë°€ë„ (Precision)**  | $\text{Precision} = \frac{TP}{TP + FP}$ | ëª¨ë¸ì´ "í˜¼ì¡"ì´ë¼ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ í˜¼ì¡ì¸ ë¹„ìœ¨ | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ     |
| **ì¬í˜„ìœ¨ (Recall)**     | $\text{Recall} = \frac{TP}{TP + FN} $ | ì‹¤ì œ í˜¼ì¡ì¸ ë°ì´í„° ì¤‘ ëª¨ë¸ì´ ì˜ ë§ì¶˜ ë¹„ìœ¨     | ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ     |
| **F1 ì ìˆ˜ (F1-score)** | $F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$ | ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê·               | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ |
| **ì •í™•ë„ (Accuracy)**   | $Accuracy = \frac{TP + TN}{TP + TN + FP + FN} $   | ì „ì²´ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨             | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ |

- ì½”ë“œ
  ```python
  from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

  # í˜¼ë™ í–‰ë ¬
  conf_matrix = confusion_matrix(y_test, y_pred)
  print("í˜¼ë™ í–‰ë ¬ (Confusion Matrix):")
  print(conf_matrix)

  # ë¶„ë¥˜ ë¦¬í¬íŠ¸ (Precision, Recall, F1)
  report = classification_report(y_test, y_pred)
  print("\në¶„ë¥˜ ì„±ëŠ¥ í‰ê°€ (Classification Report):")
  print(report)

  # ì •í™•ë„
  accuracy = accuracy_score(y_test, y_pred)
  print(f"\nëª¨ë¸ ì •í™•ë„(Accuracy): {accuracy:.2f}")
  ```


## ğŸ”¹ ë‹¨í•­ íšŒê·€ vs ë‹¤í•­ íšŒê·€
### ë‹¨í•­ íšŒê·€ (Simple Linear Regression)
í•˜ë‚˜ì˜ ë…ë¦½ ë³€ìˆ˜(X)ì™€ ì¢…ì† ë³€ìˆ˜(y) ê°„ì˜ **ì§ì„ (linear) ê´€ê³„**ë¥¼ í•™ìŠµí•˜ëŠ” íšŒê·€ ëª¨ë¸

- ìˆ˜ì‹
  - $y = w_0 + w_1x $
  - ì„¤ëª…
    | ê¸°í˜¸          | ì˜ë¯¸           |
    | ----------- | ------------ |
    | $\hat{y}$ | ì˜ˆì¸¡ê°’          |
    | $w_0$     | ì ˆí¸ (bias)    |
    | $w_1$     | ê¸°ìš¸ê¸° (weight) |
    | $x$      | ë…ë¦½ ë³€ìˆ˜        |


- ì½”ë“œ ì˜ˆì‹œ
  ```python
  linear_model = LinearRegression()
  linear_model.fit(X_train, y_train)
  y_pred_linear = linear_model.predict(X_test)

  mse_linear = mean_squared_error(y_test, y_pred_linear)
  rmse_linear = np.sqrt(mse_linear)
  print(f"ë‹¨í•­ íšŒê·€ RMSE: {rmse_linear:.2f}")
  ```

### ë‹¤í•­ íšŒê·€ (Polynomial Regression)
ë‹¨í•­ íšŒê·€ì—ì„œ í™•ì¥ëœ í˜•íƒœë¡œ, **ë¹„ì„ í˜• ê´€ê³„(ê³¡ì„ í˜• ë°ì´í„°)** ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŒ

- ìˆ˜ì‹
  - $y = w_0 + w_1x + w_2x^2 + w_3x^3 + \dots + w_nx^n$
  - ì„¤ëª…
    | ê¸°í˜¸      | ì˜ë¯¸                    |
    | ------- | --------------------- |
    | $n$   | ë‹¤í•­ì‹ì˜ ì°¨ìˆ˜ (ì˜ˆ: 2ì°¨, 3ì°¨ ë“±) |
    | $w_i$ | ê° í•­ì˜ ê°€ì¤‘ì¹˜              |
    | $x^i$ | ë…ë¦½ ë³€ìˆ˜ì˜ iì°¨ í•­           |

- ì½”ë“œ ì˜ˆì‹œ
  ```python
  from sklearn.preprocessing import PolynomialFeatures

  poly = PolynomialFeatures(degree=2)
  X_poly = poly.fit_transform(days)

  X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(
      X_poly, traffic_at_8am, test_size=0.2, random_state=42
  )

  poly_model = LinearRegression()
  poly_model.fit(X_train_poly, y_train_poly)

  y_pred_poly = poly_model.predict(X_test_poly)
  mse_poly = mean_squared_error(y_test_poly, y_pred_poly)
  rmse_poly = np.sqrt(mse_poly)

  print(f"ë‹¤í•­ íšŒê·€ RMSE: {rmse_poly:.2f}")
  ```

### ë‹¨í•­ vs ë‹¤í•­ íšŒê·€ ë¹„êµ
| í•­ëª©     | **ë‹¨í•­ íšŒê·€ (Linear Regression)** | **ë‹¤í•­ íšŒê·€ (Polynomial Regression)** |
| ------ | ----------------------------- | --------------------------------- |
| ê´€ê³„ í˜•íƒœ  | ì§ì„  ê´€ê³„ (ì„ í˜•)                    | ê³¡ì„  ê´€ê³„ (ë¹„ì„ í˜•)                       |
| ë³µì¡ë„    | ë‚®ìŒ (ë‹¨ìˆœ)     | ë†’ìŒ (ì°¨ìˆ˜ â†‘ ì‹œ ë³µì¡ë„ â†‘)  |
| í‘œí˜„ë ¥    | ì œí•œì  (ì§ì„ ë§Œ í‘œí˜„)     | ìœ ì—°í•¨ (ê³¡ì„  íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥)   |
| ê³¼ì í•© ìœ„í—˜ | ë‚®ìŒ     | ë†’ìŒ (ì°¨ìˆ˜â†‘ ì‹œ overfitting ìœ„í—˜)         |
| ì‚¬ìš© ì˜ˆì‹œ  | ì˜¨ë„ â†— â†’ íŒë§¤ëŸ‰ â†—  | ìš”ì¼ë³„ êµí†µëŸ‰ì²˜ëŸ¼ ì£¼ê¸°ì„± ìˆëŠ” ë°ì´í„°              |
| ìˆ˜ì‹ ì˜ˆì‹œ  | $y = w_0 + w_1x $    |  $y = w_0 + w_1x + w_2x^2$   |

### ì˜ˆì¸¡ ì˜ˆì‹œ ì½”ë“œ: 5ë²ˆ ì‹¤ìŠµ
```python
# ë‹¨í•­ íšŒê·€ ì˜ˆì¸¡
predicted_traffic_linear = linear_model.predict(np.array([[len(weekdays_data)]]))[0]

# ë‹¤í•­ íšŒê·€ ì˜ˆì¸¡
last_day_index_poly = poly.transform(np.array([[len(weekdays_data)]]))
predicted_traffic_poly = poly_model.predict(last_day_index_poly)[0]

print(f"ë‹¨í•­ íšŒê·€ ì˜ˆì¸¡: {predicted_traffic_linear:.2f}")
print(f"ë‹¤í•­ íšŒê·€ ì˜ˆì¸¡: {predicted_traffic_poly:.2f}")
```
- ë‹¨í•­ íšŒê·€ ì˜ˆì¸¡
  - `len(weekdays_data)` : ë°ì´í„°ì…‹ì˜ ì „ì²´ ì¼ìˆ˜ (ì˜ˆ: 31ì¼ì´ë©´ ë§ˆì§€ë§‰ ë‚ ì˜ ì¸ë±ìŠ¤ëŠ” 31)
  - `np.array([[len(weekdays_data)]])` : scikit-learnì˜ `predict()` í•¨ìˆ˜ëŠ” **2ì°¨ì› ë°°ì—´ í˜•íƒœì˜ ì…ë ¥ì„ ìš”êµ¬**í•˜ë¯€ë¡œ, ë‹¨ì¼ ì¸ë±ìŠ¤ ê°’ì„ `[[ ]]` ì´ì¤‘ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë„˜ê²¨ì¤˜ì•¼í•¨
  - `linear_model.predict(...)` : í•™ìŠµëœ ë‹¨í•­ íšŒê·€ ëª¨ë¸ì´ ì…ë ¥ê°’(ë§ˆì§€ë§‰ ë‚  ì¸ë±ìŠ¤)ì— ëŒ€í•œ ì˜ˆì¸¡ êµí†µëŸ‰(8ì‹œ ê¸°ì¤€) ì„ ê³„ì‚°
  - `[0]` : predict()ì˜ ê²°ê³¼ëŠ” ë°°ì—´ í˜•íƒœì´ë¯€ë¡œ, ì‹¤ì œ ì˜ˆì¸¡ê°’ì„ êº¼ë‚´ê¸° ìœ„í•´ [0]ì„ ë¶™ì„
  - ì¦‰, ë‹¨í•­ íšŒê·€ ëª¨ë¸ì´ í•™ìŠµí•œ "ìš”ì¼ì— ë”°ë¥¸ ì„ í˜• ì¶”ì„¸"ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆì§€ë§‰ ë‚ ì˜ 8ì‹œ êµí†µëŸ‰ì„ ì§ì„  ê´€ê³„ë¡œ ì˜ˆì¸¡

- ë‹¤í•­ íšŒê·€ ì˜ˆì¸¡
  - `poly.transform()` : `x, xÂ², xÂ³ â€¦` ê°™ì€ ë‹¤í•­ í•­ë“¤ì„ ìë™ìœ¼ë¡œ ìƒì„±í•´ì¤Œ
    - ì˜ˆë¥¼ ë“¤ì–´, `degree=2`ë¼ë©´ ì…ë ¥ê°’ `x = 31 â†’ [1, 31, 31Â²]` ë¡œ ë³€í™˜ë¨
  - `poly_model.predict(...)` : í•™ìŠµëœ ë‹¤í•­ íšŒê·€ ëª¨ë¸ì´ ë³€í™˜ëœ ë‹¤í•­ì‹ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë§ˆì§€ë§‰ ë‚ ì˜ 8ì‹œ êµí†µëŸ‰ì„ ê³¡ì„  í˜•íƒœì˜ ê´€ê³„ë¡œ ì˜ˆì¸¡í•¨
  - `[0]` : ì˜ˆì¸¡ê°’ì„ ë°°ì—´ì—ì„œ êº¼ë‚´ì„œ ì‹¤ìˆ˜(float) í˜•íƒœë¡œ ì¶œë ¥
  - ì¦‰, ë‹¤í•­ íšŒê·€ ëª¨ë¸ì€ ë¹„ì„ í˜•(ê³¡ì„ ) ê´€ê³„ë¥¼ ë°˜ì˜í•˜ì—¬ êµí†µëŸ‰ì˜ ë³€í™” íŒ¨í„´(ì˜ˆ: ì£¼ì¤‘ ìƒìŠ¹ â†’ ì£¼ë§ í•˜ë½ ë“±)ì„ ë” ì •êµí•˜ê²Œ ì˜ˆì¸¡

