# 데이터 분석을 위한 머신러닝 알고리즘 1
## 선형변환 및 행렬 연산
- 머신러닝과 데이터 분석에서 행렬의 중요성
  - 데이터를 효율적으로 표현하고, 모델을 학습시키기 위해 행렬 사용

  1. 데이터 표현
      - 대부분의 데이터(이미지, 텍스트, 수치 데이터)는 행렬 형태로 저장
  2. 머신러닝 알고리즘
      - 선형 회귀, 로지스틱 회귀, 신경망 등에서 가중치와 데이터 처리 연산 및 데이터 변형, 차원 축소를 위해 행렬 사용
  3. 최적화 및 학습 과정
      - 경사 하강법 등 최적화 알고리즘 및 딥러닝에서 미분과 행렬 연산 사용

### 행렬(Matrix)
숫자가 행(row)과 열(column)로 배열된 2차원 구조

```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
print(A)
```
![alt text](image.png)
![alt text](image-1.png)

- 행렬 덧셈과 스칼라 곱
  - 행렬 간 덧셈
    - 같은 크기의 행렬끼리 요소별로 더함
  - 스칼라 곱
    - 행렬의 각 요소에 숫자를 곱함
    ```python
    import numpy as np

    A = np.array([[1, 2], [3, 4]])  # 2x2 행렬
    B = np.array([[5, 6], [7, 8]])  # 2x2 행렬

    C = A + B  # 행렬 덧셈
    D = 2 * A  # 스칼라 곱

    print(C)
    print("=" * 50)
    print(D)
    ```
    ![alt text](image-2.png)

- 행렬 곱(Matrix Multiplication)
  - 두 개의 행렬을 곱하는 연산 
  - `A (m x n) * B (n x p)` -> 결과 행렬 C (m x p)
  - 첫 번째 행렬(A)의 열 개수와 두 번째 행렬(B)의 행 개수가 같아야 곱셈 가능
  - ex. `A (2*3) * B (3*2)` -> 결과 행렬 C (2*2)
    ```python
    import numpy as np

    A = np.array([[1, 2, 3], [4, 5, 6]])
    B = np.array([[7, 8], [9, 10], [11, 12]])

    C = np.dot(A, B)  # 행렬 곱
    print(C)
    # [[ 58  64]
    # [139 154]]
    ```
    ![alt text](image-3.png)
  - `1xn` 행렬과 `nx1` 행렬의 곱으로 표현하는 형태를 벡터의 **내적**이라 볼 수 있음(결과는 스칼라 나옴)
  - 내적의 의미
    - 두 벡터가 얼마나 같은 방향을 가리키는지를 측정
    - 즉, **유사도(Similarity)**를 수치로 나타낸 것
    - 방향이 같으면 크고, 직각이면 0, 반대면 음수
  - `.dot()`
    - 벡터 내적(Dot Product), **행렬 곱(Matrix Multiplication)**을 계산하는 함수
    - 벡터끼리 연산: 내적 결과 = 스칼라(숫자) -> 유사도(방향) 의미함
    - 행렬끼리 연산: 행렬 곱 결과 = 새로운 행렬 -> 선형 변환 결과를 의미함
    - `@` 연산자와 동일 (`A @ B == np.dot(A, B)`)

- 머신러닝에서 행렬 곱 활용
  - 선형 회귀 (Linear Regression)
    - 모델의 가중치와 입력 데이터($x$)의 행렬 곱을 통해 예측값 계산
  - 신경망 (Neural Networks)
    - 각 레이어에서 가중치와 입력 데이터의 행렬 곱 수행

- 전치 행렬 (Transpose Matrix)
  - 행과 열을 바꾸는 연산
  - 행렬 A (m x n)의 전치 행렬 -> $A^T$ (n x m)
  - 원소: $A^T[i, j] = A[j, i]$
    ```python
    import numpy as np

    # 원래 행렬 A (2x3)
    A = np.array([[1, 2, 3],
                  [4, 5, 6]])

    # 전치 행렬 A^T (3x2)
    A_T = A.transpose()

    print("원래 행렬 A:")
    print(A)
    print("\n전치 행렬 A^T:")
    print(A_T)
    ```
    ![alt text](image-4.png)

### 선형 변환 (Linear Transformation)
공간을 변형시키는 규칙(한 벡터를 다른 벡터 공간으로 변환하는 과정)

- 크기(배율을 조절), 회전, 대칭, 변형(왜곡) 같은 작업을 위해 사용
  - 모든 원소에 어떠한 값을 **더하는** 변환은 선형변환XX
    - 즉, 평행이동은 선형변환XX
    - 선형변환은 항상 원점(0)을 원점으로 보내야 하지만, 평행 이동은 원점을 다른 위치로 옮겨버림
    - ex. `T(x)= x + [3,3]` -> 원점이 `0 -> [3,3]`으로 움직였으므로 선형변환XX
- 행렬 연산(특히 행렬 곱)을 통해 구현
- 선형 변환은 공간의 점들을 일정한 규칙에 따라 이동시키는 것
- 데이터 분석에서 차원을 줄여서 중요한 정보만 남길 때 사용가능

  ![alt text](image-5.png)
  - 좌표 하나를 움직였다기엔 좌표 각각이 다름
  - 공간 전체를 정해진 규칙에 따라 변형한 것!

- 선형 변환의 대표 예시
  - 크기 변경 (Scaling)
    - 벡터를 일정 배율로 확대 또는 축소하는 변환
    - 방향은 그대로, 길이만 변환하는 것
    - 스칼라 곱으로 표현 가능
      ```python
      import numpy as np

      A = np.array([[2, 0], [0, 2]])
      v = np.array([3, 4])

      result = np.dot(A, v)
      print(result)
      # [6, 8]
      ```
      ![alt text](image-6.png)


## Regression
### 회귀(Regression)
연속적인 숫자 값을 예측하는 지도 학습의 한 유형

- 목적
  - **입력 데이터를 기반으로 특정 연속적 목표 값(출력)을 예측**하는 것

- 회귀 문제의 목표
  - 입력 변수(특징)를 사용하여 출력 값의 추세를 학습하고, 이를 기반으로 새로운 데이터에 대해 예측을 수행
    
- 회귀 활용 예시
  - 주택 가격 예측
    - 집의 크기, 위치, 연도 등의 정보를 바탕으로 가격을 예측
  - 주식 시장 분석
    - 과거 주가 데이터를 바탕으로 미래 주가를 예측
  - 날씨 예측
    - 기온, 습도, 풍속 등의 데이터를 사용해 내일의 기온을 예측

- 회귀와 분류의 차이점
  - 회귀: 연속적인 값을 예측하는 문제(ex. 온도, 가격).
  - 분류: 이산적인 범주를 예측하는 문제(ex. 스팸/비스팸, 고양이/개)

- 회귀 문제의 예시
  - 주택 가격 예측
    - 주택의 크기, 방 개수, 위치 등의 특징(입력 변수)을 기반으로 주택의 가격(출력 변수)을 예측하는 문제
    - ex. 100m^2의 아파트가 5억 원일 가능성을 예측
  - 주식 시장 분석
    - 과거 주가 데이터, 거래량, 경제 지표 등을 바탕으로 미래 주가를 예측하는 문제
    - ex. 주식 X의 다음 달 가격을 예측
  - 날씨 예측
    - 기온, 습도, 풍속 등의 환경 데이터를 바탕으로 다음 날의 기온을 예측
    - ex. 내일의 최고 기온을 25°C로 예측
  - 자동차 연비 예측
    - 자동차의 배기량, 마력, 중량 등의 데이터를 기반으로 연비를 예측
    - ex. 특정 차량이 1리터당 12km를 갈 가능성 예측
  - 판매량 예측
    - 과거 판매 기록, 계절, 마케틸 투자 등을 통해 미래의 판매량을 예측
    - ex. 특정 제품이 다음 달에 1000개 판매될 가능성 예측

※ 머신러닝이 가진 문제 풀이 방식

-> `x`를 `f(x)`에 넣어서 `y`를 예측($\hat{y}$)

- 입력 변수 (Features, 독립 변수)
  - 회귀 문제에서 입력 변수는 **예측에 필요한 다양한 특성**을 의미
  - 입력 변수는 하나일 수도 있고, 여러 개일 수도 있음
  - 예시
    - 주택 가격 예측에서의 입력 변수: 주택 크기, 위치, 방 개수 등
    - 자동차 연비 예측에서의 입력 변수: 배기량, 중량, 마력 등

- 출력 변수 (Target, 종속 변수)
  - 회귀 문제의 출력은 **예측하고자 하는 연속적인 값**
  - 출력 변수는 모델이 학습하여 예측하는 목표 값
    - 주택 가격 예측에서의 출력 변수: 예측된 주택의 가격
    - 자동차 연비 예측에서의 출력 변수: 1리터당 이동할 수 있는 거리(km/L)

- 모델의 목적
  - 입력 변수(특성)와 출력 변수(목표 값) 간의 관계를 학습해, 새로운 입력이 주어졌을 때 적절한 출력 값을 예측
  - 예시: 주택의 크기와 위치 정보를 입력으로 받아 주택 가격을 예측


- 회귀
  - Training Data를 이용해서 데이터의 특성과 상관 관계 등을 파악하고, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 연속적인 (숫자) 값으로 예측하는 것
  - ex. 공부시간과 시험 성적 관계 등

    ![alt text](image-7.png)

- 회귀의 종류
  - 단순 선형회귀
    - 한 개의 독립변수(X)로 종속변수(Y)를 예측  
    - 선형적(1차항 이상의 연산없음)
    - 독립변수와 종속변수 간의 관계를 직선(1차 함수)으로 표현 
  - 다중 선형회귀
    - 두 개 이상의 독립변수로 종속변수를 예측
    - 독립변수들이 종속변수에 선형적으로 영향을 준다고 가정
    - 2차원에서는 직선, 3차원 이상에서는 평면(Hyperplane)으로 표현  

    ![alt text](image-8.png)
  - 다항 회귀
    - 독립변수의 2차항 이상을 포함해 종속변수를 예측
    - 실제 데이터가 직선보다 곡선에 더 잘 맞을 때 사용
    - 다항식으로 확장하여 비선형 관계도 표현 가능

      ![alt text](image-9.png)

### 선형 회귀
입력 변수(특성)와 출력 변수(목표 값) 간의 **선형 관계를 가정**하여 예측하는 모델

-> 데이터가 직선으로 표현될 수 있을 때, 가장 간단하고 직관적인 방법

- 모델 수식
  - $y = wx + b$
    - `y` : 출력 값(예측 값)
    - `x` : 입력 값
    - `w` : 가중치(기울기)
    - `b` : 절편

    ![alt text](image-11.png)

- 예측 방법
  - 주어진 데이터를 기반으로 최적의 직선(회귀선)을 찾아 입력 값에 대한 출력 값을 예측
  - 모델은 오차를 최소화하는 방향으로 직선의 기울기 w와 절편 b를 학습

- 장점
  - 해석이 용이하고 계산이 빠름
  - 비교적 단순한 문제에 대해 좋은 성능을 발휘

- 단점
  - 입력 변수와 출력 변수 간의 관계가 비선형적일 경우 성능이 저하될 수 있음
  - 과적합의 위험이 있으며, 다중 공선성이 있을 때 문제가 발생할 수 있음
    - ※ 다중 공선성: 회귀분석에서 독립변수들 간에 강한 선형 상관관계가 존재하는 현상

- 사용 예시
  - 주택 가격 예측 
    - ex. 주택 크기에 따른 가격 예측
  - 광고 비용과 판매량의 관계 분석

- 코드
  ```python
  # 필수 라이브러리 불러오기
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LinearRegression
  from sklearn.metrics import mean_squared_error, r2_score
  from sklearn.datasets import fetch_california_housing
  import pandas as pd

  # 캘리포니아 주택 가격 데이터셋 로드
  data = fetch_california_housing()
  X = pd.DataFrame(data.data, columns=data.feature_names)  # 독립 변수(특징 데이터)
  y = data.target  # 종속 변수 (중간 주택 가격)

  # 주택 크기(House Size) 특성만 선택
  X_house_size = X[['HouseAge']]

  # 데이터를 학습용 및 테스트용 데이터로 분할
  X_train, X_test, y_train, y_test = train_test_split(
      X_house_size, y, test_size=0.3, random_state=42
  )

  # 선형 회귀 모델 학습
  model = LinearRegression()
  model.fit(X_train, y_train)

  # 예측 수행
  y_pred = model.predict(X_test)

  # 모델 평가
  mse = mean_squared_error(y_test, y_pred)  # 평균 제곱 오차 계산
  r2 = r2_score(y_test, y_pred)  # 결정 계수(R²) 계산

  # 결과 출력
  print(f"Mean Squared Error (MSE): {mse:.2f}")
  print(f"R² Score: {r2:.4f}")
  ```
  ![alt text](image-13.png)
  - train_test_split
    ```python
    from sklearn.model_selection import train_test_split
    ```
    - 데이터를 학습용(train)과 테스트용(test)으로 분할하는 함수
    - 주요 파라미터
      - `X`, `y`: 입력 데이터와 레이블(목표값)
      - `test_size`: 테스트 데이터 비율 
      - `random_state`: 랜덤 시드(재현성을 위해 고정)

  - LinearRegression
    ```python
    from sklearn.linear_model import LinearRegression
    ```
    - 선형 회귀 모델 클래스
    - 입력 변수(특성)와 출력 변수 간의 선형 관계를 학습
    - 주요 메서드
      - `.fit(X_train, y_train)` : 모델 학습
      - `.predict(X_test)` : 학습된 모델로 새로운 데이터 예측
      - `.coef_` : 회귀 계수(가중치, w 값)
      - `.intercept_` : 절편(bias, b 값)

  - mean_squared_error
    ```python
    from sklearn.metrics import mean_squared_error
    ```
    - 회귀 모델 평가 지표 중 하나
    - 실제값(y_true)과 예측값(y_pred)의 차이 제곱 평균
    - 값이 작을수록 모델의 예측이 실제와 가깝다
  
  - r2_score
    ```python
    from sklearn.metrics import r2_score
    ```
    - 결정계수(R², Coefficient of Determination)
    - 모델이 데이터를 얼마나 잘 설명하는지 비율로 나타냄


### 다중 회귀
여러 개의 입력 변수(특성)를 사용하여 출력 변수를 예측하는 선형 회귀 모델
  
-> 단일 입력 변수를 사용하는 선형 회귀와 달리, 다중 회귀는 다수의 변수를 고려하여 보다 정확한 예측 가능

- 모델의 수식
  - $y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$
    - `y` : 예측 값
    - $x_1, x_2, ... , x_n$ : 입력 변수들
    - $w_1, w_2, ... , w_n$ : 각 특성의 가중치
    - `b` : 절편

- 예측 방법
  - 각 입력 변수에 해당하는 가중치를 곱한 수, 그 값을 모두 더하여 예측 값을 계산
  - 모델은 모든 입력 변수의 영향을 고려하여 오차를 최소화하는 방향으로 가중치와 절편을 학습

- 코드
  ```python
  # 필요한 라이브러리 임포트
  from sklearn.model_selection import train_test_split
  from sklearn.linear_model import LinearRegression
  from sklearn.metrics import mean_squared_error, r2_score
  from sklearn.datasets import fetch_california_housing
  import pandas as pd
  import matplotlib.pyplot as plt

  # 캘리포니아 주택 가격 데이터셋 로드
  data = fetch_california_housing()
  X = pd.DataFrame(data.data, columns=data.feature_names)  # 특성 데이터
  y = data.target  # 목표 변수 (중간 주택 가격)

  # 데이터를 학습용 및 테스트용으로 분할
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

  # 선형 회귀 모델 학습
  model = LinearRegression()
  model.fit(X_train, y_train)

  # 예측 수행
  y_pred = model.predict(X_test)

  # 모델 평가
  mse = mean_squared_error(y_test, y_pred)  # 평균 제곱 오차 계산
  r2 = r2_score(y_test, y_pred)  # 결정 계수(R²) 계산

  # 결과 출력
  print(f"Mean Squared Error (MSE): {mse:.2f}")
  print(f"R² Score: {r2:.4f}")
  ```
  ![alt text](image-14.png)
  - 선형 회귀와 모델 훈련, 예측 과정은 동일함
    - `X`에 포함되는 **열의 개수**에만 차이가 있음
      ```python
      X = pd.DataFrame(data.data, columns=data.feature_names)  # 모든 독립변수 사용
      ```

※ 선형 회귀 vs 다중 회귀

![alt text](image-12.png)


### 다항 회귀
입력 변수(특성)와 출력 변수(목표 값) 간의 **비선형 관계**를 가정하여 예측하는 회귀 방법

-> 선형 회귀와 달리, 입력 변수의 거듭제곱을 포함한 **다항식을 사용하여 비선형 패턴을 학습**

- 장점
  - 데이터가 비선형적인 경우, 선형 회귀보다 더 나은 예측 성능을 보임
  - 다양한 비선형 패턴을 유연하게 모델링할 수 있음

- 단점
  - 과적합(Overfitting)의 위험이 큼
  - 특히 다항 차수가 높아질수록 모델이 복잡해져 과적합 가능성이 증가
  - 데이터가 부족할 경우, 모델이 안정적이지 않을 수 있음

- 사용 예시
  - 곡선 패턴이 있는 데이터 예측
    - ex. 온도 변화에 따른 전력 사용량 예측
  - 주택 가격 예측에서, 방 개수와 가격 간의 비선형 관계 모델링

- 모델 수식
  - $y = w_1x + w_2x^2 + w_3x^3 + ... + w_nx^n + b$
    - `y` : 예측 값
    - `x` : 입력 변수
    - $w_1, w_2, ... , w_n$ : 각 특성의 가중치
    - `b` : 절편

- 예측 방법
  - 입력 값 x의 거듭제곱을 사용해 곡선 형태의 관계를 모델링
  - 선형 회귀로는 설명할 수 없는 비선형 패턴을 더 잘 포착 가능

- 코드
  ```python
  # 필요한 라이브러리 임포트
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import PolynomialFeatures
  from sklearn.linear_model import LinearRegression
  from sklearn.metrics import mean_squared_error, r2_score
  from sklearn.datasets import fetch_california_housing
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt

  # 캘리포니아 주택 가격 데이터셋 로드
  data = fetch_california_housing()
  df = pd.DataFrame(data.data, columns=data.feature_names)  # 데이터를 DataFrame으로 변환
  df["MedHouseVal"] = data.target  # 목표 변수(주택 가격) 추가

  # 특정 특성 선택 (가구당 평균 방 개수)
  X = df[["AveRooms"]]  # 독립 변수(특징)
  y = df["MedHouseVal"]  # 종속 변수(주택 가격)

  # 데이터를 학습용 및 테스트용으로 분할
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

  # 다항 특성 변환 (차수=2)
  poly = PolynomialFeatures(degree=2)  # X를 [1, X, X^2] 형태로 변환
  X_train_poly = poly.fit_transform(X_train)
  X_test_poly = poly.transform(X_test)

  # 다항 회귀 모델 학습
  model = LinearRegression()
  model.fit(X_train_poly, y_train)

  # 예측 수행
  y_pred = model.predict(X_test_poly)

  # 모델 평가
  mse = mean_squared_error(y_test, y_pred)  # 평균 제곱 오차 계산
  r2 = r2_score(y_test, y_pred)  # 결정 계수(R²) 계산

  # 결과 출력
  print(f"Mean Squared Error (MSE): {mse:.2f}")
  print(f"R² Score: {r2:.4f}")
  ```
  ![alt text](image-16.png)

  - PolynomialFeatures
    ```python
    from sklearn.preprocessing import PolynomialFeatures
    poly = PolynomialFeatures(degree=2)
    ```
    - 입력 데이터 X를 다항식 특성으로 확장해줌
    - 파라미터
      - `degree`: 다항식 차수
      - `include_bias`: True면 상수항 1 추가 (기본값 True)
      - `interaction_only`: 상호작용 항만 포함할지 여부 (기본값 False)
    - 메서드
      - `.fit_transform(X)`: 학습 + 변환 (훈련 데이터에 사용)
      - `.transform(X)`: 변환만 수행 (테스트 데이터에 사용)
    - 예시
      ```python
      X = [[2], [3]]
      poly = PolynomialFeatures(degree=2)
      print(poly.fit_transform(X))
      # [[1, 2, 4], [1, 3, 9]]
      ```
  
  - fit_transform / transform 
    - `fit_transform`: **훈련 데이터**를 기준으로 변환 규칙을 학습(fit)한 뒤, 변환(transform)까지 수행
    - `transform` : 이미 학습된 규칙을 그대로 적용 (**테스트 데이터 변환할 때 사용**)

※ 선형 회귀 vs 다항 회귀

![alt text](image-15.png)


## 베이즈 정리, 나이브 베이즈
### 나이브 베이즈 분류
- 문제
  - 문제 정의: 10만개의 메일 중 스팸 메일과 정상 메일을 분류하고 싶다면?
  - 메일: 독립 사건으로 가정하는 텍스트 데이터
    - 첫번째 메일이 스팸이라해서 두번째 메일이 정상일 확률이 높다? XX -> 그래서 독립
  - 해결방안: 나이브 베이즈 분류 알고리즘 활용

- 나이브 베이즈 분류
  - 각 특징들이 **독립적(서로 영향을 미치지 않을 것)이라는 가정** 설정
  - 베이즈 정리(Bayes Rule)를 활용한 확률 통계학적 분류 알고리즘

- 베이즈 정리

  ![alt text](image-17.png)
  - 두 확률 변수의 사전 확률과 사후 확률사이의 관계를 나타내는 정리
  - 즉, 조건부 확률을 역으로 추정할 수 있게 해주는 공식
  - 이메일 문제
    - $P(A|B)$ : 단어 **B**(예: "할인")이 등장했을 때, 메일이 **스팸(A)** 일 확률 (우리가 구하고 싶은 값, 사후확률)
    - $P(B|A)$ : 메일이 스팸일 때, 그 안에 "할인"이라는 단어가 들어갈 확률
    - $P(A)$ : 스팸 메일이 전체 메일 중 차지하는 비율 (사전 확률, prior) 
    - $P(B)$ : "할인"이라는 단어가 전체 메일에서 등장할 확률
    - `A` : 메일이 스팸이다
    - `B` : 메일에 '할인'이라는 단어가 있다
      1. 스팸 메일 중 70%는 "할인" 단어를 포함 (P(B|A) = 0.7)  
      2. 전체 메일 중 40%는 스팸 (P(A) = 0.4)  
      3. 전체 메일 중 "할인" 단어가 나올 확률은 0.2 (P(B) = 0.2)
    - 베이즈 정리 적용
    
      $$
      P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)} 
      = \frac{0.7 \cdot 0.4}{0.2} 
      = 1.4 \quad (\text{> 1이므로 실제 값은 정규화 필요})
      $$
    - 정리
      - $P(B|A)$: 스팸일 때 "할인"이 포함될 확률 (조건부 확률 -> 관측하기 쉬움)  
      - **베이즈 정리**를 통해 역으로  
      - "할인"이 포함된 메일이 스팸일 확률 $P(A|B)$을 계산할 수 있음  
      - 따라서, 단어 출현 확률을 바탕으로 스팸 여부를 추정하는 데 활용 가능  

  - 문제 적용: 스팸 메일 분류
    1. 스팸 메일과 정상 메일의 단어를 체크
        - 스팸과 정상메일이 독립을 가정하고 있음을 반드시 기억
        - 
    2. 새로운 메일의 단어들에 대한 확률로 스팸 메일을 구분
        - $P(스팸 | 단어1, 단어2, 단어3...) > P(정상 | 단어1, 단어2, 단어3...)$ 이면 스팸
        - 동일한 단어들에 대해서 스팸/정상의 확률을 계산하는 것임
        - `P(스팸 | 단어1, 단어2..)`, `P(정상 | 단어1, 단어2..)`는 최종적으로 분류하고 싶은 조건부 확률
        - 즉, 단어들이 주어졌을 때 스팸일 확률과 정상일 확률을 비교하는 것

- 나이브 베이즈 분류
  - 베이즈 정리를 활용하여 **입력값이 해당 클래스에 속할 확률을 계산하여 분류**
    - ex. $P(스팸 | 단어1, 단어2..) > P(정상 | 단어1, 단어2..)$ 이면 스팸
      - 특징: 메일에 등장하는 단어들
      - 클래스: 스팸, 정상
      - 베이즈 정리
        $$
        P(\text{스팸} \,|\, \text{단어1}, \text{단어2}, \ldots) 
        = \frac{P(\text{단어1}, \text{단어2}, \ldots \,|\, \text{스팸}) \cdot P(\text{스팸})}{P(\text{단어1}, \text{단어2}, \ldots)}
        $$
      - 각 특징들이 독립임을 가정하고 있으므로,
        $$
        P(\text{단어1}, \text{단어2}, \text{단어3} \,|\, \text{스팸})
        \;\approx\;
        P(\text{단어1} \,|\, \text{스팸}) \cdot 
        P(\text{단어2} \,|\, \text{스팸}) \cdot 
        P(\text{단어3} \,|\, \text{스팸})
        $$
      - 따라서, 단어들의 조건부 확률을 곱하여 $P(\text{스팸} | 단어들)$ 을 계산할 수 있고
      - 이를 통해 새로운 메일이 스팸인지 정상인지 분류 가능하다

  - 각 특징들이 독립이라면 다른 분류 방식에 비해 결과가 좋고, 학습 데이터도 적게 필요

  - 각 특징들이 독립이 아니라면 즉, 특징들이 서로 영향을 미치면 분류 결과 신뢰성 하락
    - 독립이 아니면 조건부 확률을 곱으로 단순화 할 수 없기 때문
    - $P(단어1,단어2∣스팸) ≠ P(단어1∣스팸)⋅P(단어2∣스팸)$

  - 학습 데이터에 없는 범주의 데이터일 경우 정상적 예측 불가능
    - 학습 데이터에 없으면 확률이 0이 되는데, 확률들을 곱하고 있기 때문에 하나만 0이어도 결과가 0이 됨

- 코드
  ```python
  # 전체 이메일 중 40%가 스팸
  # 전체 이메일 중 60%가 정상 메일 
  # 사전 확률 (Prior)
  P_spam = 0.4  # 스팸 메일 확률
  P_ham = 0.6   # 정상 메일 확률

  # 단어별 조건부 확률 (각 단어가 특정한 메일에 포함될 확률)
  # 예제 단어 (스팸 단어: "당첨", "무료", "혜택", 정상 단어: "회의", "보고서", "협조")
  # ex) 당첨이 스팸 메일에서 등장할 확률이 12/30
  P_당첨_given_spam = 12 / 30
  P_무료_given_spam = 10 / 30
  P_혜택_given_spam = 8 / 30

  P_당첨_given_ham = 2 / 20
  P_무료_given_ham = 3 / 20
  P_혜택_given_ham = 1 / 20

  # 특정 이메일이 "당첨", "무료", "혜택"을 포함할 경우 확률 계산
  P_words_given_spam = P_당첨_given_spam * P_무료_given_spam * P_혜택_given_spam
  P_words_given_ham = P_당첨_given_ham * P_무료_given_ham * P_혜택_given_ham

  # 최종 확률 계산 (Posterior)
  P_spam_given_words = P_words_given_spam * P_spam
  P_ham_given_words = P_words_given_ham * P_ham

  # 결과 출력
  {
      "P(스팸 | 당첨, 무료, 혜택)": P_spam_given_words,
      "P(정상 | 당첨, 무료, 혜택)": P_ham_given_words,
      "결론": "정상 메일" if P_ham_given_words > P_spam_given_words else "스팸 메일"
  }
  ```
  ![alt text](image-23.png)


## Classification
### 분류(Classification)
입력 데이터를 여러 개의 카테고리 중 하나에 속하도록 지정하는 작업

-> 주로 지도 학습(Supervised Learning) 방식으로 이루어짐

-> 데이터의 특징(Feature)을 기반으로 해당 데이터가 어느 범주에 속하는지를 예측

- 분류 문제의 목표
  - 학습 알고리즘은 `함수 f` 를 생성하여 입력 `벡터 x` 가 어떤 카테고리 `y` 에 속하는지를 예측
  - ex. 이미지 인식
    - 입력: 이미지의 픽셀 값
    - 출력: 이미지에 포함된 객체를 나타내는 카테고리 번호

- 분류 문제의 예시
  - 객체 인식(Object Recognition)
    - 이미지 속 사물을 인식하여 해당 사물이 무엇인지 분류하는 작업
  - 얼굴 인식(Face Recognition)
    - 사진 속 인물을 인식하고 자동으로 태그하는 기술로, 사용자와 컴퓨터 간의 자연스러운 상호작용을 가능하게 함


- 분류와 회귀의 차이

  | 구분 | 분류 (Classification) | 회귀 (Regression) |
  |------|------------------------|-------------------|
  | **정의** | 데이터를 미리 정의된 카테고리(범주)로 분류하는 작업 | 연속적인 숫자 값을 예측하는 작업 |
  | **출력값** | 이산적인 값 (카테고리/레이블) | 연속적인 실수 값 |
  | **예시** | 이메일 스팸 여부, 암 진단(양성/음성), 이미지 객체 인식 | 주택 가격 예측, 온도 변화 예측, 주식 시장 가격 예측 |
  | **핵심 차이점** | 결과가 ‘카테고리’로 표현 → 이산적 | 결과가 ‘숫자’로 표현 → 연속적 |

  ![alt text](image-18.png)

- 분류 분석
  - Training Data를 이용해서 데이터의 특성과 상관 관계 등을 파악하고,
  - 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에,
  - 그 결과를 어떤 종류의 값으로 분류 될 수 있는지 예측하는 것
  - ex. 스팸문자 분류(Spam or Ham), 암 판별(악성종양 or 종양)

    ![alt text](image-19.png)

- 입력 데이터 (Features, 입력 변수)
  - 분류 문제에서 입력 데이터는 하나 이상의 특징(Feature)으로 이루어진 벡터로 표현
  - 특징 벡터
    - ex. 이미지 인식에서 이미지의 각 픽셀 값이 특징이 될 수 있음
    - 입력 데이터는 연속형(숫자) or 이산형(범주형) 값으로 구성될 수 있음

- 출력 데이터 (Labels, 출력 변수)
  - 분류 문제의 출력은 데이터가 속한 클래스(범주)를 나타냄
  - 출력 레이블: `y`
    - ex. 이진 분류에서 출력은 0(음성) or 1(양성)
    - 다중 클래스 분류에서는 y가 1, 2, 3 등 여러 클래스를 나타냄

- 입력과 출력의 관계
  - 분류 모델은 주어진 입력 `x`에 대해 적절한 출력 `y`를 예측하는 함수 `f(x)`를 학습
  - ex. f(x)는 특정 사진을 입력받아 그것이 고양이인지, 개인지, 또는 새인지를 예측

### 결정 경계 (Decision Boundaries)
분류 모델이 데이터를 분류하기 위해 **공간을 나누는 경계선**

-> 입력 데이터를 특징 공간(Feature Space)에서 서로 다른 클래스 영역으로 구분

- 결정 경계의 역할
  - 각 클래스에 속하는 **데이터 포인트를 구분하는 기준을 제공**
  - 새로운 데이터가 주어졌을 때, 그 **데이터가 어느 클래스에 속하는지 결정**
    
- 결정 경계 예시
  - 선형 결정 경계
    - 단순한 직선이나 평면으로 클래스 간의 경계를 나눔
    - ex. 로지스틱 회귀, 선형 서포트 벡터 머신(SVM)

  - 비선형 결정 경계
    - 더 복잡한 경계를 만들어 클래스 간의 경계를 나눔
    - ex. 결정 트리, 랜덤 포레스트, 심층 신경망(Deep Neural Networks)

- 결정 경계 시각화
  - 2차원 특징 공간에서는 결정 경계를 쉽게 시각화 가능
  - 각 클래스 영역이 어떻게 나누어져 있는지를 시각적으로 표현하면 모델의 동작을 더 쉽게 이해할 수 있음


**※ 분류 문제에 회귀 알고리즘 적용하기**

-> 일반적인 선형 회귀는 분류 문제에 그대로 적용할 수 없음

-> Why? 선형 회귀의 출력값은 `-∞ ~ +∞` 범위를 가질 수 있음

-> 하지만 분류 문제의 출력값은 보통 0 또는 1 (이산값)

![alt text](image-20.png)

-> ex. 항공기 지연 여부(지연=1, 정상=0)를 예측한다고 가정했을 때

-> 선형 회귀를 적용하면 출력값이 0.7, 1.5, 1000 같은 값이 나올 수 있음

-> 우리의 목표는 지연 여부 판별인데 결과값이 1000이라면? "지연 여부"라는 **이진 분류 문제**에서 의미가 없음 

-> 그럼 어떡할까?

-> 예를 들어, 풍속이 어떤 수준 이상이면 지연이 발생하고, 그 미만이면 지연이 발생하지 않는다고 가정할 수 있음 

-> 즉, 특정 임계값(threshold)을 기준으로 **0(정상) / 1(지연)** 형태의 이진 분류로 문제를 정의할 수 있음

-> 다시 말해, wind speed가 1000일 때 단순히 "1000"이라는 예측값이 아니라, **delay=1인지 delay=0인지로 분류**할 수 있어야 함  

-> 로지스틱 회귀(Logistic Regression)

### 로지스틱 회귀
연속적인 출력을 확률(0~1)로 변환하고, 임계값을 기준으로 이진 분류를 가능하게 해주는 것

-> 이처럼 분류 문제에 적용하기 위해 출력 값의 범위를 수정한 회귀를 로지스틱 회귀(Logistic Regression)라고 함

![alt text](image-21.png)

-> 최소값 0, 최대값 1로 결과값을 수렴시키기 위해 Sigmoid (logistic) 함수 사용

- 로지스틱 회귀 (Logistic Regression)
  - 이진 분류를 위한 대표적인 선형 모델
  - 입력 데이터를 기반으로 특정 클래스에 속할 확률을 예측
  - 회귀라는 이름이 붙었지만 **분류 문제에 사용**

- 로지스틱 함수 (Sigmoid 함수)
  - 예측값을 0과 1 사이의 확률로 변환하는 함수

- 장점
  - 해석이 간단하고 계산이 빠름
  - 데이터가 선형적으로 분리될 수 있는 경우 좋은 성능

- 단점
  - 비선형 데이터에는 적합하지 않을 수 있음
  - 복잡한 결정 경계를 만들기 어려움

- 로지스틱 함수
  - S자형 곡선을 가지는 함수

    ![alt text](image-22.png)
    - x 값이 커질 경우 `g(x)` 값은 점점 `1`에 수렴
    - x 값이 작아질 경우 `g(x)` 값은 점점 `0`에 수렴

- 코드
  ```python
  # 필요한 라이브러리 임포트
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import accuracy_score
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split

  # 데이터셋 로드 (Iris 데이터셋 사용)
  data = load_iris()
  X = data.data  # 특성 데이터
  y = data.target  # 목표 변수 (클래스 레이블)

  # 데이터를 학습용 및 테스트용으로 분할
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

  # 로지스틱 회귀 모델 학습
  model = LogisticRegression(max_iter=200)
  model.fit(X_train, y_train)

  # 예측 수행
  y_pred = model.predict(X_test)

  # 모델 평가
  accuracy = accuracy_score(y_test, y_pred)  # 정확도 계산

  # 결과 출력
  print(f"Accuracy: {accuracy * 100:.2f}%")
  ```
  ![alt text](image-24.png)

  - LogisticRegression
    ```python
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(max_iter=200)
    ```
    - 로지스틱 회귀 모델(분류기) 생성
    - 주요 파라미터
      - `max_iter` : 최적화를 반복하는 횟수 (기본값 100, 데이터가 복잡할 경우 수렴하지 않아 늘려야 함)
      - `multi_class` : 다중 분류 방법 ('ovr' = one-vs-rest, 'multinomial')
    - 주요 메서드
      - `.fit(X_train, y_train)` : 모델 학습 (특성과 레이블을 이용해 파라미터 최적화)
      - `.predict(X_test)`: 학습된 모델로 새 데이터의 클래스를 예측
      - `.predict_proba(X_test)`: 클래스별 확률 값 반환


### 결정 트리(Decision Tree)
데이터를 feature에 따라 트리 구조로 분할하여 분류하거나 예측하는 알고리즘

-> 각 노드는 특성의 조건을 기반으로 데이터를 분기하며, 최종 리프 노드는 예측 결과(클래스)를 나타냄

- 작동 방식
  1. 특성 선택
      - 각 특성을 기준으로 데이터를 분할할 수 있는 최적의 조건을 찾음
  2. 분할
      - 데이터를 해당 특성 조건에 따라 좌우로 분기함
  3. 리프 노드 도달 시 각 분할된 노드가 특정 클래스(분류)로 분류될 때까지 분할을 반복함

      ![alt text](image-25.png)

- 특성 선택 기준
  - 지니 지수(Gini Index) 또는 엔트로피(Entropy)를 사용하여 분할의 순도를 측정

    ![alt text](image-26.png)
  - 순도가 높을수록(한쪽 클래스만 치우칠수록) 해당되는 특성은 더 좋은 분할 기준(값이 작으면 좋음)
  - 엔트로피
    - 데이터 집합의 불순도(uncertainty)를 측정하는 지표
    - 값이 높을수록 불순도가 크다 (여러 클래스가 섞여 있음)
    - 값이 낮을수록 순도가 크다 (한쪽 클래스에 치우쳐 있음, 불확실성이 작음)
  - 지니 계수
    - 데이터의 불순도(impurity)를 측정하는 또 다른 지표
    - 클래스가 섞여 있을수록 값이 커지고, 한 클래스에 치우칠수록 값이 작아짐

- 장점
  - 해석이 용이
    - 트리 구조로 시각화가 가능하며, 결과 해석이 직관적
  - 비선형 데이터에도 적합
    - 복잡한 결정 경계를 만들 수 있음
  - 데이터 전처리 과정이 간단하며, 범주형 데이터도 다룰 수 있음

- 단점
  - **트리가 너무 깊어지면 과적합(overfitting)**의 위험이 있음
  - 작은 변화에도 모델이 민감하게 반응할 수 있음
  - 데이터가 많아질수록 성능 저하가 발생할 수 있음

- 사용 예시
  - 고객 이탈 예측, 의사결정 지원 시스템, 의료 진단 등 다양한 분야에서 활용

- 코드
  ```python
  # 필요한 라이브러리 임포트
  from sklearn.tree import DecisionTreeClassifier
  from sklearn.metrics import accuracy_score
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split

  # 데이터셋 로드 (Iris 데이터셋 사용)
  data = load_iris()
  X = data.data  # 특성 데이터
  y = data.target  # 목표 변수 (클래스 레이블)

  # 데이터를 학습용 및 테스트용으로 분할
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

  # 결정트리 모델 학습
  model = DecisionTreeClassifier(random_state=42)  # 결정트리 모델 생성
  model.fit(X_train, y_train)

  # 예측 수행
  y_pred = model.predict(X_test)

  # 모델 평가
  accuracy = accuracy_score(y_test, y_pred)  # 정확도 계산

  # 결과 출력
  print(f"Accuracy: {accuracy * 100:.2f}%")
  ```

  - DecisionTreeClassifier
    ```python
    from sklearn.tree import DecisionTreeClassifier
    model = DecisionTreeClassifier(random_state=42)
    ```
    - 결정 트리(Decision Tree) 기반의 분류기 생성
    - 주요 파라미터
      - `criterion` : 분할 품질을 측정하는 함수 ("gini"(기본값), "entropy")
      - `max_depth` : 트리의 최대 깊이 제한
      - `min_samples_split` : 노드를 분할하기 위한 최소 샘플 수
      - `min_samples_leaf` : 리프 노드에 있어야 하는 최소 샘플 수
      - `random_state` : 무작위성 제어 (재현성 확보)
    - 주요 메서드
      - `.fit(X_train, y_train)` : 학습용 데이터로 모델 훈련
      - `.predict(X_test)` : 학습된 모델로 새 데이터 예측
      - `.score(X, y)`: 정확도 계산


## KNN
### k-최근접 이웃 알고리즘 (k-Nearest Neighbors, k-NN)
지도 학습에서 사용되는 간단한 분류 알고리즘으로, 데이터 포인트가 주어졌을 때 그 데이터와 가장 가까운 k개의 이웃을 기준으로 클래스를 결정함

-> 기존 데이터 가운데 가장 가까운 k개 이웃의 정보로 새로운 데이터를 예측하는 방법론

-> 유사한 특성을 가진 데이터는 유사 범주에 속하는 경향이 있다는 가정 하에 분류함

-> 설정된 K값에 따라 가까운 거리 내의 이웃의 수에 따라 분류함


- 작동 방식
  1. 거리 계산
      - 새로운 데이터와 훈련 데이터 간의 거리를 계산
  2. k개의 이웃 선택
      - 가장 가까운 k개의 이웃을 선택
  3. 다수결 투표
      - 선택된 이웃 중 가장 많은 클래스에 속하는 클래스를 새로운 데이터의 클래스로 예측

      ![alt text](image-27.png)
  
  - 예시

    ![alt text](image-28.png)
    - 새로운 고객 데이터(검정색)가 들어왔을 때
    - K=1이면 주황색 클래스로 분류
    - K=30이면 초록색 클래스로 분류

- 하이퍼파라미터 `k`
  - k값은 미리 정해지는 값이며
  - k가 너무 작으면 과적합(overfitting)이 발생할 수 있음
    - 극단적으로, k가 1이면 이상치나 노이즈에 민감해짐
  - k가 너무 크면 과소적합(underfitting)이 발생할 수 있음
    - 모든 클래스를 같은 클래스로 예측할 수도 있음

- 장점
  - 직관적이고 이해하기 쉬움
  - 복잡한 결정 경계도 학습할 수 있음

- 단점
  - 데이터가 많아지면 계산 비용이 많아짐
  - 차원의 저주(Curse of Dimensionality)에 취약함
    - 차원의 저주는 차원이 증가할수록 발생하는 문제들을 말함
    - ① 고차원으로 갈수록 모든 점들이 비슷한 거리에 위치하게 됨 -> 가까움/멀음의 구분이 어려워져, KNN의 가까운 이웃 개념이 무너짐
    - ② 차원이 커질수록 데이터가 공간 전체에 흩어져서 밀도가 낮아짐 -> 이웃 찾으려면 더 많은 데이터 필요 -> 학습 데이터 부족 문제 유발
    - ③ 차원 늘어날수록 계산량 증가
  - 중요한 특징을 자동으로 학습하지 않으므로 Feature Engineering이 필요할 수 있음

- 사용 예시
  - 이미지 인식, 추천 시스템, 패턴 인식 등에서 사용

- 코드
  ```python
  # 필요한 라이브러리 임포트
  from sklearn.model_selection import train_test_split
  from sklearn.neighbors import KNeighborsClassifier
  from sklearn.metrics import accuracy_score
  from sklearn.datasets import load_iris

  # 데이터셋 로드 (Iris 데이터셋 사용)
  data = load_iris()
  X = data.data  # 특성 데이터
  y = data.target  # 목표 변수 (클래스 레이블)

  # 데이터를 학습용 및 테스트용으로 분할
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

  # KNN 모델 학습 (K=5로 설정)
  model = KNeighborsClassifier(n_neighbors=5)
  model.fit(X_train, y_train)

  # 예측 수행
  y_pred = model.predict(X_test)

  # 모델 평가
  accuracy = accuracy_score(y_test, y_pred)  # 정확도 계산

  # 결과 출력
  print(f"Accuracy: {accuracy * 100:.2f}%")
  ```
  - KNeighborsClassifier
    ```python
    from sklearn.neighbors import KNeighborsClassifier
    model = KNeighborsClassifier(n_neighbors=5)
    ```
    - K-최근접 이웃(KNN) 분류 모델 생성
    - 주요 파라미터
      - `n_neighbors=5` : 분류할 때 가장 가까운 이웃 5개를 기준으로 다수결 결정
      - `metric='minkowski'` : 거리 계산 방식 (기본은 유클리드 거리)
      - `weights='uniform'` : 모든 이웃을 동일한 가중치로 취급 (옵션 'distance'를 주면 가까운 이웃일수록 더 큰 가중치 부여)
    - 주요 메서드
      - `.fit(X_train, y_train)` : 학습 데이터 저장 (KNN은 "훈련"이라기보다 단순히 데이터 저장)
      - `.predict(X_test)` : 테스트 데이터의 클래스 예측


## 추가 개념 및 정리
### 날짜 처리 함수
- `pd.to_datetime()`
  ```python
  data['일자'] = pd.to_datetime(data['일자'], format="%Y%m%d")
  ```
  - Pandas 객체(Series, DataFrame 컬럼 등)를 날짜/시간(datetime) 형식으로 변환
  - 문자열 또는 정수로 저장된 날짜 데이터를 분석/계산 가능한 datetime64 타입으로 바꿔줌
  - 주요 파라미터
    - `arg` : 변환 대상 (Series, list, array 등)
    - `format` : 날짜 데이터의 입력 형식 지정 ex. "20230101" -> "%Y%m%d"

- `.dt.weekday`
  ```python
  data['요일'] = data['일자'].dt.weekday + 1
  ```
  - datetime64 형식에서 요일 정보를 정수(0~6) 로 추출
  - 반환값: `0` 월요일 ~ `6` 일요일
    - `+1` 하면 `1` 월요일 ~ `7` 일요일
  - 관련 속성
    - `.dt.day_name()` : 요일 이름 ("Monday", "Tuesday" …) 반환
    - `.dt.month` : 월(1~12) 반환
    - `.dt.day` : 일(1~31) 반환

### `pd.concat()` + 리스트 컴프리헨션
- 피처와 타깃 값 설정
  - '요일'과 '시간대' (1시~23시)의 정보를 피처로 사용하고, '0시' 시간대의 교통량을 타깃 값으로 설정해라
    ```python
    X = filtered_data[['요일']]  # 요일 정보를 독립 변수로 사용
    print(X.head())
    X = pd.concat([X, filtered_data[[f'{h}시' for h in range(1, 24)]]], axis=1) # 열 추가이므로 axis=1
    print(X.head())
    y = filtered_data['0시']
    ```
    ![alt text](image-29.png)
    - X (Feature) : 모델 입력값 -> '요일' 컬럼과 '1시' ~ '23시' 교통량 데이터
    - y (Target) : 모델이 맞춰야 하는 정답 -> '0시' 시간대의 교통량

- `pd.concat()`
  ```python
  X = pd.concat([X, filtered_data[[f'{h}시' for h in range(1, 24)]]], axis=1)
  ```
  - 여러 DataFrame이나 Series를 하나로 합치는 함수
  - `filtered_data[[f'{h}시' for h in range(1, 24)]]` : '1시'~'23시' 컬럼만 있는 DataFrame
  - DataFrame들을 **열 방향(axis=1)**으로 합쳐서 새로운 입력 데이터셋 생성
  - 두 개를 열(column) 기준으로 합침
  - 주요 파라미터
    - `objs` : 합칠 대상 (리스트 형태로 전달)
    - `axis` : 축 설정 (axis=0 -> 행 기준, axis=1 -> 열 기준)
    - `ignore_index` : 인덱스를 새로 매길지 여부 (기본 False -> 기존 인덱스 유지)
    - `join` : 결합 방식 ('outer', 'inner')
  