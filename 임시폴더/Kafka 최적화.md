# Kafka ìµœì í™”

## ì±•í„°ì˜ í¬ì¸íŠ¸
- Kafka ì„±ëŠ¥ ìµœì í™”
- í”„ë¡œë“€ì„œ ë° ì»¨ìŠˆë¨¸ì˜ ì„±ëŠ¥ íŠœë‹
- ì£¼ìš” ì„¤ì • íŒŒë¼ë¯¸í„° ì´í•´
- Kafka ë°ì´í„°ì˜ ì €ì¥

# Kafka ì„±ëŠ¥ ìµœì í™” ê°œë¡ 

## Kafkaì˜ ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•œ ì´ìœ 
- Kafkaì˜ ì„±ëŠ¥ ìµœì í™”ê°€ í•„ìš”í•œ ì´ìœ 
  - ëŒ€ê·œëª¨ íŠ¸ë˜í”½ ì²˜ë¦¬: ë°ì´í„°ì˜ ì²˜ë¦¬ëŸ‰(Throughput) ë° ì§€ì—°(Latency) ìµœì í™”
  - ì•ˆì •ì„± ê°œì„ : ë°ì´í„° ì†ì‹¤ì— ëŒ€í•œ ì•ˆì •ì„±(Durability)ë¥¼ ë†’ì„
  - ë¦¬ì†ŒìŠ¤ì˜ íš¨ìœ¨ì  ì‚¬ìš©: ì œí•œëœ ë¦¬ì†ŒìŠ¤ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ë©° ìì› ë¶€í•˜ë¥¼ ë‚®ì¶¤

- ì„±ëŠ¥ ìµœì í™” ì£¼ìš” ì§€í‘œ
  - í”„ë¡œë“€ì„œ ì»¨ìŠˆë¨¸ì˜ ì²˜ë¦¬ ì†ë„
  - ë¸Œë¡œì»¤ ë° ì£¼í‚¤í¼ ì„¤ì •
  - ìŠ¤í† ë¦¬ì§€, ë„¤íŠ¸ì›Œí¬ ë“± ìì› ìµœì í™”

# í”„ë¡œë“€ì„œ ë° ì»¨ìŠˆë¨¸ ì„±ëŠ¥ íŠœë‹

## Producer ì„±ëŠ¥ ìµœì í™”

### ì§ë ¬í™” ë°©ì‹ ì„ ì •
1. **StringSerializer**: ë‹¨ìˆœí•œ ë¬¸ìì—´ ì§ë ¬í™”, ë¬¸ìì—´ì„ UTF-8ë¡œ ì¸ì½”ë”©, ì••ì¶• íš¨ìœ¨ ë‚®ìŒ  
2. **ByteArraySerializer**: ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë°”ì´íŠ¸ ë°°ì—´ë¡œ ì§ë ¬í™”, ë‹¤ì–‘í•œ í˜•ì‹ ì²˜ë¦¬ ê°€ëŠ¥, ë¹ ë¥¸ ë³€í™˜ / ë¶€ì¡±í•œ ì‚¬ìš©ì„±  
3. **JsonSerializer**: JSON í˜•ì‹ìœ¼ë¡œ ì§ë ¬í™”, ê°€ë…ì„± ë†’ì§€ë§Œ ì••ì¶• íš¨ìœ¨ ë‚®ìŒ  
4. **AvroSerializer**: Avro í¬ë§·ì„ ì‚¬ìš©í•œ ì§ë ¬í™”, ìŠ¤í‚¤ë§ˆ ê¸°ë°˜, ì••ì¶• íš¨ìœ¨ì´ ì¢‹ê³  ë¹ ë¦„ (Kafka ê¶Œì¥ ë°©ì‹)

### StringSerializer
```python
from confluent_kafka import Producer

producer = Producer({'bootstrap.servers': 'localhost:9092'})
# ë©”ì‹œì§€ ì „ì†¡
producer.produce(
    topic='test-topic',
    key='my_key'.encode('utf-8'),
    value='Hello Kafka!'.encode('utf-8')
)

# ë‚´ë¶€ ì „ì†¡ í ì²˜ë¦¬
producer.poll(0)
# ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ ëŒ€ê¸°
producer.flush()
```

```python
from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    key_serializer=str.encode,      # ë¬¸ìì—´ UTF-8 ì¸ì½”ë”©
    value_serializer=str.encode     # ë¬¸ìì—´ UTF-8 ì¸ì½”ë”©
)

producer.send('test-topic', key='my_key', value='Hello Kafka!')
producer.flush()
```

---

### ByteArraySerializer
```python
from confluent_kafka import Producer

producer = Producer({'bootstrap.servers': 'localhost:9092'})

producer.produce(
    topic='test-topic',
    key='my_key'.encode('utf-8') if 'my_key' else None,
    value=b'BinaryData' if isinstance(b'BinaryData', bytes) else b'BinaryData'.encode()
)

producer.poll(0)
producer.flush()
```

```python
producer = KafkaProducer(
  bootstrap_servers='localhost:9092',
  key_serializer=lambda k: k.encode() if k else None,
  value_serializer=lambda v: v if isinstance(v, bytes) else v. encode()
)

producer.send('test-topic', key='my_key', value=b'BinaryData')
```

---

### JsonSerializer
```python
import json
from confluent_kafka import Producer

producer = Producer({'bootstrap.servers': 'localhost:9092'})

producer.produce(
    topic='test-topic',
    value=json.dumps({"name": "Alice", "age": 25}).encode('utf-8')
)

producer.poll(0)
producer.flush()
```

---

```python
import json
from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')  # JSON ì§ë ¬í™” í›„ UTF-8 ì¸ì½”ë”©
)

producer.send('test-topic', value={"name": "Alice", "age": 25})
producer.flush()
```

### Producer ì„±ëŠ¥ ìµœì í™”
- ì§ë ¬í™” ë°©ì‹ ì„ ì •
  - AvroSerializer: Avro í¬ë§·ì„ ì‚¬ìš©í•œ ì§ë ¬í™”, ìŠ¤í‚¤ë§ˆ ê¸°ë°˜, ì••ì¶• íš¨ìœ¨ì´ ì¢‹ê³  ë¹ ë¦„ (Kafka ê¶Œì¥ ë°©ì‹)

```python
from kafka import KafkaProducer
from fastavro import parse_schema, schemaless_writer
import io

# 1ï¸. Avro ìŠ¤í‚¤ë§ˆ ì •ì˜
avro_schema = {
    "type": "record",
    "name": "User",
    "fields": [
        {"name": "name", "type": "string"},
        {"name": "age", "type": "int"}
    ]
}
parsed_schema = parse_schema(avro_schema)

# 2ï¸. ë©”ì‹œì§€ ì§ë ¬í™”
record = {"name": "Alice", "age": 25}
bytes_writer = io.BytesIO()
schemaless_writer(bytes_writer, parsed_schema, record)
avro_bytes = bytes_writer.getvalue()

# 3ï¸. Kafka Producer ìƒì„± ë° ì „ì†¡
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    key_serializer=str.encode,
    value_serializer=lambda v: avro_bytes   # ì´ë¯¸ ì§ë ¬í™”ë¨
)

producer.send('avro-topic', key='user1', value=avro_bytes)
producer.flush()
```
```python
from confluent_kafka import SerializingProducer
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.avro import AvroSerializer

schema_registry_conf = {'url': 'http://localhost:8081'}
schema_registry_client = SchemaRegistryClient(schema_registry_conf)

avro_schema = """
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"}
  ]
}
"""

avro_serializer = AvroSerializer(schema_registry_client, avro_schema)

producer = SerializingProducer({
    'bootstrap.servers': 'localhost:9092',
    'key.serializer': str.encode,
    'value.serializer': avro_serializer
})

producer.produce(topic='test-topic', key='user1', value={"name": "Alice", "age": 25})
producer.flush()
```

# í”„ë¡œë“€ì„œ ë° ì»¨ìŠˆë¨¸ ì„±ëŠ¥ íŠœë‹

## Producer ì„±ëŠ¥ ìµœì í™”
### íŒŒí‹°ì…”ë‹ ë°©ì‹ ì„ ì •

1. **key ê¸°ë°˜ íŒŒí‹°ì…”ë‹**: í•´ì‹œ ê¸°ë°˜, ê°™ì€ keyë¥¼ ê°€ì§„ ê°’ë“¤ë¼ë¦¬ ê°™ì€ íŒŒí‹°ì…˜ ë°°ì¹˜  
2. íŠ¹ì • íŒŒí‹°ì…˜ ì§€ì •: ë°”ì´íŠ¸ ë°°ì—´ ê·¸ëŒ€ë¡œ ì „ì†¡  
3. StickyPartitioner: ì •ìˆ˜ ê°’ì„ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¡œ ë³€í™˜, Batchë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ RoundRobinë³´ë‹¤ íš¨ìœ¨ì   
4. ì»¤ìŠ¤í…€ íŒŒí‹°ì…”ë„ˆ: ë³¸ì¸ë§Œì˜ ë¡œì§ì„ ë§Œë“  íŒŒí‹°ì…”ë„ˆ  

```python
# 1. Kafka Producer ì„¤ì •
producer = SerializingProducer({
    'bootstrap.servers': 'localhost:9092',
    'key.serializer': StringSerializer('utf_8'),
    'value.serializer': StringSerializer('utf_8')
})

# 2. ì „ì†¡í•  key ë¦¬ìŠ¤íŠ¸
keys = ["user1", "user2", "user3", "user1", "user2"]

# 3. ë©”ì‹œì§€ ì „ì†¡
for i, key in enumerate(keys):
    value = f"Data {i}"
    producer.produce(topic="test-topic", key=key, value=value)
    print(f"Sent: Key={key}, Value={value}")

# 4. ì „ì†¡ ì™„ë£Œ ë³´ì¥
producer.flush()
```

1. **key ê¸°ë°˜ íŒŒí‹°ì…”ë‹**: í•´ì‹œ ê¸°ë°˜, ê°™ì€ keyë¥¼ ê°€ì§„ ê°’ë“¤ë¼ë¦¬ ê°™ì€ íŒŒí‹°ì…˜ ë°°ì¹˜  
2. íŠ¹ì • íŒŒí‹°ì…˜ ì§€ì •: ë°”ì´íŠ¸ ë°°ì—´ ê·¸ëŒ€ë¡œ ì „ì†¡  
3. StickyPartitioner: ì •ìˆ˜ ê°’ì„ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¡œ ë³€í™˜, Batchë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ RoundRobinë³´ë‹¤ íš¨ìœ¨ì   
4. ì»¤ìŠ¤í…€ íŒŒí‹°ì…”ë„ˆ: ë³¸ì¸ë§Œì˜ ë¡œì§ì„ ë§Œë“  íŒŒí‹°ì…”ë„ˆ  

```python
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    key_serializer=str.encode,   # Keyë¥¼ UTF-8ë¡œ ë³€í™˜
    value_serializer=str.encode  # Valueë¥¼ UTF-8ë¡œ ë³€í™˜
)

for i in range(5):
    producer.send('test-topic', value=f"Fixed Partition Data {i}", partition=0)
    print(f"Sent to Partition 0: Message {i}")

producer.flush()
```

1. key ê¸°ë°˜ íŒŒí‹°ì…”ë‹: í•´ì‹œ ê¸°ë°˜, ê°™ì€ keyë¥¼ ê°€ì§„ ê°’ë“¤ë¼ë¦¬ ê°™ì€ íŒŒí‹°ì…˜ ë°°ì¹˜  
2. **íŠ¹ì • íŒŒí‹°ì…˜ ì§€ì •**: ë°”ì´íŠ¸ ë°°ì—´ ê·¸ëŒ€ë¡œ ì „ì†¡  
3. StickyPartitioner: ì •ìˆ˜ ê°’ì„ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¡œ ë³€í™˜, Batchë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ RoundRobinë³´ë‹¤ íš¨ìœ¨ì   
4. ì»¤ìŠ¤í…€ íŒŒí‹°ì…”ë„ˆ: ë³¸ì¸ë§Œì˜ ë¡œì§ì„ ë§Œë“  íŒŒí‹°ì…”ë„ˆ  

```python
# 1. Kafka Producer ì„¤ì •
producer = SerializingProducer({
    'bootstrap.servers': 'localhost:9092',
    'key.serializer': StringSerializer('utf_8'),
    'value.serializer': StringSerializer('utf_8')
})

# 2. ê³ ì • íŒŒí‹°ì…˜ 0ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡
for i in range(5):
    value = f"Fixed Partition Data {i}"
    producer.produce(
        topic="test-topic",
        value=value,
        partition=0
    )
    print(f"Sent to Partition 0: Message {i}")

# 3. ì „ì†¡ ì™„ë£Œ ë³´ì¥
producer.flush()
```

1. key ê¸°ë°˜ íŒŒí‹°ì…”ë‹: í•´ì‹œ ê¸°ë°˜, ê°™ì€ keyë¥¼ ê°€ì§„ ê°’ë“¤ë¼ë¦¬ ê°™ì€ íŒŒí‹°ì…˜ ë°°ì¹˜  
2. íŠ¹ì • íŒŒí‹°ì…˜ ì§€ì •: ë°”ì´íŠ¸ ë°°ì—´ ê·¸ëŒ€ë¡œ ì „ì†¡  
3. **StickyPartitioner**: ì •ìˆ˜ ê°’ì„ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¡œ ë³€í™˜, Batchë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ RoundRobinë³´ë‹¤ íš¨ìœ¨ì   
   (ê°™ì€ íŒŒí‹°ì…˜ì— ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ëª¨ì•„ì„œ ë³´ë‚´ê³ , ì¼ì • ì¡°ê±´ì´ ë˜ë©´ ë‹¤ë¥¸ íŒŒí‹°ì…˜ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì „ëµ)  
4. ì»¤ìŠ¤í…€ íŒŒí‹°ì…”ë„ˆ: ë³¸ì¸ë§Œì˜ ë¡œì§ì„ ë§Œë“  íŒŒí‹°ì…”ë„ˆ  

| ë©”ì‹œì§€ ë²ˆí˜¸ | íŒŒí‹°ì…˜ ì˜ˆì‹œ |
|-------------|-------------|
| message-0 | 1 |
| message-1 | 1 |
| message-2 | 1 |
| message-3 | 1 |
| message-4 | 1 |
| message-5 | 2 |
| message-6 | 2 |
| message-7 | 2 |
| message-8 | 0 |
| message-9 | 0 |

```python
# 1. Kafka Producer ì„¤ì •
producer = SerializingProducer({
    'bootstrap.servers': 'localhost:9092',
    'value.serializer': StringSerializer('utf_8')  # Key ì—†ì´ Valueë§Œ ì„¤ì •
})

# 2. Key ì—†ì´ ë©”ì‹œì§€ ì „ì†¡ (Sticky íŒŒí‹°ì…”ë„ˆ ì ìš©ë¨)
for i in range(10):
    value = f"Sticky Message {i}"
    producer.produce(topic="test-topic", value=value)
    print(f"Sent Message {i}")

# 3. ì „ì†¡ ì™„ë£Œ ë³´ì¥
producer.flush()
```

1. key ê¸°ë°˜ íŒŒí‹°ì…”ë‹: í•´ì‹œ ê¸°ë°˜, ê°™ì€ keyë¥¼ ê°€ì§„ ê°’ë“¤ë¼ë¦¬ ê°™ì€ íŒŒí‹°ì…˜ ë°°ì¹˜  
2. íŠ¹ì • íŒŒí‹°ì…˜ ì§€ì •: ë°”ì´íŠ¸ ë°°ì—´ ê·¸ëŒ€ë¡œ ì „ì†¡  
3. **StickyPartitioner**: ì •ìˆ˜ ê°’ì„ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¡œ ë³€í™˜, Batchë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ RoundRobinë³´ë‹¤ íš¨ìœ¨ì   
   (ê°™ì€ íŒŒí‹°ì…˜ì— ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ëª¨ì•„ì„œ ë³´ë‚´ê³ , ì¼ì • ì¡°ê±´ì´ ë˜ë©´ ë‹¤ë¥¸ íŒŒí‹°ì…˜ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì „ëµ)  
4. ì»¤ìŠ¤í…€ íŒŒí‹°ì…”ë„ˆ: ë³¸ì¸ë§Œì˜ ë¡œì§ì„ ë§Œë“  íŒŒí‹°ì…”ë„ˆ  

```python
from kafka import KafkaProducer
from kafka.partitioner.default import DefaultPartitioner

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=str.encode,     # UTF-8 ì¸ì½”ë”©
    # partitioner=DefaultPartitioner() # ê¸°ë³¸ í•´ì‹± íŒŒí‹°ì…”ë„ˆ (ë¼ìš´ë“œë¡œë¹ˆ ë°©ì‹)
)

# Key ì—†ì´ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ë©´ Sticky Partitionerê°€ ìë™ ì ìš©ë¨
for i in range(10):
    producer.send('test-topic', value=f"Sticky Message {i}")
    print(f"Sent Message {i}")

producer.flush()
```

1. key ê¸°ë°˜ íŒŒí‹°ì…”ë‹: í•´ì‹œ ê¸°ë°˜, ê°™ì€ keyë¥¼ ê°€ì§„ ê°’ë“¤ë¼ë¦¬ ê°™ì€ íŒŒí‹°ì…˜ ë°°ì¹˜  
2. íŠ¹ì • íŒŒí‹°ì…˜ ì§€ì •: ë°”ì´íŠ¸ ë°°ì—´ ê·¸ëŒ€ë¡œ ì „ì†¡  
3. StickyPartitioner: ì •ìˆ˜ ê°’ì„ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¡œ ë³€í™˜, Batchë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ RoundRobinë³´ë‹¤ íš¨ìœ¨ì   
4. **ì»¤ìŠ¤í…€ íŒŒí‹°ì…”ë„ˆ**: ë³¸ì¸ë§Œì˜ ë¡œì§ì„ ë§Œë“  íŒŒí‹°ì…”ë„ˆ (kafka-python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)  
> (confluent-kafka-pythonì€ ë‚´ë¶€ì ìœ¼ë¡œ C ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ `librdkafka`ë¥¼ ê°ì‹¸ì„œ êµ¬í˜„ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë¶ˆê°€ëŠ¥)

```python
from kafka import KafkaProducer, partitioner

class CustomPartitioner:
    def __call__(self, key, all_partitions, available_partitions):
        key_int = int(key.decode())  # Keyë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
        return key_int % len(all_partitions)  # í•´ì‹œ ê³„ì‚°: ì§ìˆ˜ëŠ” 0ë²ˆ, í™€ìˆ˜ëŠ” 1ë²ˆ íŒŒí‹°ì…˜

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    key_serializer=str.encode,
    value_serializer=str.encode,
    partitioner=CustomPartitioner()  # ì‚¬ìš©ì ì§€ì • íŒŒí‹°ì…”ë„ˆ ì ìš©
)

for i in range(10):
    producer.send('test-topic', key=str(i), value=f"Message {i}")
    print(f"Sent Key={i}, Value=Message {i}")

producer.flush()
```

### Batching ì„¤ì • ìµœì í™”
1. **buffer.memory**: í”„ë¡œë“€ì„œ ë‚´ë¶€ì—ì„œ ì €ì¥í•  ìˆ˜ ìˆëŠ” RA ë²„í¼ì˜ ìµœëŒ€ í¬ê¸°, ê¸°ë³¸ê°’ 32MB  
   (confluent-kafkaì—ëŠ” í•´ë‹¹ ì˜µì…˜ ì—†ìŒ)  
2. **batch.size**: í•œ batchì˜ í¬ê¸°, í•´ë‹¹ í¬ê¸°ê°€ ë‹¤ ì°¨ë©´ ì „ì†¡ ì¤€ë¹„. ê¸°ë³¸ê°’ 16KB  
3. **linger.ms**: Batchê°€ ë§Œë“¤ì–´ì§€ëŠ” ìµœëŒ€ ëŒ€ê¸° ì‹œê°„. í•´ë‹¹ ì‹œê°„ì´ ì§€ë‚˜ë©´ ë‹¤ ì•ˆ ì°¨ë„ ì „ì†¡ ì¤€ë¹„. ê¸°ë³¸ê°’ ì—†ìŒ  

```python
from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=str.encode,
    buffer_memory=33554432,  # 32MB (ê¸°ë³¸ê°’)
    batch_size=32768,        # 32KB (ê¸°ë³¸ê°’: 16KBë³´ë‹¤ í¬ê²Œ ì„¤ì •í•´ ë°°ì¹­ íš¨ìœ¨ ì¦ê°€)
    linger_ms=5              # 5ms ë™ì•ˆ ë°°ì¹˜ë¥¼ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì „ì†¡ (ê¸°ë³¸ê°’: 0)
)

for i in range(100):
    producer.send('test-topic', value=f"Batching Message {i}")

producer.flush()
```

## Producer ì„±ëŠ¥ ìµœì í™”
### Compression ë°©ì‹ ê²°ì •
1. Gzip: ë†’ì€ ì••ì¶•ë¥ , ëŠë¦° ì²˜ë¦¬ì†ë„, ë†’ì€ CPU ì‚¬ìš©ëŸ‰ â†’ í™•ì‹¤í•œ ì••ì¶•  
2. LZ4: ì ë‹¹í•œ ì••ì¶•ë¥ , ì¤€ìˆ˜í•œ ì²˜ë¦¬ì†ë„, ì¤‘ê°„ ì •ë„ì˜ CPU ì‚¬ìš©ëŸ‰ â†’ ê· í˜•ì¡íŒ ì••ì¶•ê³¼ CPU ì‚¬ìš©  
3. Snappy: ë‚®ì€ ì••ì¶•ë¥ , ë¹ ë¥¸ ì²˜ë¦¬ì†ë„, ë‚®ì€ CPU ì‚¬ìš©ëŸ‰ â†’ ë¹ ë¥¸ ì••ì¶•ê³¼ CPU ì ˆì•½  

| Metrics | Uncompressed | Gzip | Snappy | LZ4 |
|----------|---------------|------|---------|-----|
| Avg latency (ms) | 65 | 10.41 | 10.1 | 9.26 |
| Disk space (MB) | 10 | 0.92 | 2.18 | 2.83 |
| Effective compression ratio | 1 | 0.09 | 0.21 | 0.28 |
| Process CPU usage (%) | 2.35 | 11.46 | 7.25 | 5.89 |

```python
from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    compression_type='gzip',  # Gzip ì••ì¶• ì ìš©, 'snappy', 'lz4' ê°€ëŠ¥
    value_serializer=str.encode
)

for i in range(10):
    producer.send('test-topic', value=f"Gzip Message {i}")
    print(f"Sent Gzip Message {i}")

producer.flush()
```

## Producer ì„±ëŠ¥ ìµœì í™”
### Acknowledge ë°©ì‹ ê²°ì •
1. acks=0: í”„ë¡œë“€ì„œê°€ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê³  í™•ì¸í•˜ì§€ ì•ŠìŒ  
2. acks=1: ë¦¬ë” ë¸Œë¡œì»¤ë§Œ ë°›ìœ¼ë©´ ì„±ê³µ  
3. **acks=all(-1)**: ëª¨ë“  ë³µì œë³¸ì´ ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í•  ë•Œê¹Œì§€ ëŒ€ê¸° (ê¸°ë³¸ê°’)  
4. min.insync.replicas: ë³µì œë³¸ ì¤‘ ì‹¤ì œë¡œ ì‘ë‹µí•´ì•¼ í•˜ëŠ” ìµœì†Œ ê°œìˆ˜, ê¸°ë³¸ê°’ 1 (2 ì´ìƒ ê¶Œì¥)

```python
from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=str.encode,
    acks='all'  # ë˜ëŠ” 0, 1
)

producer.send('test-topic', value='Critical Message')
producer.flush()
```

### Acknowledge ë°©ì‹ ê²°ì •
1. acks=0: í”„ë¡œë“€ì„œê°€ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê³  í™•ì¸í•˜ì§€ ì•ŠìŒ  
2. acks=1: ë¦¬ë” ë¸Œë¡œì»¤ë§Œ ë°›ìœ¼ë©´ ì„±ê³µ  
3. acks=all(-1): ëª¨ë“  ë³µì œë³¸ì´ ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í•  ë•Œê¹Œì§€ ëŒ€ê¸° (ê¸°ë³¸ê°’)  
4. **min.insync.replicas**: ë³µì œë³¸ ì¤‘ ì‹¤ì œë¡œ ì‘ë‹µí•´ì•¼ í•˜ëŠ” ìµœì†Œ ê°œìˆ˜, ê¸°ë³¸ê°’ 1 (2 ì´ìƒ ê¶Œì¥)

```bash
# config/server.properties ì˜ˆì‹œ
75 # For anythin other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
76 offsets.topic.replication.factor=1
77 transaction.state.log.replication.factor=1
78 transaction.state.log.min.isr=1
79 min.insync.replicas=2
```


## Producer ì„±ëŠ¥ ìµœì í™”
### Transaction
1. `kafka-python`ì€ transaction ê´€ë¦¬ ê¸°ëŠ¥ì´ ì—†ê³ , `confluent_kafka`ëŠ” ì¡´ì¬  
2. Transactionì´ ë„ˆë¬´ ê¸¸ì§€ ì•Šë„ë¡ ê´€ë¦¬ í•„ìš”  

```python
from confluent_kafka import Producer

producer = Producer({
    'bootstrap.servers': 'localhost:9092',
    'transactional.id': 'my-transactional-producer',  # íŠ¸ëœì­ì…˜ ID
    'enable.idempotence': True,
    'acks': 'all',
    'retries': 5,
    'max.in.flight.requests.per.connection': 5
})

# íŠ¸ëœì­ì…˜ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒ)
producer.init_transactions()

# íŠ¸ëœì­ì…˜ ì‹œì‘
producer.begin_transaction()

try:
    for i in range(5):
        producer.produce('tx-topic', key=f'key-{i}', value=f'value-{i}')

    # ì»¤ë°‹
    producer.commit_transaction()
    print("Committed successfully")
except Exception as e:
    # ë¡¤ë°±
    print(f"Transaction failed: {e}")
    producer.abort_transaction()
```

## Producer ì„±ëŠ¥ ìµœì í™”
### Retry ê´€ë ¨ ì˜µì…˜ ê²°ì •
1. retries: ëª‡ ë²ˆê¹Œì§€ ì¬ì‹œë„í• ì§€ ì„¤ì •, ê¸°ë³¸ê°’ `INT_MAX`  
2. max.in.flight.requests.per.connection: Ackë¥¼ ë°›ì§€ ì•Šê³  ë³´ë‚¼ ìˆ˜ ìˆëŠ” ë™ì‹œ ìš”ì²­ ê°œìˆ˜ (ê¸°ë³¸ê°’ 5)  
3. enable.idempotence: ë©±ë“±ì„± í”„ë¡œë“€ì„œ ì„¤ì • (ì¤‘ë³µ ì „ì†¡ ë°©ì§€), ê¸°ë³¸ê°’ `true`

```python
from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=str.encode,
    retries=5,
    max_in_flight_requests_per_connection=1,
    enable_idempotence=True
)

for i in range(10):
    producer.send('reliable-topic', value=f"Message {i}")
    print(f"Sent message {i}")

producer.flush()
```

## Consumer ì„±ëŠ¥ ìµœì í™”
### Coordinator ì„¤ì • ìµœì í™”
1. heartbeat.interval.ms: heartbeat ê°„ê²©, `session.timeout.ms`ì˜ 1/3 ìˆ˜ì¤€ì´ ì ë‹¹, ê¸°ë³¸ê°’ 3ì´ˆ  
2. session.timeout.ms: heartbeatë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„, ì´ ì´ìƒì´ë©´ í•´ë‹¹ ì»¨ìŠˆë¨¸ ì œê±° í›„ rebalance, ê¸°ë³¸ê°’ 10ì´ˆ  
3. max.poll.records: ì»¨ìŠˆë¨¸ê°€ í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ìµœëŒ€ ë°ì´í„° ìˆ˜, ê¸°ë³¸ê°’ 500  
4. max.poll.interval.ms: polling í˜¸ì¶œ ê°„ê²©. ì´ ì´ìƒì´ë©´ ì»¨ìŠˆë¨¸ ì œê±° í›„ rebalance, ê¸°ë³¸ê°’ 5ë¶„  

```python
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'coordinator-test',
    bootstrap_servers='localhost:9092',
    group_id='group-1',
    auto_offset_reset='earliest',
    enable_auto_commit=False,
    max_poll_records=100,
    session_timeout_ms=15000,
    heartbeat_interval_ms=5000,
    max_poll_interval_ms=60000,
    value_deserializer=lambda v: v.decode('utf-8')
)

for msg in consumer:
    print(f"[{msg.partition}] {msg.offset} -> {msg.value}")
    # time.sleep(10)  # simulate slow processing
```

## Consumer ì„±ëŠ¥ ìµœì í™”
### Fetching ë°©ì‹ ì„ ì •
1. fetch.min.byte: ê°€ì ¸ì˜¬ ìµœì†Œ ë°ì´í„° í¬ê¸°, ê¸°ë³¸ê°’ 1 â†’ Throughputê³¼ ë¹„ë¡€, Latency ë°˜ë¹„ë¡€  
2. fetch.max.byte: í•œ ë²ˆì— ë°›ì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ ë°ì´í„° í¬ê¸°, ê¸°ë³¸ê°’ 50MB â†’ í´ìˆ˜ë¡ ëŒ€ëŸ‰ ì²˜ë¦¬ ê°€ëŠ¥  
3. fetch.max.wait.ms: ë°ì´í„°ê°€ ëª¨ì¼ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ìµœëŒ€ ì‹œê°„, ê¸°ë³¸ê°’ 500ms  
4. max.partition.fetch.bytes: íŒŒí‹°ì…˜ í•˜ë‚˜ë‹¹ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ìµœëŒ€ ë°ì´í„°, ê¸°ë³¸ê°’ 1MB  

```python
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'fetch-test',
    bootstrap_servers='localhost:9092',
    group_id='fetch-group',
    auto_offset_reset='earliest',
    value_deserializer=lambda x: x.decode('utf-8'),
    fetch_min_bytes=1024,                         # ìµœì†Œ 1KB ì´ìƒ ë°›ì•„ì•¼ ì‘ë‹µ
    fetch_max_bytes=10 * 1024 * 1024,             # ìµœëŒ€ 10MB ì‘ë‹µ í—ˆìš©
    fetch_max_wait_ms=1000,                       # ìµœëŒ€ 1ì´ˆ ëŒ€ê¸°
    max_partition_fetch_bytes=2 * 1024 * 1024     # íŒŒí‹°ì…˜ë‹¹ 2MB
)

for msg in consumer:
    print(f"[{msg.partition}] {msg.offset} -> {msg.value}")
```

## Consumer ì„±ëŠ¥ ìµœì í™”
### Partitioning ê´€ë ¨ ì„¤ì •
1. RangeAssignor: í† í”½ë‹¹ Nê°œì”© ì—°ì†ëœ íŒŒí‹°ì…˜ ë¶„ë°° (ê¸°ë³¸ê°’)  
2. RoundRobinAssignor: ì»¨ìŠˆë¨¸ ìˆ˜ì— ë§ê²Œ ìˆœì°¨ ë°°ë¶„  
3. StickyPartitionAssignor: ê¸°ì¡´ í• ë‹¹ì„ ìœ ì§€í•˜ë©´ì„œ ë³€ê²½ ìµœì†Œí™”  
4. CooperativeStickyAssignor: Stickyë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ì§€ë§Œ ì¼ë¶€ ì»¨ìŠˆë¨¸ ë³€ê²½ ì‹œì—ë„ ë‚˜ë¨¸ì§€ ì‘ë™

```python
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'my-topic',
    bootstrap_servers='localhost:9092',
    group_id='group-A',
    auto_offset_reset='earliest',
    partition_assignment_strategy=[
        "org.apache.kafka.clients.consumer.RoundRobinAssignor"
    ]
)
```

## Consumer ì„±ëŠ¥ ìµœì í™”
### Partitioning ê´€ë ¨ ì„¤ì •
1. RangeAssignor: í† í”½ë‹¹ Nê°œì”© ì—°ì†ëœ íŒŒí‹°ì…˜ ë¶„ë°° (ê¸°ë³¸ê°’)  
2. RoundRobinAssignor: ì»¨ìŠˆë¨¸ ìˆ˜ì— ë§ê²Œ ìˆœì°¨ ë°°ë¶„ â†’ ê· ë“± ë¶„ë°°  
3. StickyPartitionAssignor: ê¸°ì¡´ í• ë‹¹ì„ ìœ ì§€í•˜ë©´ì„œ ë³€ê²½ ìµœì†Œí™” â†’ ë¦¬ë°¸ëŸ°ì‹± ì‹œ íŒŒí‹°ì…˜ ìœ ì§€ ìš°ì„   
4. CooperativeStickyAssignor: Stickyë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì§€ë§Œ ì¼ë¶€ ì»¨ìŠˆë¨¸ ë³€ê²½ â†’ Zero-Downtime ë¦¬ë°¸ëŸ°ì‹±

```text
[
 "org.apache.kafka.clients.consumer.RangeAssignor",
 "org.apache.kafka.clients.consumer.RoundRobinAssignor",
 "org.apache.kafka.clients.consumer.StickyPartitionAssignor",
 "org.apache.kafka.clients.consumer.CooperativeStickyAssignor"
]
```

## Consumer ì„±ëŠ¥ ìµœì í™”
### Commit ê´€ë ¨ ì„¤ì •
1. enable.auto.commit: ì£¼ê¸°ì ìœ¼ë¡œ offsetì„ ì»¤ë°‹, ê¸°ë³¸ê°’ `true`
2. auto.commit.interval.ms: ì£¼ê¸°ì ìœ¼ë¡œ offsetì„ ì»¤ë°‹í•˜ëŠ” ê°„ê²©, ê¸°ë³¸ê°’ 5ì´ˆ

```python
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'auto-commit-topic',
    bootstrap_servers='localhost:9092',
    group_id='test-group',
    enable_auto_commit=True,             # ìë™ ì»¤ë°‹ í™œì„±í™” (ê¸°ë³¸ê°’)
    auto_commit_interval_ms=5000,        # 5ì´ˆë§ˆë‹¤ ì˜¤í”„ì…‹ ì»¤ë°‹
    auto_offset_reset='earliest',
    value_deserializer=lambda v: v.decode('utf-8')
)

for msg in consumer:
    print(f"Received: {msg.value} (Offset: {msg.offset})")
```
ğŸ’¡ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜¤ë©´ 5ì´ˆë§ˆë‹¤ ì»¤ë°‹ â†’ ì‹¤íŒ¨í•´ë„ ì»¤ë°‹ â†’ ë°ì´í„° ìœ ì‹¤ ê°€ëŠ¥


## Consumer ì„±ëŠ¥ ìµœì í™”
### Commit ê´€ë ¨ ì„¤ì • (ìˆ˜ë™ ì»¤ë°‹)
1. enable.auto.commit: ì£¼ê¸°ì ìœ¼ë¡œ offsetì„ ì»¤ë°‹, ê¸°ë³¸ê°’ true
2. auto.commit.interval.ms: ì£¼ê¸°ì ìœ¼ë¡œ offsetì„ ì»¤ë°‹í•˜ëŠ” ê°„ê²©, ê¸°ë³¸ê°’ 5ì´ˆ

```python
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    'manual-commit-topic',
    bootstrap_servers='localhost:9092',
    group_id='test-group',
    enable_auto_commit=False,            # ìˆ˜ë™ ì»¤ë°‹ ì‚¬ìš©
    auto_offset_reset='earliest',
    value_deserializer=lambda v: v.decode('utf-8')
)

for msg in consumer:
    print(f"Processing: {msg.value} (Offset: {msg.offset})")

    # ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ ì™„ë£Œ í›„ ì˜¤í”„ì…‹ ì»¤ë°‹
    consumer.commit()
    print(f"Committed offset {msg.offset}")
```

ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ ì»¤ë°‹ í˜¸ì¶œ â†’ ì •ìƒì²˜ë¦¬ í™•ì¸ í›„ ì»¤ë°‹ (ë°ì´í„° ìœ ì‹¤ ë°©ì§€)

## Consumer ì„±ëŠ¥ ìµœì í™”
### ìˆ˜ë™ Commit ì „ëµ
1. Batch ë‹¨ìœ„ ì²˜ë¦¬: í”„ë¡œë“€ì„œì˜ Batchì²˜ëŸ¼ ì¼ì • ë°ì´í„° ëª¨ì´ë©´ ì²˜ë¦¬ â†’ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì í•©  
2. ì£¼ê¸° ì²˜ë¦¬: ì¼ì • ì‹œê°„ë§ˆë‹¤ ì²˜ë¦¬ â†’ ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ì í•© (ì˜ˆ: ì‹¤ì‹œê°„ ë¡œê·¸)  
3. ì •ìƒì²˜ë¦¬ í›„ ì»¤ë°‹: ë©”ì‹œì§€ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ í™•ì¸ í›„ ì»¤ë°‹ â†’ ì•ˆì „í•˜ì§€ë§Œ ë†’ì€ ë¶€í•˜ (ì˜ˆ: ê¸ˆìœµ ì‹œìŠ¤í…œ)  
4. 1+N Hybrid: Nê°œ ì²˜ë¦¬ í›„ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ì»¤ë°‹ â†’ ì•ˆì •ì„±ê³¼ íš¨ìœ¨ì„±ì˜ ì¤‘ê°„, Kafka ê¶Œì¥

```python
BATCH_SIZE = 100
messages = []
last_offset = None

for msg in consumer:
    process(msg)
    messages.append(msg)
    last_offset = msg.offset

    if len(messages) >= BATCH_SIZE:
        consumer.commit()
        print(f"Committed offset at {last_offset}")
        messages = []
```

## Consumer ì„±ëŠ¥ ìµœì í™”
### auto.offset.reset ì„¤ì •ê°’
- earliest: ê°€ì¥ ì´ˆê¸°ì˜ offset ê°’ìœ¼ë¡œ ì„¤ì • â†’ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•´ì•¼ í•  ë•Œ  
- latest: ê°€ì¥ ë§ˆì§€ë§‰ì˜ offset ê°’ìœ¼ë¡œ ì„¤ì • â†’ ì‹¤ì‹œê°„ ë°ì´í„° ì†Œë¹„  
- none: ì´ì „ offset ê°’ì„ ì°¾ì§€ ëª»í•˜ë©´ error ë°œìƒ  

```python
consumer = KafkaConsumer(
    'my-topic',
    bootstrap_servers='localhost:9092',
    group_id='my-group',
    auto_offset_reset='latest'  # ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì½ê¸°
)
```

Kafka ë©”ì‹œì§€ ë¡œê·¸ ì˜ˆì‹œ

| Offset | Message | Consumer ì²˜ë¦¬ |
|--------|----------|----------------|
| 0 | Hello |  |
| 1 | World |  |
| 2 | Kafka! |  |
| 3 | Streaming... |  |
| 4 | Start | Consumer: Start |
| 5 | New | Consumer: New |
| 6 | System | Consumer: System |

# Kafka ì£¼ìš” íŒŒë¼ë¯¸í„°ì˜ ì´í•´

## Topic ë° Replica ê´€ë ¨ ì„¤ì •
- num.partitions: íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì • (ê¸°ë³¸ê°’ 1, ê±°ì˜ ì•ˆ ì”€), í•œ ë²ˆ ì„¤ì • í›„ ì¤„ì¼ìˆ˜ ëŠ” ì—†ìŒ.  
- replication.factor: íŒŒí‹°ì…˜ë³„ replica ìˆ˜  
- min.sync.replicas: ì •ìƒ ë™ì‘í•´ì•¼ í•˜ëŠ” ìµœì†Œ replica ìˆ˜  

```bash
kafka-topics.sh --create --topic my-topic \
  --partitions 3 --replication-factor 2 \
  --bootstrap-server localhost:9092
```

```bash
kafka-topics.sh --alter --topic my-topic \
  --partitions 6 --bootstrap-server localhost:9092   # âœ… ê°€ëŠ¥

kafka-topics.sh --alter --topic my-topic \
  --partitions 1 --bootstrap-server localhost:9092   # âŒ ë¶ˆê°€ëŠ¥
```

## Topic ë° Replica ê´€ë ¨ ì„¤ì •
- num.partitions: íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì • (ê¸°ë³¸ê°’ 1, ê±°ì˜ ì•ˆ ì”€), í•œ ë²ˆ ì„¤ì • í›„ ì¤„ì¼ìˆ˜ ëŠ” ì—†ìŒ.  
- replication.factor: íŒŒí‹°ì…˜ë³„ replica ìˆ˜, ê¸°ë³¸ê°’ 1(ê±°ì˜ ì•ˆ ì”€)
- min.sync.replicas: ì •ìƒ ë™ì‘í•´ì•¼ í•˜ëŠ” ìµœì†Œ replica ìˆ˜  

```bash
kafka-topics.sh --create --topic my-topic \
  --partitions 3 --replication-factor 3 \
  --bootstrap-server localhost:9092
```

```json
{
  "version": 1,
  "partitions": [
    {"topic": "my-topic", "partition": 0, "replicas": [1, 2, 3]},
    {"topic": "my-topic", "partition": 1, "replicas": [2, 3, 1]},
    {"topic": "my-topic", "partition": 2, "replicas": [3, 1, 2]}
  ]
}
```

```bash
kafka-reassign-partitions.sh --execute \
  --bootstrap-server localhost:9092 \
  --reassignment-json-file reassigned.json
```

## ë„¤íŠ¸ì›Œí¬ ë° ë©”ëª¨ë¦¬ ì„¤ì • ìµœì í™”
### ë„¤íŠ¸ì›Œí¬ ë° ë©”ëª¨ë¦¬ ê´€ë ¨ ì£¼ìš” ì„¤ì •ê°’
- socket.send/receive.buffer.bytes: ë„¤íŠ¸ì›Œí¬ ë²„í¼ í¬ê¸°, ê¸°ë³¸ê°’ 100KB, 0 ì„¤ì • ì‹œ ìë™ ì¡°ì •(ì¶”ì²œ)  
- log.flush.interval.messages/ms: ë¡œê·¸ í”ŒëŸ¬ì‹œ ì£¼ê¸° ì¡°ì ˆ, ê¸°ë³¸ê°’ `Long.MAX_VALUE`  
- message.max.byte: ë¸Œë¡œì»¤ê°€ ìˆ˜ìš© ê°€ëŠ¥í•œ ë©”ì‹œì§€ ìµœëŒ€ í¬ê¸°, ê¸°ë³¸ê°’ 1MB, 10MB ì´ìƒ ë„˜ê¸°ì§€ ì•ŠëŠ”ê²ƒ ì¶”ì²œ 
- num.network.threads: ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜, ê¸°ë³¸ê°’ 3, CPU ì½”ì–´ ìˆ˜ì™€ ë¹„ìŠ·í•˜ê²Œ ì„¤ì •

ğŸ“„ config/server.properties
```properties
# 1. ë„¤íŠ¸ì›Œí¬ ìµœì í™”
socket.send.buffer.bytes=512000
socket.receive.buffer.bytes=512000
num.network.threads=8

# 2. ë©”ì‹œì§€ í¬ê¸° ìµœì í™” (ìµœëŒ€ 5MB í—ˆìš©)
message.max.bytes=5242880

# 3. ë¡œê·¸ í”ŒëŸ¬ì‹œ ì£¼ê¸° ìµœì í™”
log.flush.interval.messages=10000
log.flush.interval.ms=1000
```

## ë¸Œë¡œì»¤ ë¦¬ì†ŒìŠ¤ ìµœì í™”

### ê¸°íƒ€ ë¸Œë¡œì»¤ ë¦¬ì†ŒìŠ¤ ì„¤ì •ê°’
- KAFKA_HEAP_OPTS: ì¹´í”„ì¹´ê°€ ì‚¬ìš©í•  JVM í™ ë©”ëª¨ë¦¬ í¬ê¸°, ê¸°ë³¸ê°’ 1GB, 4~8GB ì¶”ì²œ
- num.io.threads: Disk I/O ìŠ¤ë ˆë“œ ìˆ˜, ê¸°ë³¸ê°’ 8, CPU ì½”ì–´ì— ë§ì¶° ë†’ì´ê¸°  
- replica.fetch.min.bytes: íŒ”ë¡œì›Œê°€ ë¦¬ë”ë¡œë¶€í„° ë°›ëŠ” ë°ì´í„° í¬ê¸°,ê¸°ë³¸ê°’ 1 Byte, 1MB ì •ë„ ì¶”ì²œ

ğŸ“„ terminal
```bash
export KAFKA_HEAP_OPTS="-Xmx8G -Xms8G"  # 8GB ë©”ëª¨ë¦¬ ì‚¬ìš©
```
> ë„ˆë¬´ ì»¤ì§€ë©´ (16GB ì´ìƒ) JAVA GCë¥¼ ê³ ë ¤í•´ì•¼ í•¨ 

ğŸ“„ config/server.properties
```properties
num.io.threads=16                      # I/O ìŠ¤ë ˆë“œ ê°œìˆ˜ ì¦ê°€
replica.fetch.min.bytes=1048576        # 1MB ì´ìƒ ëª¨ì•„ì„œ ë³µì œ
```
> HDDì¼ë•ŒëŠ” CPU ì½”ì–´ë³´ë‹¤ ì ê²Œ, NVMe ê°™ì€ ê³ ì„±ëŠ¥ ì¥ì¹˜ëŠ” ê·¸ ì´ìƒ
> í´ìˆ˜ë¡ ë„¤íŠ¸ì›Œí¬ íš¨ìœ¨ì´ ì˜¤ë¥´ì§€ë§Œ ì§€ì—° ì‹œê°„ì´ ê¸¸ì–´ì§

## Zookeeper ê´€ë ¨ ì„¤ì •
### Zookeeper ê´€ë ¨ ìµœì í™” ê°’
- maxClientCnxns: ì£¼í‚¤í¼ ìµœëŒ€ ì—°ê²° ìˆ˜, ê¸°ë³¸ê°’ 60, ë¸Œë¡œì»¤ 1ê°œë‹¹ 20 ì •ë„ í•„ìš”  
- syncLimit: ë¦¬ë”-íŒ”ë¡œì›Œ ìµœëŒ€ ì§€ì—° ì‹œê°„, ê¸°ë³¸ê°’ 10ì´ˆ  
- autopurge.snapRetainCount: ì €ì¥ ì¤‘ì¸ ìŠ¤ëƒ…ìƒ· ìˆ˜, ê¸°ë³¸ê°’ 3  
- autopurge.purgeInterval: ì €ì¥ëœ ë¡œê·¸ ì‚­ì œ ì£¼ê¸°, ê¸°ë³¸ê°’ 24ì‹œê°„  

ğŸ“„ ë‹¨ë… ì„¤ì¹˜ ì‹œ  
`zookeeper/conf/zoo.cfg`  

OR

ğŸ“„ ì¹´í”„ì¹´ ë‚´ì¥ ì£¼í‚¤í¼ ì‚¬ìš© ì‹œ  
`config/zookeeper.properties`  

```properties
syncLimit=5
maxClientCnxns=200
autopurge.snapRetainCount=3
autopurge.purgeInterval=24
```

# Kafka ë°ì´í„°ì˜ ì €ì¥

## Log ì €ì¥ ë°©ì‹ ìµœì í™”
### Segment ì €ì¥ ê´€ë ¨ ì„¤ì •ê°’

- log.retention.ms: ë¡œê·¸ ë³´ê´€ ì‹œê°„ (ê¸°ë³¸ 7ì¼), ì¥ê¸° ë³´ê´€ ì‹œ ë³„ë„ì˜ ì €ì¥ì¥ì¹˜ë¡œ ë°±ì—… ì¶”ì²œ
- log.segment.bytes: ì„¸ê·¸ë¨¼íŠ¸ í¬ê¸° ì„¤ì •, ê¸°ë³¸ê°’ 1GB  
- log.cleanup.policy: ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ(`delete`)í• ì§€ ë˜ëŠ” ì••ì¶•(`compact`)í• ì§€ ê²°ì •, ê¸°ë³¸ê°’ì€ 'delete', ë™ì‹œì— ì„¤ì •ë„ ê°€ëŠ¥  
- log.cleaner.enable: ë°ì´í„° ì •ë¦¬ ì‹œ í‚¤ë³„ ìµœì‹  ë¡œê·¸ë§Œ ë‚¨ê¸¸ì§€ ì—¬ë¶€, ê¸°ë³¸ê°’ false

ğŸ“„ config/server.properties
```properties
# 1. ë¡œê·¸ ë³´ê´€ ìµœì í™” (7ì¼ ìœ ì§€)
log.retention.ms=604800000

# 2. ì„¸ê·¸ë¨¼íŠ¸ í¬ê¸° ìµœì í™” (1GB)
log.segment.bytes=1073741824

# 3. ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì •ë¦¬ í™œì„±í™”
log.cleaner.enable=true

# 4. ë¡œê·¸ ì •ë¦¬ ì •ì±… (ì‚­ì œ ë˜ëŠ” ì••ì¶•)
log.cleanup.policy=delete,compact
```

## Log ì €ì¥ ë°©ì‹ ìµœì í™”
### Segment ì••ì¶•í•˜ê¸°
- ì•ì„œ ë‚˜ì™”ë˜ ë©”ì‹œì§€ì˜ ì••ì¶•ê³¼ëŠ” ë‹¤ë¦„  
- ë™ì¼í•œ key ê°’ì˜ ìµœì‹  ë°ì´í„°ë§Œ ë‚¨ê²Œ í•˜ëŠ” ê²ƒì´ ëª©í‘œ  

| offset | key | value |
|--------|------|--------|
| 0 | user123 | ë¡œê·¸ì¸ ì„±ê³µ |
| 1 | user456 | ê²°ì œ ì™„ë£Œ |
| 2 | user123 | ë¡œê·¸ì•„ì›ƒ |
| 3 | user456 | ìƒí’ˆ ì¡°íšŒ |
| 4 | user123 | íšŒì› íƒˆí‡´ |

â¡ ì••ì¶• í›„
| offset | key | value |
|--------|------|--------|
| 3 | user456 | ìƒí’ˆ ì¡°íšŒ |
| 4 | user123 | íšŒì› íƒˆí‡´ |