# 배치 처리 워크플로우 구축 (Airflow + Spark)
## 배치 워크플로우
### 배치 처리(Batch Processing)
일정량의 데이터를 모아서 한꺼번에 처리하는 방식
- 정해진 시간(ex. 매일 밤 12시) 또는 특정 이벤트(ex. 파일 업로드) 발생 시 실행
- 대량 데이터를 처리하는데 적합하며, 주로 ETL 파이프라인, 데이터 웨어하우스 적재 등에 활용
- 주요 활용 사례
  - 데이터 웨어하우스 적재(ex. Redshift, Snowflake, HDFS)
  - 사용자 로그 분석(ex. 하루 단위 사용자 접속 로그 집계)
  - 기계 학습 모델 학습을 위한 데이터 준비
- 즉, 여러 데이터 소스를 수집하여 일정 시간 단위로 처리한 후 결과를 목적지에 저장

  ![alt text](image-67.png)
  - source 예시
    - 웹로그, DB에 이미 저장된 데이터, 파일 시스템, API에서 가져와지는 데이터
  - Batch Processor 예시
    - spark
  - Destination 예시
    - 저장(데이터 웨어하우스, 데이터 마트), 분석용 저장소(엘라스틱서치)

### 실시간 처리(Real-time Processing)
- 데이터가 발생하는 즉시 실시간으로 처리하는 방식
- 지연 시간이 짧고, 스트리밍 데이터(예 : 실시간 로그, IoT 센서 데이터)에 최적화
- 주로 실시간 모니터링, 이상 탐지, 실시간 추천 시스템 등에 활용

- 즉, 데이터가 생성되자마자 거의 실시간으로 처리되어 목적지에 전달됨

  ![alt text](image-68.png)
  - source 예시
    - 웹 로그, 클릭, IoT 센서 등
  - Destination 예시
    - 실시간 대시보드, 경보(알림), 추천시스템 등
    - 데이터를 모으지 않고 발생 즉시 처리하는 것
  
### 배치 처리 vs 실시간 처리
| 항목 | 배치 처리 | 실시간 처리 |
|------|---------|------------|
| 데이터 처리 방식 | 일정량의 데이터를 모아서 한 번에 처리 | 데이터가 들어오는 즉시 처리 |
| 처리 속도 | 느림(분~시간 단위) | 빠름(밀리초~초 단위) |
| 적용 사례 | 데이터 웨어하우스, 머신러닝 학습 데이터 준비 | 실시간 이상치 탐지, 실시간 추천 시스템, IoT데이터 분석 |
| 리소스 사용 | 주어진 시간에만 리소스 사용 | 지속적인 리소스 사용 |
| 비용 | 상대적으로 저렴(일정한 리소스 사용) | 고비용 |
| 사례 | 매일 오후 20시 로그 데이터 집계 | 실시간 주식 거래 분석 |



### Apache Spark
대규모 데이터를 빠르고 효율적으로 처리하는 분산 데이터 처리 프레임워크

- Spark 특징
  - In-Memory Computing
    - Spark는 데이터를 메모리(RAM)에서 처리하기 때문에 디스크 I/O가 많은 Hadoop보다 훨씬 빠름
    - 메모리에서 데이터를 유지한 채 연산을 수행하여 반복 연산(ex. 머신러닝, 데이터 변환)이 Hadoop보다 최대 100배 빠름

  - 다양한 데이터 처리 방식 지원
    - Spark는 단순한 배치 처리가 아니라 여러 가지 방식으로 데이터를 처리할 수 있음
      - RDD(기본적인 Spark 데이터 구조)
      - DataFrame
      - Spark SQL

  - Scalability
    - 수십~수천 대의 클러스터 노드에서 병렬 실행 가능
    - AWS, Azure, Google Cloud 환경에서도 손쉽게 확장 가능

  - 배치 & 실시간 데이터 처리 모두 가능
    - Spark는 기본적으로 배치 처리를 지원하지만, 스트리밍 처리도 가능

### Spark Data Structure
- RDD (Resilient Distributed Dataset)
  - 분산된 데이터를 저장하고 처리하는 기본 단위 (Hadoop의 HDFS와 유사)
  - 변경 불가능(Immutable) → 안정적인 분산 처리를 지원
  - 여러 노드에서 병렬로 처리 가능

- DataFrame (Pandas와 유사)
  - 구조화된 데이터(테이블 형태)를 처리하는 최적화된 데이터 구조
  - Spark SQL과 연동 가능 → SQL 쿼리를 사용하여 데이터 변환

- Spark SQL
  - SQL을 활용해 데이터를 쉽게 조회하고 변환 가능
  - 다양한 데이터 소스(HDFS, S3, JDBC, Hive, Cassandra 등)에서 데이터를 가져올 수 있음

### Apache Spark 의 RDD(Resilient Distributed Dataset)
- RDD(Resilient Distributed Dataset)는 여러 Worker Node에 분산되어 저장되고 병렬 처리함
- Driver Node는 RDD의 연산을 스케줄링하고 각 Worker Node에 작업 분배
  - 중앙 제어 역할

    ![alt text](image-69.png)

- Apache Spark의 RDD 배치 처리 과정

  ![alt text](image-70.png)
  1. 데이터를 parallelize 또는 외부 소스에서 불러와 RDD를 생성
      - 어떤 연산도 수행되지 않은 상태 (준비 상태의 분산 데이터 구조)
  2. Transformation 연산(map, filter 등)은 RDD를 새로 생성하지만 즉시 실행되지는 않음
  3. 여러 Transformation이 체이닝되어 실행 계획(DAG)을 구성
  4. Action 연산이 호출될 때 DAG가 실행되어 실제 데이터 처리가 수행

### Apache Spark 의 Dataframe
Spark DataFrame은 Hive, CSV, JSON, RDBMS, XML, RDD, Cassandra 등 다양한 소스의 데이터를 통합하여 구조화된 형식으로 표현됨

- Spark SQL을 통해 생성된 DataFrame은 열(Column) 기반의 테이블 형태로 데이터를 처리하며, SQL 쿼리도 사용 가능

  ![alt text](image-71.png)
  - 어떤 데이터 형태이든 정형 데이터로 표현 가능하면 처리 할 수 있음

- Apache Spark의 DataFrame 배치 처리 과정
  
  ![alt text](image-72.png)
  1. 다양한 데이터 소스에서 `read()` 또는 `load()`를 통해 DataFrame을 생성
  2. 생성된 DataFrame에 `select()`, `filter()`, `groupBy()` 등의 Transformation 연산 적용
  3. 여러 단계의 Transformation이 체이닝되며, 새로운 DataFrame이 연속적으로 생성
  4. 마지막에 `show()`, `count()`, `write()`와 같은 **Action이 호출되어 실제 실행**이 이루어짐

### Apache Spark 배치 처리의 주요 활용 사례
- 데이터 웨어하우스 적재(ETL)
  - Spark를 활용해 데이터를 정제 & 변환 후 데이터 웨어하우스에 저장
  - ex. 매일 수집한 CSV 데이터를 Parquet 변환 후 Snowflake, BigQuery 적재

- 로그 데이터 분석
  - 웹사이트, 애플리케이션, 서버 로그 데이터를 분석하여 사용자 행동 분석
  - ex. 사용자의 클릭 로그를 분석하여 마케팅 전략 최적화

- 머신러닝 데이터 전처리
  - 대량의 데이터를 Spark에서 전처리하여 머신러닝 모델 학습에 사용
  - ex. 추천 시스템을 위한 사용자 행동 데이터 전처리

### Airflow에서 Spark를 활용
![alt text](image-73.png)
- 단순 Python 코드로 처리하기 어려운 대규모 데이터 처리
  - Pandas는 메모리에 로드할 수 있는 데이터 크기에 한계가 있음
  - 병렬 처리가 어려워 속도가 느림
  - 데이터를 여러 노드에서 나눠서 처리하는 기능이 없음

- Spark를 활용하여 대량 데이터 ETL 수행
  - 단순 SQL이나 Python으로는 복잡한 ETL 작업을 수행하기 어려움
  - 대규모 데이터 변환이 필요한 경우, Spark의 강력한 데이터 처리 기능이 필요

- Spark 작업을 모니터링하고 실패 시 자동 재시도할 수 있음
  - Spark 작업이 실패하면 Airflow에서 자동으로 감지하고 재시도 가능
  - Airflow Web UI에서 로그를 확인하고, 어떤 Task에서 문제가 발생했는지 쉽게 파악할 수 있음

### 배치 워크플로우에서 DAG의 역할
- 배치 워크플로우에서 DAG의 중요성
  - 자동화  
    - 사람이 직접 실행할 필요 없이 정해진 스케줄에 따라 실행
  - 유지보수 용이  
    - 실행 로그 및 실패 이력을 기록하여 문제 해결 가능 (Airflow web UI)
  - 확장성  
    - 여러 작업을 DAG 내에서 관리하고, 병렬 실행 가능
  - 재시도 및 오류 감지  
    - Task 실패 시 자동으로 재시도하여 안정적인 운영 가능
    - 어느 위치에서 에러 났는지 확인 가능하므로 용이하게 사용 가능

- 배치 워크플로우에서 DAG 처리 흐름 예시
  - 병렬 태스크와 순차 태스크가 함께 구성된 DAG로, 데이터 로드부터 후속 처리까지 단계별로 실행 흐름을 정의할 수 있음
  - 작업 간 의존성을 명확히 설정하고, 병렬 처리를 통해 전체 실행 시간을 단축할 수 있음

    ![alt text](image-74.png)

- DAG에서 CSV파일 처리 작업 실행 (CSV 파일 읽기 & 변환)
  - PythonOperator를 사용하여 CSV 파일 읽기 & 변환

    ![alt text](image-75.png)
    - `data_engineering\06_Airflow\Chapter4_dagSpark\dags\1_csv_test.py` 참고
    - pandas 기반 처리 (변환)
    - 이때, data 디렉토리 사용 권한을 부여해야 함
      ```sh
      # 읽기, 쓰기, 실행 권한 부여
      sudo chmod -R 777 output data
      ```
    - 사진엔 input data가 없어 `cat` 으로 만듦
    - `1_csv_test.py` 

- DAG에서 BashOperator를 활용한 데이터 전처리 스크립트 실행
  - Shell로 작성된 데이터 전처리 스크립트를 BashOperator로 실행

    ![alt text](image-76.png)
    - `data_engineering\06_Airflow\Chapter4_dagSpark\plugins\shell\process_csv.sh` 참고

### Airflow에서 배치 작업 최적화
- 병렬 실행 및 성능 최적화 방법

  ![alt text](image-77.png)


## Airflow Decorator
Python Operator를 직관적으로 쓸 수 있게하여 태스크 정의, 의존성 설정 등에 활용

### Airflow에서 배치 작업 최적화
- Airflow Decorators
  - Python 함수 데코레이터로 태스크 정의
    - `@task` 데코레이터를 사용하여 Python 함수를 간단하게 Airflow 태스크로 변환
    - 코드의 가독성 향상: 태스크 정의가 간결하고 직관적
    - DAG 정의 간소화: DAG과 태스크 간의 관계를 쉽게 정의할 수 있음

  - 주요 구성
    - `@task`: 함수 데코레이터, Python 함수가 Airflow 태스크로 변환
    - DAG 정의: dag 매개변수로 태스크가 속할 DAG을 정의
    - 태스크 간 의존성: 데코레이터를 사용하여 태스크 간 의존성 간단히 설정  

### Airflow Decorators 코드 비교
- 전통적인 방식
  - PythonOperator를 사용하여 각 태스크를 명시적으로 정의
  - XCom을 사용하여 데이터를 전달하고 의존성 설정

    ```python
    def extract_data():
        print("Extracting data...")
        return "raw_data"

    def transform_data(data):
        print(f"Transforming data: {data}")
        return f"transformed_{data}"

    def load_data(data):
        print(f"Loading data: {data}")
        return f"loaded_data"

    # DAG 정의
    with DAG(dag_id="example_dag", schedule_interval="@daily", start_date=datetime(2025, 11, 1)) as dag:

        # PythonOperator를 사용하여 각 태스크를 명시적으로 정의
        extract_task = PythonOperator(task_id="extract_data", python_callable=extract_data)
        transform_task = PythonOperator(task_id="transform_data", python_callable=transform_data, op_args=["{{ task_instance.xcom_pull(task_ids='extract_data') }}"])
        load_task = PythonOperator(task_id="load_data", python_callable=load_data, op_args=["{{ task_instance.xcom_pull(task_ids='transform_data') }}"])

        # 태스크 간 의존성 설정
        extract_task >> transform_task >> load_task
    ```

- Python 함수 데코레이터로 태스크 정의
  - `@task`: @task 데코레이터를 사용하여 Python 함수가 자동으로 Airflow 태스크로 변환
  - 데이터 전달: 함수 간 데이터 전달이 자동으로 처리되므로 XCom을 수동으로 사용하지 않음
  - 의존성 설정: 함수 간 의존성은 변수를 통해 자연스럽게 설정

    ```python
    # DAG 정의
    @dag(schedule_interval="@daily", start_date=datetime(2025, 11, 1))
    def example_dag():

        # @task 데코레이터로 Airflow 태스크로 변환
        @task
        def extract_data():
            print("Extracting data...")
            return "raw_data"

        @task
        def transform_data(data):
            print(f"Transforming data: {data}")
            return f"transformed_{data}"

        @task
        def load_data(data):
            print(f"Loading data: {data}")
            # 데이터 로딩 예시
            return f"loaded_{data}"

        # 함수 간 의존성 설정
        data = extract_data()   # 첫 번째 태스크
        transformed_data = transform_data(data)  # 두 번째 태스크
        load_data(transformed_data)  # 세 번째 태스크

    # DAG 실행
    example_dag()
    ```

## SparkSubmitOperator
### DAG에서 SparkSubmitOperator 활용법
- SparkSubmitOperator 개요 및 역할
  - Apache Airflow에서 Spark 애플리케이션을 실행하기 위한 전용 연산자
  - Spark 클러스터(YARN, Kubernetes 등)에 `.py`, `.jar`, `.scala` 파일 등을 제출(submit)
  - 복잡한 Spark 작업을 Airflow DAG 내에 통합하여 자동화된 데이터 파이프라인 구성 가능
  - DAG 태스크로서 Spark 작업 실행을 스케줄링 및 추적 가능함
  - SparkSubmit 명령어를 Python 코드로 대체하여 운영 효율성 확보

- SparkSubmitOperator 주요 파라미터
  - application
    - 실행할 Spark 애플리케이션 경로 (`.py`, `.jar` 등)
  - conf
    - Spark 설정 (ex. `"spark.executor.memory": "2g"`)
    - Spark 실제 실행시킬 때 원하는 설정
  - executor_memory, driver_memory
    - 리소스 지정
  - application_args
    - 애플리케이션에 전달할 인자 목록
  - conn_id
    - Spark 클러스터 연결 정보 (ex. spark_default)
  - name
    - Spark 작업 이름

- `docker-compose.yaml`에 Spark 서비스 추가하기

  ```yaml
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8083:8080"
      - "7077:7077"
    networks:
      - airflow
    command: /bin/bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags/scripts:/opt/airflow/dags/scripts
      - ${AIRFLOW_PROJ_DIR:-.}/output:/opt/airflow/output
      - ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data

  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8084:8081"
    networks:
      - airflow
    command: /bin/bash -c "sleep 5; /opt/spark/sbin/start-worker.sh $${SPARK_MASTER_URL} && tail -f /dev/null"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags/scripts:/opt/airflow/dags/scripts
      - ${AIRFLOW_PROJ_DIR:-.}/output:/opt/airflow/output
      - ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data
  ```

- JAVA_HOME 세팅 및 경로 설정

  ```yaml
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Seoul
    JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
  ```

- Docker 네트워크 설정

  ```yaml
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8083:8080"
      - "7077:7077"
    networks:
      - airflow
    command: /bin/bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags/scripts:/opt/airflow/dags/scripts
      - ${AIRFLOW_PROJ_DIR:-.}/output:/opt/airflow/output
      - ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data
  ```
  ```yaml
  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8084:8081"
    networks:
      - airflow
    command: /bin/bash -c "sleep 5; /opt/spark/sbin/start-worker.sh $${SPARK_MASTER_URL} && tail -f /dev/null"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}/dags/scripts:/opt/airflow/dags/scripts
      - ${AIRFLOW_PROJ_DIR:-.}/output:/opt/airflow/output
      - ${AIRFLOW_PROJ_DIR:-.}/data:/opt/airflow/data
  ```

- DAG/scripts 경로 & volume 마운트 설정

  ```yaml
  # volumes:
  - ${AIRFLOW_PROJ_DIR:-.}/airflow/dags:/opt/airflow/dags
  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
  - ${AIRFLOW_PROJ_DIR:-.}/airflow/plugins:/opt/airflow/plugins
  - ${AIRFLOW_PROJ_DIR:-.}/airflow/dags/scripts:/opt/airflow/dags/scripts  # scripts 폴더 추가
  ```
  ![alt text](image-78.png)

- Airflow UI에서 Spark Connection 추가 (★필수★)

  ![alt text](image-79.png)
  - Spark Connection 추가해야 Airflow가 해당 Connection id에 대한 정보를 알 수 있음

- SparkSubmitOperator를 통한 Spark Job 실행
  - `data_engineering\06_Airflow\Chapter4_dagSpark\dags\4_spark_submit_example.py` 참고

  ![alt text](image-80.png)

  ![alt text](image-81.png)

  ![alt text](image-82.png)

  ![alt text](image-83.png)


## DAG 실행 오류 확인
### DAG 실행 오류 확인(Airflow Web UI 로그)
- DAG 실행 중, 에러 발생할 경우 각 Task에 관한 상세 로그 확인

  ![alt text](image-84.png)

- DAG 실행 로그는 Airflow 환경에 저장됨

  ![alt text](image-85.png)

  ![alt text](image-86.png)
  