# AI강의Ⅱ Wrap Up
## AI를 위한 Python과 Math
- AI를 위한 준비과정  
  1. 파이썬 기본 문법
  2. 클래스  
  3. Numpy  
  4. Pandas  
  5. Matplot  
  6. Seabon  
  7. 선형회귀  
  8. 로지스틱회귀  
  9. Gradient Decent  

### 파이썬 기본 문법
- 함수
  ````python
  # 함수 기본 문법
  def sum1(a, b):
      return a + b

  # 파라미터 타입까지 표기
  def sum2(a: int, b: int):
      return a + b

  # 리턴 타입까지 표현 -> 디폴트
  def sum3(a: int, b: int) -> int:
      return a + b

  c = sum1(10, 20)
  print(c)
  ````
  - 현업에서는 리턴 타입까지 표현하는 함수 표기 방법을 주로 사용함

- 클래스
  ```python
  # 클래스 생성
  class Person:
    def __init__(self):
      self.name = 'HI' #instance attribute
      self.age = '30' #instance attribute

    def sayHi(self):
      print('안녕')

    def sayBye(self):
      print(f'잘가 {self.name} 님')


  # 객체 생성
  bts = Person()
  bts.sayHi()
  bts.sayHi()
  bts.sayBye()
  ```

### Numpy & 선형대수학 기본 내용
- 벡터 표현
  ```python
  import numpy as np

  v = np.array([[1, 2, 3],
                [1, 1, 5]])
  ```
  - Numpy를 이용해 `[1, 1]`, `[2, 1]`, `[3, 5]`와 같은 벡터를 표현함
  - 벡터는 **방향**과 **크기**를 가지는 값으로, 고정된 위치가 아니라 움직임이나 힘의 방향을 나타냄
    - 예를 들어, `(1, 1)`이라는 좌표는 고정 좌표계에서는 한 점을 의미하지만,
    - 벡터에서는 `(0, 0)`을 기준으로 `(1, 1)` 방향으로 힘을 가하는 것
    - ex. 권투 선수가 `(0, 0)` 위치에서 `(1, 1)` 방향으로 펀치를 뻗는 것이라고 이해할 수 있음
  - 이렇게 어떤 방향으로 얼마만큼의 영향을 주는지를 표현하는 단위가 바로 **벡터**

- 변환행렬
  ```python
  #변환행렬 1 : x축 방향 2배 스케일링
  S = np.array([[2, 0],
                [0, 1]])

  #변환행렬 2 : 90도 반시계 방향 회전
  R = np.array([[0, -1],
              [1, 0]])

  #변환행렬 합성
  RS = R @ S

  #합성된 변환행렬로 한방에 변환
  v_scaled = RS @ v

  print(v_scaled)
  ```
  - 선형적으로 공간을 변형하는 개념
    - 벡터에 **스케일링(크기 조정)** 이나 **회전(방향 변경)** 같은 선형 변환을 적용함

  - `S`: x축 방향으로 2배 스케일링
    - ex. 쨉하는 동영상을 가로로 늘리는 것처럼 벡터의 x성분을 2배로 키움

  - `R`: 90도 반시계 방향 회전
    - ex. 쨉을 왼쪽으로 틀어주는 것과 같음 (방향만 바뀌고 크기는 그대로)
  
  - `RS`: 변환행렬 두 개를 한 번에 적용한 것
    - 먼저 스케일링, 그다음 회전 순서
  
  - 이러한 변환행렬은 MLP(다층 퍼셉트론)에서 `y = Wx + b` 연산을 할 때 자주 사용됨
    - ex. 입력 벡터 x에 가중치 행렬 W를 곱하는 것이 바로 선형변환의 예시

### Pandas
- 용도: 파이썬으로 엑셀을 하는 것과 같음
  - 데이터 확인
  - 불필요한 데이터 삭제
  - EDA와 전처리
    - AI에서 주로 사용
  - csv 파일 불러오기

- 데이터 확인
  ```python
  import seaborn as sns
  import pandas as pd

  # 아이리스(꽃) 데이터셋 읽어오기 (Seaborn 공식 자료)
  df = pd.read_csv("https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv")
  df.head(5) #상위 5줄만 가볍게 출력
  ```
  ![alt text](image.png)

- 불필요한 데이터 삭제 및 필터링
  ```python
  df_sub = df.drop(columns=['sepal_length', 'sepal_width']) # drop = 시리즈 제거
  df_filtered = df_sub[df_sub["petal_width"] >= 0.3] # 필터링
  df_sub.head(5)
  ```
  ![alt text](image-1.png)

### 시각화 (Matplot, Seaborn, Plotly)
- Matplot으로 시각화
  ```python
  import matplotlib.pyplot as plt
  import numpy as np

  # 데이터 준비
  x = np.linspace(0, 10, 15)   # 0 ~ 10 사이를 20등분
  y = np.sin(x)

  # 라인 그래프 생성
  plt.plot(x, y, label="sin(x)", color="blue")

  # 그래프 설정
  plt.title("Matplot Example")  # 제목
  plt.xlabel("x axis")          # x축 라벨
  plt.ylabel("y axis")          # y축 라벨
  print()
  ```
  ![alt text](image-2.png)

- Seaborn으로 시각화
  ```python
  import numpy as np
  import seaborn as sns

  # 데이터 준비
  x = np.linspace(0, 10, 20) # 0 ~ 10 까지 20 등분
  y = np.sin(x)

  # 라인 그래프 생성 후, 설정을 위한 객체 반환
  ax = sns.lineplot(x=x, y=y, label="sin(x)", color="blue")

  # 그래프 설정
  ax.set_title("Seaborn Example")
  ax.set_xlabel("x axis")
  ax.set_ylabel("y axis")
  print()
  ```
  ![alt text](image-3.png)
  - Matplot과 사용 방법 동일함

- Plotly으로 시각화
  ```python
  import numpy as np
  import plotly.graph_objects as go

  # 데이터 준비
  x = np.linspace(-5, 5, 100)
  y = np.linspace(-5, 5, 100)
  X, Y = np.meshgrid(x, y)
  Z = np.sin(X) * np.cos(Y)

  # 3D Surface 그래프 생성
  fig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y, colorscale='Viridis')])

  # 그래프 설정
  fig.update_layout(
      title='3D Surface: sin(x) * cos(y)',
      scene=dict(
          xaxis_title='X axis',
          yaxis_title='Y axis',
          zaxis_title='Z axis'
      ),
      template='plotly_white',
      width=500,   # 가로 크기
      height=500   # 세로 크기
  )

  fig.show()
  ```
  ![alt text](image-4.png)

- 시각화 도구를 3가지(Matplot, Seaborn, Plotly)나 사용하는 이유
  1. Matplot
      - 데이터를 차트로 표현하는 기본 Library
      - 가장 많이 사용(가장 기본적)
  2. Seabon
      - Matplot 기반으로 만들어짐
      - Pandas와 함께 사용하면 코드를 한줄로 작성해 그래프 그릴 수 있음
  3. Plotly
      - 3D 표현 가능
      - 마우스 반응형(인터랙티브 그래프)

※참고: 시대의 흐름과 시작점
- 유비쿼터스 시대 -> IoT 시대 -> 빅데이터 시대 -> Data Science 시대를 거쳐 'AI 시대'가 찾아옴
- Data Science의 시작은 3개 Library 활용으로 정립됨
  - `Numpy`, `Pandas`, `Matplot`
    - Numpy에서 제공되는 타입으로 소수점 연산을 빠르게 계산하고
    - 그 값을 Pandas로 모아 분석하고, 필터링하고, 전처리함
    - 최종적으로 Matplot을 이용해 시각화까지 진행
- 이렇게 3개 라이브러리를 다루는 과정이
차후 EDA 학습의 빌드업이 됨

### 선형회귀모델
- 추세선 방정식을 의미함
  ```python
  import seaborn as sns
  import matplotlib.pyplot as plt

  # 예시 데이터 : 팁스
  tips = sns.load_dataset("tips")

  # 모델 에시 (선형 회귀 모델)
  # reg plot은 추세선을 그려주는 기능을 가지고 있습니다.
  sns.regplot(x="total_bill", y="tip", data=tips, ci=None, line_kws={"color": "red"})
  print()
  ```
  ![alt text](image-5.png)
  - 추세선을 찾아내면 '미래 데이터'를 예측할 수 있음

※ 선형회귀모델의 '회귀'는 값을 예측하는 방법을 의미함("과거로 돌아가다"는 의미XX)

- 추세선 알아내는 원리
  1. 실제 값과 추세선의 오차를 구함
      - 예측값 $y=ax+b$ 와 실제 데이터의 차이를 계산
  
        ![alt text](image-6.png)

  2. 오차 제곱의 합(MSE)을 계산
      - $MSE = 1/n \sum(y_{예측} - y_{실제})^2$
      - 오차를 제곱해 합한 값이 작을수록 모델이 데이터를 잘 맞춘다는 의미
  
  3. a, b 값을 반복하며 최소 MSE를 찾음 (완전 탐색)
      ```python
      for a in range(1, 100):
          for b in range(1, 100):
              y = a*x + b
              # 실제 데이터와의 MSE 계산
              # MSE가 최소가 되는 a, b 선택
      plot(y = min_a*x + min_b)
      ```
      - 이 방법은 계산량이 많아 시간이 오래 걸림
  
  4. Gradient Descent로 최적화
      - 임의의 a, b에서 시작해 MSE가 줄어드는 방향으로 조금씩 이동
      - 경사가 있는 그래프를 따라 점차 내려가며 최소값(최적 a, b)을 찾음
      - 훨씬 빠르고 효율적
      - a 값에 대한 MSE 변화를 나타낸 예시 그림

        ![alt text](image-7.png)
  
  5. 이동하는 크기 = 학습률 (learning rate)
      - 경사를 따라 이동할 때, 한 번에 **얼마만큼 이동할지** 결정하는 값
      - 이 값을 너무 크게 잡으면, 최소 지점을 지나쳐서 양쪽으로 계속 튀는 현상 발생
      - 이 값을 너무 작게 잡으면, 너무 느리게 이동해서 학습 속도가 매우 느려짐
      - 학습률은 사람이 직접 정하는 값 -> **하이퍼파라미터** (모델이 스스로 찾지 못하고, 사람이 직접 설정해줘야 하는 값)

- $y=ax+b$에서 MSE가 가장 작아지는 a, b 지점 시각화
  - a(기울기), b(절편)를 기준으로 계산된 MSE 손실곡면 위에서 최소값을 가지는 지점을 시각화한 그래프
    
    ![alt text](image-9.png)
    - 빨간 점은 Gradient Descent가 이동한 경로를 의미
    - $z$축 = MSE

### 로지스틱회귀
- A에 속하는지, B에 속하는지 확률을 구하는 분류 모델
  - 주어지는 데이터
    - 2시간 공부 한 A 학생 : 불합격
    - 3시간 공부 한 B 한색 : 불합격
    - 4시간 공부 한 C 학생 : 합격
    - 3시간 공부 한 D 학생 : 불합격
    - 5시간 공부 한 E 학생 : 합격
    - 1시간 공부 한 F 학생 : 불합격
  
  ```python
  import numpy as np
  import matplotlib.pyplot as plt

  # 공부 시간 (X), 합격 여부 (y)
  X = np.array([1,2,3,4,5,6,2,3,4,5,6,7,1,2,3,4,5,6,7,8,2,3,4,5,6,7,8,9,10])
  y = np.array([0,0,0,1,1,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1])

  def sigmoid(z):
      return 1 / (1 + np.exp(-z))

  # 하이퍼 파라미터 초기화
  lr = 0.1  # 학습률
  epochs = 5000

  # a, b 초기값 세팅
  np.random.seed(42)  # 재현성 고정
  a = np.random.randn()  # 기울기
  b = np.random.randn()  # 절편

  # Gradient Descent
  for epoch in range(epochs):
      # 예측
      y_hat = sigmoid(a * X + b)

      # Loss (Cross Entropy)
      eps = 1e-10
      loss = -np.mean(y*np.log(y_hat+eps) + (1-y)*np.log(1-y_hat+eps))

      # 기울기 계산
      error = y_hat - y
      grad_a = np.mean(error * X)
      grad_b = np.mean(error)

      # 파라미터 업데이트 (점프!)
      a -= lr * grad_a
      b -= lr * grad_b

      # Loss값 출력
      if epoch % 500 == 0:
        print(f"Epoch {epoch}, Loss={loss:.4f}, a={a:.4f}, b={b:.4f}")

  # 학습 결과
  print()
  print("최적 기울기 a:", a)
  print("최적 절편 b:", b)
  print("최종 Loss:", loss)

  # 시각화
  X_test = np.linspace(0, 10, 200)
  y_prob = sigmoid(a * X_test + b)

  plt.plot(X_test, y_prob, color="red", linewidth=2)
  plt.xlabel("Study Time")
  plt.ylabel("Pass Probability")
  plt.grid(True)
  ```
  ![alt text](image-10.png)
  ![alt text](image-11.png)
  - 로지스틱 회귀도 MSE가 가장 낮은 a, b 찾는 것이 목적
  - 시그모이드 함수에 a, b를 넣으면 '확률'이 나옴
  - 결과
    - 4시간 공부하면 합격확률 0.85
    - 즉, 가성비있는 공부 시간은 4시간!
    - 6시간이상 공부하는 것은 의미XX
  
- 선형회귀 vs 로지스틱 회귀
  - 선형회귀
    - $y = ax + b$ 형태
  
  - 로지스틱 회귀
    - $y = sigmoid(ax + b)$ 함수 형태
      ```python
      def sigmoid(z):
          return 1 / (1 + np.exp(-z))
      ```

  - 비교
    ```python
    for epoch in range(epochs):
          # 예측
          y_hat = a * X + b           # 선형회귀 선 그려짐 (추세선)
          y_hat = sigmoid(a * X + b)  # 로지스틱회귀 선 그려짐 (확률선)

    . . .
    ```


## EDA
- EDA (Exploratory Data Analysis)
  - 데이터를 살펴보는 행동
    - 데이터 샘플을 확인  
    - 필드 확인  
    - 기본 통계량 확인  
    - 분포 확인  
    - 상관계수 확인  
    - 등등

- 데이터 샘플 확인
  ```python
  import seaborn as sns

  # seaborn 내장 데이터 셋 불러오기 (데이터 프레임)
  df = sns.load_dataset("mpg")
  df.head()
  ```
  ![alt text](image-12.png)

- 변수 간 관계 살펴보기 (feat.시각화)
  ```python
  import seaborn as sns

  df = sns.load_dataset("mpg")

  sns.regplot(data=df, x="weight", y="acceleration", scatter_kws={'alpha':0.6})
  print()
  ```
  ![alt text](image-13.png)

- 상관계수 (feat.히트맵)
  - 뭐랑 뭐랑 상관있습니까? 할 때의 그 상관임
  - 상관계수가 1에 가까우면 양의 상관관계, -1에 가까우면 음의 상관관계가 있는 것
    ```python
    import numpy as np
    import seaborn as sns

    df = sns.load_dataset("mpg")

    # 상관계수 행렬 계산
    corr = df.corr(numeric_only=True)

    # 삼각형 마스크 만들기 (상단 삼각형 가리기)
    mask = np.triu(np.ones_like(corr, dtype=bool))

    # 히트맵 출력
    sns.heatmap(corr, mask=mask, annot=True, cmap="coolwarm", linewidths=1)
    ```
    ![alt text](image-14.png)
    - 연비(mpg)와 가장 큰 상관관계를 갖는 데이터는 'weight'
      - 절댓값이 가장 크기 때문
      - 즉, 무게(weight)가 높아질수록 연비(mpg)가 낮아짐
  
- 추세선 및 분포 확인 (feat. pair plot)
  ```python
  import seaborn as sns
  import matplotlib.pyplot as plt

  df = sns.load_dataset("mpg")

  # pairplot 시각화
  sns.pairplot(df[["mpg", "weight", "acceleration"]], corner=True, kind="reg", plot_kws={'line_kws': {'color': 'red'}})
  print()
  ```
  ![alt text](image-15.png)
  - 데이터 간 추세선을 시각적으로 확인할 수 있음
  - 대각선에는 각 변수의 분포(histogram)가 표시됨
  - 하지만 분포만으로는 데이터 전체를 설명할 수 없음
    - ex. 똑같은 모양의 분포라도 평균, 분산 등이 다를 수 있음
  - 그래서 분포를 설명할 수 있는 수치적인 값들이 필요함
    - ex. 최솟값(min), 최댓값(max), 평균(mean), 분산(var), 표준편차(std) 등


## MLP
### MLP (Multi-Layer Perceptron)
- 하나 이상의 은닉층(hidden layer) 을 가진 신경망 구조
  - 딥러닝에서 가장 기본이 되는 모델 구조 중 하나

- 신경망 구조의 수식
  - 단순하게 선형회귀처럼 $y = Wx + b$ 형태로 표현할 수 있음
  - 즉, 단순한 추세선도 신경망으로 그릴 수 있음
    ```python
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import matplotlib.pyplot as plt
    import seaborn as sns

    # 데이터 준비 (Seaborn의 tips 데이터셋)
    tips = sns.load_dataset("tips")

    # 입력(X): total_bill, 출력(y): tip
    # 아래 dtype은 데이터 타입을 float 으로 저장하라는 뜻입니다
    X = torch.tensor(tips["total_bill"].values, dtype=torch.float32).reshape(-1, 1)
    y = torch.tensor(tips["tip"].values, dtype=torch.float32).reshape(-1, 1)

    # 모델 정의 - 선형함수 y = wx + b 하나 생성
    model = nn.Linear(1, 1)  # 입력 1개(total_bill), 출력 1개(tip)

    # 손실함수 & 옵티마이저
    # 옵티마이저 = weight, bias를 변경하는 알고리즘 종류, GD를 사용합니다.
    criterion = nn.MSELoss() # MSE 사용
    optimizer = optim.SGD(model.parameters(), lr=0.001) # 옵티마이저 = GD

    # 학습 루프
    for epoch in range(2000): # 점프 2000번~!
        y_hat = model(X) #모델 예측

        loss = criterion(y_hat, y) #로스 계산

        optimizer.zero_grad() #미분 계산 초기화 (미분 Ready)
        loss.backward() #미분 계산 수행

        optimizer.step() #GD 점프 1회 (w, b 값이 업데이트 됩니다.)

        if epoch % 400 == 0:
            print(f"Epoch {epoch:4d} | Loss: {loss.item():.4f}")

    # 학습 결과 출력
    print()
    print('[학습완료]')
    print(f'학습된 가중치(weight): {model.weight.item()}')
    print(f'학습된 편향(bias)) : {model.bias.item()}')
    print()

    # 예측 선 준비
    with torch.no_grad():
      x_line = torch.linspace(X.min(), X.max(), 100).reshape(-1, 1)
      y_line = model(x_line)

    # 시각화
    plt.scatter(X.numpy(), y.numpy(), color='skyblue', alpha=0.6)
    plt.plot(x_line.numpy(), y_line.numpy(), color='red', linewidth=2)
    plt.title("Linear Regression with PyTorch")
    plt.xlabel("Total Bill ($)")
    plt.ylabel("Tip ($)")
    plt.grid(True)
    ```
    ![alt text](image-16.png)

    ![alt text](image-17.png)
    - 선형 회귀식 $y=ax+b$ (혹은 $y=Wx+b$)는 직선만 표현 가능 -> 복잡한 데이터에는 한계가 있음
    - 그래서 여기에 활성화 함수(Activation Function) 를 넣음
    
- ReLU
  - 대표적인 활성화 함수로, 0 이하일 땐 0, 그 외(양수)엔 그대로 출력하는 함수임
  - 이걸 층(layer) 사이사이에 넣으면 모델이 비선형성을 학습하게 됨

    ![alt text](image-20.png)
  - 즉, 단순 선형 함수였던 $y=Wx+b$가
    - ReLU를 중간에 끼우고, 여러 번 쌓이면서
    - 곡선, 꺾인 선, 복잡한 형태 등 모두 학습할 수 있게 됨
      
      ![alt text](image-19.png)

- 이렇게 여러 층을 차례로 연결하는 방식을 AI에서는 Sequential Connection (순차 연결) 이라고 부름
  - 함수의 중첩과 ReLU 활성화 함수를 이용하면, 모든 선을 다 그릴 수 있음

### MLP 예시: MNIST 숫자 데이터셋 분류
- 모델 학습
  ```python
  import pandas as pd
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import DataLoader

  #===
  # 1. 데이터 준비 (csv -> data loader에 입력 해두기 까지)
  #===
  train_df = pd.read_csv("sample_data/mnist_train_small.csv")

  # 전처리를 안하니까 가끔씩 학습 실패해서... / 255로 스케일링이 필요했습니다. (0 ~ 1 값으로 정규화)
  X_train = torch.tensor(train_df.iloc[:, 1:].values, dtype=torch.float32) / 255.0
  y_train = torch.tensor(train_df.iloc[:, 0].values, dtype=torch.long)

  # Data Loader 세팅 완료
  train_dataset = list(zip(X_train, y_train))
  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

  #===
  # 2. 모델 준비
  #===
  # Layer의 개수, 중간 Layer의 입력과 출력의 개수는 모두 하이퍼파라미터 입니다.
  model = nn.Sequential(
      nn.Linear(784, 256), # 한 batch당 입력과 출력 개수를 적는 것을 잊지마세요.
      nn.ReLU(),
      nn.Linear(256, 128),
      nn.ReLU(),
      nn.Linear(128, 10) # 최종 출력값은 10개 입니다. '0' ~ '9' 의 점수 값이 나옵니다.
  )

  #===
  # 3. Loss & Optimizer (Adam)
  #===
  loss_fn = nn.CrossEntropyLoss()
  optimizer = optim.Adam(model.parameters(), lr=0.001)

  #===
  # 4. 학습
  #===
  epochs = 5
  for epoch in range(epochs):
      total_loss = 0
      for X_batch, y_batch in train_loader:

          outputs = model(X_batch) # 예측

          loss = loss_fn(outputs, y_batch) # loss 계산 (softmax도 함께 수행함)

          optimizer.zero_grad() # 미분 엔진 초기화
          loss.backward() # 미분 계산
          optimizer.step() # 파라미터 업데이트

          total_loss += loss.item() # Loss 누적

      print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}")

  #===
  # 5. 학습 완성된 파라미터들, 파일로 저장
  #===
  # 학습 결과로 찾아낸 파라미터 값들을 파일로 저장합니다.
  # .pth 확장자로 저장합니다. (pth = pytorch 약어)
  torch.save(model.state_dict(), "model.pth")
  ```
  ![alt text](image-21.png)
  1. CSV 데이터 불러오기
      - 손글씨 이미지 데이터를 pandas로 불러오고
      - torch.tensor로 바꿔서 0~1 사이로 정규화

  2. DataLoader에 넣기
      - 데이터를 (X, y) 형태로 묶어서 batch 단위로 불러올 수 있게 준비

  3. MLP 모델 구성
      - 입력 784 → 은닉층 256 → 은닉층 128 → 출력 10
      - 각 층 사이에 ReLU 활성화 함수 사용

  4. Loss 함수 & Optimizer 설정
      - CrossEntropyLoss 사용 (Softmax + Loss 포함)
      - 최적화는 Adam 사용, 학습률은 0.001 (하이퍼파라미터)

  5. 학습 루프
      - `for epoch in range(epochs)`로 반복하면서
      - 모델이 예측 → Loss 계산 → 역전파 → 파라미터 업데이트

  6. 학습 완료 후 저장
      - 학습된 모델의 파라미터들을 .pth 파일로 저장
  
  - 즉, 픽셀 하나하나를 입력해서, 0부터 9까지 숫자를 구분하는 방정식을 MLP가 직접 학습한 것

- 학습된 모델로 추론 (모델 불러와서 테스트 데이터에 적용)
  ```python
  import pandas as pd
  import torch
  import torch.nn as nn
  from torch.utils.data import DataLoader
  import matplotlib.pyplot as plt

  #===
  # 1. 모델 구조 정의 (학습할 때랑 똑같아야 함)
  #===
  model = nn.Sequential(
      nn.Linear(784, 256),
      nn.ReLU(),
      nn.Linear(256, 128),
      nn.ReLU(),
      nn.Linear(128, 10)
  )

  #===
  # 2. 모델 불러오기
  #===
  model.load_state_dict(torch.load("model.pth"))  # 저장했던 파일 로드

  #===
  # 3. 테스트 데이터 준비
  #===
  test_df = pd.read_csv("sample_data/mnist_test.csv")

  # 학습했던 것과 정규화 똑같이 해주기
  X_test = torch.tensor(test_df.iloc[:, 1:].values, dtype=torch.float32) / 255.0
  y_test = torch.tensor(test_df.iloc[:, 0].values, dtype=torch.long)

  # DataLoader 준비 완료
  test_dataset = list(zip(X_test, y_test))
  test_loader = DataLoader(test_dataset, batch_size=3, shuffle=True) # batch = 3 장씩

  #===
  # 4. 모델에 입력 데이터 이미지 3장 선택
  #===
  images = None
  for X_batch, y_batch in test_loader:
    images = X_batch
    break

  # 입력할 이미지 출력
  print('입력 이미지')
  for i in range(3):
      plt.subplot(1, 3, i+1)
      record = images[i].reshape(28, 28);
      plt.imshow(record, cmap="gray")
      plt.axis("off")
  plt.show()
  print()

  #===
  # 5. 추론하기!
  #===
  with torch.no_grad():
      outputs = model(images)
      preds = torch.argmax(outputs, dim=1) # 10개의 output 값중 가장 큰 값의 index 찾기
      print(f'모델 추론 결과 : {preds}')
  ```
  ![alt text](image-22.png)
  1. 모델 구조 다시 정의하기
      - 모델을 불러올 땐, 학습할 때와 같은 구조로 다시 만들어줘야 함

  2. 저장된 파라미터 불러오기
      - `torch.load("model.pth")`로 `.pth` 파일 로드
      - `model.load_state_dict()`로 모델에 파라미터 적용

  3. 테스트 데이터 불러오기
      - 학습 때와 마찬가지로 pandas로 CSV 파일 읽고
      - 0~1 사이로 정규화까지 동일하게 진행
      - DataLoader에 넣어서 batch 단위로 사용 (여기선 3장씩)

  4. 이미지 출력해보기
      - test 이미지 중 3장만 꺼내서 matplotlib으로 시각화
      - 실제로 우리가 분류할 이미지를 확인

  5. 추론(Inference)
      - `model(images)`로 예측 결과 계산
      - argmax를 통해 가장 높은 점수를 받은 숫자 선택
      - 최종적으로 모델이 인식한 숫자 출력

  - 즉, 학습한 MLP 모델에 테스트 이미지를 넣고, 모델이 어떤 숫자라고 판단했는지 추론한 결과를 확인함

- MLP 구조 정리 (Multi-Layer Perceptron)
  ```python
  model = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 10)
    )
  ```
  - 784 → 256 → 128 → 10
    - 입력층부터 출력층까지 순차적으로 연결된 신경망 구조
    - 각 숫자는 각 층의 출력 노드(=뉴런) 개수를 의미함
      | 층    | 노드 수    | 의미                            |
      | ---- | ------- | ----------------------------- |
      | 입력층  | **784** | 28×28 이미지 → 픽셀 하나당 하나의 입력값    |
      | 은닉층1 | **256** | 첫 번째 중간 계산 단계                 |
      | 은닉층2 | **128** | 두 번째 중간 계산 단계                 |
      | 출력층  | **10**  | 0~9 숫자 중 어떤 숫자인지 예측 (클래스 수만큼) |

  - 수식으로 표현하면?

    ![alt text](image-23.png)
    - $x$: 입력 벡터 (784차원, 이미지 1장)
    - $W_n, b_n$: 각 층의 가중치와 편향
    - $ReLU$: 비선형성 추가를 위한 활성화 함수
    - $y$: 최종 출력 (10차원 → 숫자 0~9 각각의 점수)

### MLP 결론
MLP는 선형 연산 $y=Wx+b$에 ReLU 같은 활성화 함수와 함수의 중첩(층 쌓기)을 더함으로써, 비선형적인 함수도 학습할 수 있는 모델이다


---
---
---
## 토큰화 / 임베딩


### 토큰화
- 문장을 토큰 단위로 나누는 과정

### 임베딩
- 각 토큰화된 단어들에게 의미를 부여하는 과정
- 의미공간의 Vector값을 갖게된다.


## 합성데이터와 데이터 증강

### 합성데이터 (Synthetic Data)
- 실제 세계에 없는 새로운 데이터를 생성하는 것

### 데이터 증강 (Data Augmentation)
- 실제 데이터를 변형하여 데이터 셋을 확보하는 것


## CNN

### CNN (Convolutioonal Neural Network)
- 이미지, 영상에서 시각 데이터의 특징을 기반으로 사물을 인식할 수 있는 딥러닝 알고리즘
- 2012년 AI겨울을 끝내고, CNN 기반의 AlexNet을 통해 AI 시대를 다시 열게 되었음


## RNN / LSTM


### RNN (Recurrent Beural Network)
- AI에서 핵심 딥러닝 알고리즘 중 하나
- 시퀀스 데이터를 학습시키고, 추론시키는데 특화된 딥러닝
- Attention의 필요성을 이해하기 위해 학습했음

### LSTM(Long Short-Term Memory)
- RNN의 가장 큰 단점: 과거 정보를 오래 기억하지 못한다.
  - 과거 정보가 사라지는 이유: 이전 정보를 계속 곱하면서 진행하기 때문(기울기 소실 발생)
- LSTM은 입력/출력/망각 게이트를 통해 중요한 정보는 기억하고 필요 없는 정보는 버림
  - 긴 문맥도 이해할 수 있게 됨
- LSTM의 한계점: 문맥 내 모든 단어 간의 관계를 한 번에 파악하기 어려움



## Attention과 Transformer 모델


### Attention
- 입력데이터에 중요한 부분에 집중할 수 있게 돕는 기술
- 문맥을 이해하는 벡터를 만들어는 역할을 함

### Transformer
- 어텐션을 인코더, 디코더 형태로 활용하여 Foundation Medel 시대를 열게된 딥러닝 모델


## Transformer 기반 이미지 모델
### Transformer 기반 이미지 모델
- 트랜스포머는 Text, 이미지, 음성 등 여러 모달을 다루는 모델의 성능을 극대화 시킴



## RAG

### RAG (Retrieval-Augmented Generation)
- LLM이 신뢰성 있는 최신 정보를 참고하여 답변하게 하는 기술
- 환각 현상을 줄일 수 있음

### LangChain 사용방법과 함께 RAG 구현 방법을 실습

## PEFT
### 효율적인 파인튜닝 (PEFT)
- 모든 파라미터를 변경하지 않고, 일부 파라미터만 변경하여 파인듀닝의 효율을 극대화 시킴
- LoRA를 중심적으로 다룸

---
사실상 여기가 끝
---

# 관통 PJT에 AI 적용하기
- AI를 사용하기 위한 구조
- FastAPI 소개
- FastAPI 테스트
- Post 요청 해보기
- 허깅페이스 Local 테스트
- Fast API로 챗봇 만들기

# AI를 사용하기 위한 구조

## 추론 서버를 포함한 아키텍처
### AI를 사용하기 위한 기본 구조
- 추론서버 : AI 추론용 서버를 두고 REST API로 추론
- 백엔드에서 추론 서버를 호출

# FastAPI 소개

## FastAPI를 ML에서 가장 많이 사용되는 Web Framework 입니다.
- REST API 서버 용도로 사용됩니다.
````
from fastapi import FastAPI

app = FastAPI()

@app.get("/hello/lunch")
def get_lunch():

  # 이곳에 코드를 넣습니다.

  return {"lunch": "Big-mac"}

# 서버 실행
# uvicorn main:app --reload
````

# FastAPI 테스트

## PyCharm 환경에서 Fast API를 테스트합니다.
- SW역량평가 환경이므로, 모든 PC에 설치되어 있습니다.

# Post 요청 해보기

## FastAPI에서 Post 요청
- Post 요청시 응답을 받아 출력하는 코드를 작성해 봅니다.
  - HTML 파일에서 접근을 위한 CORS 설정이 필요합니다.
  - 우측 Front는 ChatGPT로 생성합니다.

# 허깅페이스 Local 테스트

## 허깅페이스의 Transformer Library 사용
- Qwen3-0.6B 모델을 사용합니다.
- CPU 환경에서 추론이 잘 되는지 테스트합니다.
````
while True:
  prompt = input("입력 : ")
  result = incoke(prompt)
  print(f"AI : {result}\n")
````

# Fast API로 챗봇 만들기

## Fast API로 간단한 챗봇을 만들어봅니다.
- Front
  - ChatGPT로 html 파일을 생성합니다.
  - 전송 버튼을 누를때마다 POST 요청을 합니다.
- Back
  - Fast API로 Local LLM을 동작시킵니다.

# 끝으로
- 관통 PJT 활용 아이디어
- 앞으로의 학습 방향

# 관통 PJT 활용 아이디어 1

## Image to TEXT
- 손글씨, 인쇄된 종이 문서 등 사진으로 부터 Text를 추출해주는 Application 제작

# 관통 PJT 활용 아이디어 2

## Image to Text + Voice to Text
- 영화 이미지와 음성을 Text로 출력해주는 서비스

# 관통 PJT 활용 아이디어 3

## Image to Image
- 이미지와 Mask를 넣어, 다른 이미지로 변경해주는 서비스 제작
- InstantX/Qwen-Image-ControlNet-Inpainting

# 관통 PJT 활용 아이디어 4

## Text to Image
- 나만의 이미지를 생성해주는 Web Application
- https://huggingface.co/stabilityai/stable-diffusion-sl-refiner-1.0

# 관통 PJT 활용 아이디어 5

## Text to Voice
- 텍스트를 음성으로 출력해주는 TTS 서비스
- https://huggingface.co/hexgrad/Kokoro-82M

## 허깅페이스에서 다양한 아이디어를 얻자

## 허깅페이스 모델과 데이터셋
- 다양한 AI Application을 제작할 수 있습니다.

# 앞으로의 학습 방향

## 앞으로의 학습 방향은?

## 이제부터는 프로젝트로 기술을 통합하고, Hugging Face 중심으로 포트폴리오를 만들어야 합니다.

## 추천 학습 방향
- 모달별 모델 사용해서 데모 서비스 만들기
- 모달별 파인튜닝 노하우 습득하기
- 파인튜닝과 RAG 서비스 운영하기
- Local LLM으로 지속적인 학습하기
- 나만의 모델서버 / 추론서버 운영