# AI강의Ⅱ Wrap Up
## AI를 위한 Python과 Math
- AI를 위한 준비과정  
  1. 파이썬 기본 문법
  2. 클래스  
  3. Numpy  
  4. Pandas  
  5. Matplot  
  6. Seabon  
  7. 선형회귀  
  8. 로지스틱회귀  
  9. Gradient Decent  

### 파이썬 기본 문법
- 함수
  ````python
  # 함수 기본 문법
  def sum1(a, b):
      return a + b

  # 파라미터 타입까지 표기
  def sum2(a: int, b: int):
      return a + b

  # 리턴 타입까지 표현 -> 디폴트
  def sum3(a: int, b: int) -> int:
      return a + b

  c = sum1(10, 20)
  print(c)
  ````
  - 현업에서는 리턴 타입까지 표현하는 함수 표기 방법을 주로 사용함

- 클래스
  ```python
  # 클래스 생성
  class Person:
    def __init__(self):
      self.name = 'HI' #instance attribute
      self.age = '30' #instance attribute

    def sayHi(self):
      print('안녕')

    def sayBye(self):
      print(f'잘가 {self.name} 님')


  # 객체 생성
  bts = Person()
  bts.sayHi()
  bts.sayHi()
  bts.sayBye()
  ```

### Numpy & 선형대수학 기본 내용
- 벡터 표현
  ```python
  import numpy as np

  v = np.array([[1, 2, 3],
                [1, 1, 5]])
  ```
  - Numpy를 이용해 `[1, 1]`, `[2, 1]`, `[3, 5]`와 같은 벡터를 표현함
  - 벡터는 **방향**과 **크기**를 가지는 값으로, 고정된 위치가 아니라 움직임이나 힘의 방향을 나타냄
    - 예를 들어, `(1, 1)`이라는 좌표는 고정 좌표계에서는 한 점을 의미하지만,
    - 벡터에서는 `(0, 0)`을 기준으로 `(1, 1)` 방향으로 힘을 가하는 것
    - ex. 권투 선수가 `(0, 0)` 위치에서 `(1, 1)` 방향으로 펀치를 뻗는 것이라고 이해할 수 있음
  - 이렇게 어떤 방향으로 얼마만큼의 영향을 주는지를 표현하는 단위가 바로 **벡터**

- 변환행렬
  ```python
  #변환행렬 1 : x축 방향 2배 스케일링
  S = np.array([[2, 0],
                [0, 1]])

  #변환행렬 2 : 90도 반시계 방향 회전
  R = np.array([[0, -1],
              [1, 0]])

  #변환행렬 합성
  RS = R @ S

  #합성된 변환행렬로 한방에 변환
  v_scaled = RS @ v

  print(v_scaled)
  ```
  - 선형적으로 공간을 변형하는 개념
    - 벡터에 **스케일링(크기 조정)** 이나 **회전(방향 변경)** 같은 선형 변환을 적용함

  - `S`: x축 방향으로 2배 스케일링
    - ex. 쨉하는 동영상을 가로로 늘리는 것처럼 벡터의 x성분을 2배로 키움

  - `R`: 90도 반시계 방향 회전
    - ex. 쨉을 왼쪽으로 틀어주는 것과 같음 (방향만 바뀌고 크기는 그대로)
  
  - `RS`: 변환행렬 두 개를 한 번에 적용한 것
    - 먼저 스케일링, 그다음 회전 순서
  
  - 이러한 변환행렬은 MLP(다층 퍼셉트론)에서 `y = Wx + b` 연산을 할 때 자주 사용됨
    - ex. 입력 벡터 x에 가중치 행렬 W를 곱하는 것이 바로 선형변환의 예시

### Pandas
- 용도: 파이썬으로 엑셀을 하는 것과 같음
  - 데이터 확인
  - 불필요한 데이터 삭제
  - EDA와 전처리
    - AI에서 주로 사용
  - csv 파일 불러오기

- 데이터 확인
  ```python
  import seaborn as sns
  import pandas as pd

  # 아이리스(꽃) 데이터셋 읽어오기 (Seaborn 공식 자료)
  df = pd.read_csv("https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv")
  df.head(5) #상위 5줄만 가볍게 출력
  ```
  ![alt text](image.png)

- 불필요한 데이터 삭제 및 필터링
  ```python
  df_sub = df.drop(columns=['sepal_length', 'sepal_width']) # drop = 시리즈 제거
  df_filtered = df_sub[df_sub["petal_width"] >= 0.3] # 필터링
  df_sub.head(5)
  ```
  ![alt text](image-1.png)

### 시각화 (Matplot, Seaborn, Plotly)
- Matplot으로 시각화
  ```python
  import matplotlib.pyplot as plt
  import numpy as np

  # 데이터 준비
  x = np.linspace(0, 10, 15)   # 0 ~ 10 사이를 20등분
  y = np.sin(x)

  # 라인 그래프 생성
  plt.plot(x, y, label="sin(x)", color="blue")

  # 그래프 설정
  plt.title("Matplot Example")  # 제목
  plt.xlabel("x axis")          # x축 라벨
  plt.ylabel("y axis")          # y축 라벨
  print()
  ```
  ![alt text](image-2.png)

- Seaborn으로 시각화
  ```python
  import numpy as np
  import seaborn as sns

  # 데이터 준비
  x = np.linspace(0, 10, 20) # 0 ~ 10 까지 20 등분
  y = np.sin(x)

  # 라인 그래프 생성 후, 설정을 위한 객체 반환
  ax = sns.lineplot(x=x, y=y, label="sin(x)", color="blue")

  # 그래프 설정
  ax.set_title("Seaborn Example")
  ax.set_xlabel("x axis")
  ax.set_ylabel("y axis")
  print()
  ```
  ![alt text](image-3.png)
  - Matplot과 사용 방법 동일함

- Plotly으로 시각화
  ```python
  import numpy as np
  import plotly.graph_objects as go

  # 데이터 준비
  x = np.linspace(-5, 5, 100)
  y = np.linspace(-5, 5, 100)
  X, Y = np.meshgrid(x, y)
  Z = np.sin(X) * np.cos(Y)

  # 3D Surface 그래프 생성
  fig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y, colorscale='Viridis')])

  # 그래프 설정
  fig.update_layout(
      title='3D Surface: sin(x) * cos(y)',
      scene=dict(
          xaxis_title='X axis',
          yaxis_title='Y axis',
          zaxis_title='Z axis'
      ),
      template='plotly_white',
      width=500,   # 가로 크기
      height=500   # 세로 크기
  )

  fig.show()
  ```
  ![alt text](image-4.png)

- 시각화 도구를 3가지(Matplot, Seaborn, Plotly)나 사용하는 이유
  1. Matplot
      - 데이터를 차트로 표현하는 기본 Library
      - 가장 많이 사용(가장 기본적)
  2. Seabon
      - Matplot 기반으로 만들어짐
      - Pandas와 함께 사용하면 코드를 한줄로 작성해 그래프 그릴 수 있음
  3. Plotly
      - 3D 표현 가능
      - 마우스 반응형(인터랙티브 그래프)

※참고: 시대의 흐름과 시작점
- 유비쿼터스 시대 -> IoT 시대 -> 빅데이터 시대 -> Data Science 시대를 거쳐 'AI 시대'가 찾아옴
- Data Science의 시작은 3개 Library 활용으로 정립됨
  - `Numpy`, `Pandas`, `Matplot`
    - Numpy에서 제공되는 타입으로 소수점 연산을 빠르게 계산하고
    - 그 값을 Pandas로 모아 분석하고, 필터링하고, 전처리함
    - 최종적으로 Matplot을 이용해 시각화까지 진행
- 이렇게 3개 라이브러리를 다루는 과정이
차후 EDA 학습의 빌드업이 됨

### 선형회귀모델
- 추세선 방정식을 의미함
  ```python
  import seaborn as sns
  import matplotlib.pyplot as plt

  # 예시 데이터 : 팁스
  tips = sns.load_dataset("tips")

  # 모델 에시 (선형 회귀 모델)
  # reg plot은 추세선을 그려주는 기능을 가지고 있습니다.
  sns.regplot(x="total_bill", y="tip", data=tips, ci=None, line_kws={"color": "red"})
  print()
  ```
  ![alt text](image-5.png)
  - 추세선을 찾아내면 '미래 데이터'를 예측할 수 있음

※ 선형회귀모델의 '회귀'는 값을 예측하는 방법을 의미함("과거로 돌아가다"는 의미XX)

- 추세선 알아내는 원리
  1. 실제 값과 추세선의 오차를 구함
      - 예측값 $y=ax+b$ 와 실제 데이터의 차이를 계산
  
        ![alt text](image-6.png)

  2. 오차 제곱의 합(MSE)을 계산
      - $MSE = 1/n \sum(y_{예측} - y_{실제})^2$
      - 오차를 제곱해 합한 값이 작을수록 모델이 데이터를 잘 맞춘다는 의미
  
  3. a, b 값을 반복하며 최소 MSE를 찾음 (완전 탐색)
      ```python
      for a in range(1, 100):
          for b in range(1, 100):
              y = a*x + b
              # 실제 데이터와의 MSE 계산
              # MSE가 최소가 되는 a, b 선택
      plot(y = min_a*x + min_b)
      ```
      - 이 방법은 계산량이 많아 시간이 오래 걸림
  
  4. Gradient Descent로 최적화
      - 임의의 a, b에서 시작해 MSE가 줄어드는 방향으로 조금씩 이동
      - 경사가 있는 그래프를 따라 점차 내려가며 최소값(최적 a, b)을 찾음
      - 훨씬 빠르고 효율적
      - a 값에 대한 MSE 변화를 나타낸 예시 그림

        ![alt text](image-7.png)
  
  5. 이동하는 크기 = 학습률 (learning rate)
      - 경사를 따라 이동할 때, 한 번에 **얼마만큼 이동할지** 결정하는 값
      - 이 값을 너무 크게 잡으면, 최소 지점을 지나쳐서 양쪽으로 계속 튀는 현상 발생
      - 이 값을 너무 작게 잡으면, 너무 느리게 이동해서 학습 속도가 매우 느려짐
      - 학습률은 사람이 직접 정하는 값 -> **하이퍼파라미터** (모델이 스스로 찾지 못하고, 사람이 직접 설정해줘야 하는 값)

- $y=ax+b$에서 MSE가 가장 작아지는 a, b 지점 시각화
  - a(기울기), b(절편)를 기준으로 계산된 MSE 손실곡면 위에서 최소값을 가지는 지점을 시각화한 그래프
    
    ![alt text](image-9.png)
    - 빨간 점은 Gradient Descent가 이동한 경로를 의미
    - $z$축 = MSE

### 로지스틱회귀
- A에 속하는지, B에 속하는지 확률을 구하는 분류 모델
  - 주어지는 데이터
    - 2시간 공부 한 A 학생 : 불합격
    - 3시간 공부 한 B 한색 : 불합격
    - 4시간 공부 한 C 학생 : 합격
    - 3시간 공부 한 D 학생 : 불합격
    - 5시간 공부 한 E 학생 : 합격
    - 1시간 공부 한 F 학생 : 불합격
  
  ```python
  import numpy as np
  import matplotlib.pyplot as plt

  # 공부 시간 (X), 합격 여부 (y)
  X = np.array([1,2,3,4,5,6,2,3,4,5,6,7,1,2,3,4,5,6,7,8,2,3,4,5,6,7,8,9,10])
  y = np.array([0,0,0,1,1,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1])

  def sigmoid(z):
      return 1 / (1 + np.exp(-z))

  # 하이퍼 파라미터 초기화
  lr = 0.1  # 학습률
  epochs = 5000

  # a, b 초기값 세팅
  np.random.seed(42)  # 재현성 고정
  a = np.random.randn()  # 기울기
  b = np.random.randn()  # 절편

  # Gradient Descent
  for epoch in range(epochs):
      # 예측
      y_hat = sigmoid(a * X + b)

      # Loss (Cross Entropy)
      eps = 1e-10
      loss = -np.mean(y*np.log(y_hat+eps) + (1-y)*np.log(1-y_hat+eps))

      # 기울기 계산
      error = y_hat - y
      grad_a = np.mean(error * X)
      grad_b = np.mean(error)

      # 파라미터 업데이트 (점프!)
      a -= lr * grad_a
      b -= lr * grad_b

      # Loss값 출력
      if epoch % 500 == 0:
        print(f"Epoch {epoch}, Loss={loss:.4f}, a={a:.4f}, b={b:.4f}")

  # 학습 결과
  print()
  print("최적 기울기 a:", a)
  print("최적 절편 b:", b)
  print("최종 Loss:", loss)

  # 시각화
  X_test = np.linspace(0, 10, 200)
  y_prob = sigmoid(a * X_test + b)

  plt.plot(X_test, y_prob, color="red", linewidth=2)
  plt.xlabel("Study Time")
  plt.ylabel("Pass Probability")
  plt.grid(True)
  ```
  ![alt text](image-10.png)
  ![alt text](image-11.png)
  - 로지스틱 회귀도 MSE가 가장 낮은 a, b 찾는 것이 목적
  - 시그모이드 함수에 a, b를 넣으면 '확률'이 나옴
  - 결과
    - 4시간 공부하면 합격확률 0.85
    - 즉, 가성비있는 공부 시간은 4시간!
    - 6시간이상 공부하는 것은 의미XX
  
- 선형회귀 vs 로지스틱 회귀
  - 선형회귀
    - $y = ax + b$ 형태
  
  - 로지스틱 회귀
    - $y = sigmoid(ax + b)$ 함수 형태
      ```python
      def sigmoid(z):
          return 1 / (1 + np.exp(-z))
      ```

  - 비교
    ```python
    for epoch in range(epochs):
          # 예측
          y_hat = a * X + b           # 선형회귀 선 그려짐 (추세선)
          y_hat = sigmoid(a * X + b)  # 로지스틱회귀 선 그려짐 (확률선)

    . . .
    ```


## EDA
- EDA (Exploratory Data Analysis)
  - 데이터를 살펴보는 행동
    - 데이터 샘플을 확인  
    - 필드 확인  
    - 기본 통계량 확인  
    - 분포 확인  
    - 상관계수 확인  
    - 등등

- 데이터 샘플 확인
  ```python
  import seaborn as sns

  # seaborn 내장 데이터 셋 불러오기 (데이터 프레임)
  df = sns.load_dataset("mpg")
  df.head()
  ```
  ![alt text](image-12.png)

- 변수 간 관계 살펴보기 (feat.시각화)
  ```python
  import seaborn as sns

  df = sns.load_dataset("mpg")

  sns.regplot(data=df, x="weight", y="acceleration", scatter_kws={'alpha':0.6})
  print()
  ```
  ![alt text](image-13.png)

- 상관계수 (feat.히트맵)
  - 뭐랑 뭐랑 상관있습니까? 할 때의 그 상관임
  - 상관계수가 1에 가까우면 양의 상관관계, -1에 가까우면 음의 상관관계가 있는 것
    ```python
    import numpy as np
    import seaborn as sns

    df = sns.load_dataset("mpg")

    # 상관계수 행렬 계산
    corr = df.corr(numeric_only=True)

    # 삼각형 마스크 만들기 (상단 삼각형 가리기)
    mask = np.triu(np.ones_like(corr, dtype=bool))

    # 히트맵 출력
    sns.heatmap(corr, mask=mask, annot=True, cmap="coolwarm", linewidths=1)
    ```
    ![alt text](image-14.png)
    - 연비(mpg)와 가장 큰 상관관계를 갖는 데이터는 'weight'
      - 절댓값이 가장 크기 때문
      - 즉, 무게(weight)가 높아질수록 연비(mpg)가 낮아짐
  
- 추세선 및 분포 확인 (feat. pair plot)
  ```python
  import seaborn as sns
  import matplotlib.pyplot as plt

  df = sns.load_dataset("mpg")

  # pairplot 시각화
  sns.pairplot(df[["mpg", "weight", "acceleration"]], corner=True, kind="reg", plot_kws={'line_kws': {'color': 'red'}})
  print()
  ```
  ![alt text](image-15.png)
  - 데이터 간 추세선을 시각적으로 확인할 수 있음
  - 대각선에는 각 변수의 분포(histogram)가 표시됨
  - 하지만 분포만으로는 데이터 전체를 설명할 수 없음
    - ex. 똑같은 모양의 분포라도 평균, 분산 등이 다를 수 있음
  - 그래서 분포를 설명할 수 있는 수치적인 값들이 필요함
    - ex. 최솟값(min), 최댓값(max), 평균(mean), 분산(var), 표준편차(std) 등


## MLP
### MLP (Multi-Layer Perceptron)
- 하나 이상의 은닉층(hidden layer) 을 가진 신경망 구조
  - 딥러닝에서 가장 기본이 되는 모델 구조 중 하나

- 신경망 구조의 수식
  - 단순하게 선형회귀처럼 $y = Wx + b$ 형태로 표현할 수 있음
  - 즉, 단순한 추세선도 신경망으로 그릴 수 있음
    ```python
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import matplotlib.pyplot as plt
    import seaborn as sns

    # 데이터 준비 (Seaborn의 tips 데이터셋)
    tips = sns.load_dataset("tips")

    # 입력(X): total_bill, 출력(y): tip
    # 아래 dtype은 데이터 타입을 float 으로 저장하라는 뜻입니다
    X = torch.tensor(tips["total_bill"].values, dtype=torch.float32).reshape(-1, 1)
    y = torch.tensor(tips["tip"].values, dtype=torch.float32).reshape(-1, 1)

    # 모델 정의 - 선형함수 y = wx + b 하나 생성
    model = nn.Linear(1, 1)  # 입력 1개(total_bill), 출력 1개(tip)

    # 손실함수 & 옵티마이저
    # 옵티마이저 = weight, bias를 변경하는 알고리즘 종류, GD를 사용합니다.
    criterion = nn.MSELoss() # MSE 사용
    optimizer = optim.SGD(model.parameters(), lr=0.001) # 옵티마이저 = GD

    # 학습 루프
    for epoch in range(2000): # 점프 2000번~!
        y_hat = model(X) #모델 예측

        loss = criterion(y_hat, y) #로스 계산

        optimizer.zero_grad() #미분 계산 초기화 (미분 Ready)
        loss.backward() #미분 계산 수행

        optimizer.step() #GD 점프 1회 (w, b 값이 업데이트 됩니다.)

        if epoch % 400 == 0:
            print(f"Epoch {epoch:4d} | Loss: {loss.item():.4f}")

    # 학습 결과 출력
    print()
    print('[학습완료]')
    print(f'학습된 가중치(weight): {model.weight.item()}')
    print(f'학습된 편향(bias)) : {model.bias.item()}')
    print()

    # 예측 선 준비
    with torch.no_grad():
      x_line = torch.linspace(X.min(), X.max(), 100).reshape(-1, 1)
      y_line = model(x_line)

    # 시각화
    plt.scatter(X.numpy(), y.numpy(), color='skyblue', alpha=0.6)
    plt.plot(x_line.numpy(), y_line.numpy(), color='red', linewidth=2)
    plt.title("Linear Regression with PyTorch")
    plt.xlabel("Total Bill ($)")
    plt.ylabel("Tip ($)")
    plt.grid(True)
    ```
    ![alt text](image-16.png)

    ![alt text](image-17.png)
    - 선형 회귀식 $y=ax+b$ (혹은 $y=Wx+b$)는 직선만 표현 가능 -> 복잡한 데이터에는 한계가 있음
    - 그래서 여기에 활성화 함수(Activation Function) 를 넣음
    
- ReLU
  - 대표적인 활성화 함수로, 0 이하일 땐 0, 그 외(양수)엔 그대로 출력하는 함수임
  - 이걸 층(layer) 사이사이에 넣으면 모델이 비선형성을 학습하게 됨

    ![alt text](image-20.png)
  - 즉, 단순 선형 함수였던 $y=Wx+b$가
    - ReLU를 중간에 끼우고, 여러 번 쌓이면서
    - 곡선, 꺾인 선, 복잡한 형태 등 모두 학습할 수 있게 됨
      
      ![alt text](image-19.png)

- 이렇게 여러 층을 차례로 연결하는 방식을 AI에서는 Sequential Connection (순차 연결) 이라고 부름
  - 함수의 중첩과 ReLU 활성화 함수를 이용하면, 모든 선을 다 그릴 수 있음

### MLP 예시: MNIST 숫자 데이터셋 분류
- 모델 학습
  ```python
  import pandas as pd
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import DataLoader

  #===
  # 1. 데이터 준비 (csv -> data loader에 입력 해두기 까지)
  #===
  train_df = pd.read_csv("sample_data/mnist_train_small.csv")

  # 전처리를 안하니까 가끔씩 학습 실패해서... / 255로 스케일링이 필요했습니다. (0 ~ 1 값으로 정규화)
  X_train = torch.tensor(train_df.iloc[:, 1:].values, dtype=torch.float32) / 255.0
  y_train = torch.tensor(train_df.iloc[:, 0].values, dtype=torch.long)

  # Data Loader 세팅 완료
  train_dataset = list(zip(X_train, y_train))
  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

  #===
  # 2. 모델 준비
  #===
  # Layer의 개수, 중간 Layer의 입력과 출력의 개수는 모두 하이퍼파라미터 입니다.
  model = nn.Sequential(
      nn.Linear(784, 256), # 한 batch당 입력과 출력 개수를 적는 것을 잊지마세요.
      nn.ReLU(),
      nn.Linear(256, 128),
      nn.ReLU(),
      nn.Linear(128, 10) # 최종 출력값은 10개 입니다. '0' ~ '9' 의 점수 값이 나옵니다.
  )

  #===
  # 3. Loss & Optimizer (Adam)
  #===
  loss_fn = nn.CrossEntropyLoss()
  optimizer = optim.Adam(model.parameters(), lr=0.001)

  #===
  # 4. 학습
  #===
  epochs = 5
  for epoch in range(epochs):
      total_loss = 0
      for X_batch, y_batch in train_loader:

          outputs = model(X_batch) # 예측

          loss = loss_fn(outputs, y_batch) # loss 계산 (softmax도 함께 수행함)

          optimizer.zero_grad() # 미분 엔진 초기화
          loss.backward() # 미분 계산
          optimizer.step() # 파라미터 업데이트

          total_loss += loss.item() # Loss 누적

      print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}")

  #===
  # 5. 학습 완성된 파라미터들, 파일로 저장
  #===
  # 학습 결과로 찾아낸 파라미터 값들을 파일로 저장합니다.
  # .pth 확장자로 저장합니다. (pth = pytorch 약어)
  torch.save(model.state_dict(), "model.pth")
  ```
  ![alt text](image-21.png)
  1. CSV 데이터 불러오기
      - 손글씨 이미지 데이터를 pandas로 불러오고
      - torch.tensor로 바꿔서 0~1 사이로 정규화

  2. DataLoader에 넣기
      - 데이터를 (X, y) 형태로 묶어서 batch 단위로 불러올 수 있게 준비

  3. MLP 모델 구성
      - 입력 784 → 은닉층 256 → 은닉층 128 → 출력 10
      - 각 층 사이에 ReLU 활성화 함수 사용

  4. Loss 함수 & Optimizer 설정
      - CrossEntropyLoss 사용 (Softmax + Loss 포함)
      - 최적화는 Adam 사용, 학습률은 0.001 (하이퍼파라미터)

  5. 학습 루프
      - `for epoch in range(epochs)`로 반복하면서
      - 모델이 예측 → Loss 계산 → 역전파 → 파라미터 업데이트

  6. 학습 완료 후 저장
      - 학습된 모델의 파라미터들을 .pth 파일로 저장
  
  - 즉, 픽셀 하나하나를 입력해서, 0부터 9까지 숫자를 구분하는 방정식을 MLP가 직접 학습한 것

- 학습된 모델로 추론 (모델 불러와서 테스트 데이터에 적용)
  ```python
  import pandas as pd
  import torch
  import torch.nn as nn
  from torch.utils.data import DataLoader
  import matplotlib.pyplot as plt

  #===
  # 1. 모델 구조 정의 (학습할 때랑 똑같아야 함)
  #===
  model = nn.Sequential(
      nn.Linear(784, 256),
      nn.ReLU(),
      nn.Linear(256, 128),
      nn.ReLU(),
      nn.Linear(128, 10)
  )

  #===
  # 2. 모델 불러오기
  #===
  model.load_state_dict(torch.load("model.pth"))  # 저장했던 파일 로드

  #===
  # 3. 테스트 데이터 준비
  #===
  test_df = pd.read_csv("sample_data/mnist_test.csv")

  # 학습했던 것과 정규화 똑같이 해주기
  X_test = torch.tensor(test_df.iloc[:, 1:].values, dtype=torch.float32) / 255.0
  y_test = torch.tensor(test_df.iloc[:, 0].values, dtype=torch.long)

  # DataLoader 준비 완료
  test_dataset = list(zip(X_test, y_test))
  test_loader = DataLoader(test_dataset, batch_size=3, shuffle=True) # batch = 3 장씩

  #===
  # 4. 모델에 입력 데이터 이미지 3장 선택
  #===
  images = None
  for X_batch, y_batch in test_loader:
    images = X_batch
    break

  # 입력할 이미지 출력
  print('입력 이미지')
  for i in range(3):
      plt.subplot(1, 3, i+1)
      record = images[i].reshape(28, 28);
      plt.imshow(record, cmap="gray")
      plt.axis("off")
  plt.show()
  print()

  #===
  # 5. 추론하기!
  #===
  with torch.no_grad():
      outputs = model(images)
      preds = torch.argmax(outputs, dim=1) # 10개의 output 값중 가장 큰 값의 index 찾기
      print(f'모델 추론 결과 : {preds}')
  ```
  ![alt text](image-22.png)
  1. 모델 구조 다시 정의하기
      - 모델을 불러올 땐, 학습할 때와 같은 구조로 다시 만들어줘야 함

  2. 저장된 파라미터 불러오기
      - `torch.load("model.pth")`로 `.pth` 파일 로드
      - `model.load_state_dict()`로 모델에 파라미터 적용

  3. 테스트 데이터 불러오기
      - 학습 때와 마찬가지로 pandas로 CSV 파일 읽고
      - 0~1 사이로 정규화까지 동일하게 진행
      - DataLoader에 넣어서 batch 단위로 사용 (여기선 3장씩)

  4. 이미지 출력해보기
      - test 이미지 중 3장만 꺼내서 matplotlib으로 시각화
      - 실제로 우리가 분류할 이미지를 확인

  5. 추론(Inference)
      - `model(images)`로 예측 결과 계산
      - argmax를 통해 가장 높은 점수를 받은 숫자 선택
      - 최종적으로 모델이 인식한 숫자 출력

  - 즉, 학습한 MLP 모델에 테스트 이미지를 넣고, 모델이 어떤 숫자라고 판단했는지 추론한 결과를 확인함

- MLP 구조 정리 (Multi-Layer Perceptron)
  ```python
  model = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 10)
    )
  ```
  - 784 → 256 → 128 → 10
    - 입력층부터 출력층까지 순차적으로 연결된 신경망 구조
    - 각 숫자는 각 층의 출력 노드(=뉴런) 개수를 의미함
      | 층    | 노드 수    | 의미                            |
      | ---- | ------- | ----------------------------- |
      | 입력층  | **784** | 28×28 이미지 → 픽셀 하나당 하나의 입력값    |
      | 은닉층1 | **256** | 첫 번째 중간 계산 단계                 |
      | 은닉층2 | **128** | 두 번째 중간 계산 단계                 |
      | 출력층  | **10**  | 0~9 숫자 중 어떤 숫자인지 예측 (클래스 수만큼) |

  - 수식으로 표현하면?

    ![alt text](image-23.png)
    - $x$: 입력 벡터 (784차원, 이미지 1장)
    - $W_n, b_n$: 각 층의 가중치와 편향
    - $ReLU$: 비선형성 추가를 위한 활성화 함수
    - $y$: 최종 출력 (10차원 → 숫자 0~9 각각의 점수)

### MLP 결론
MLP는 선형 연산 $y=Wx+b$에 $ReLU$ 같은 활성화 함수와 함수의 중첩(층 쌓기)을 더함으로써, 비선형적인 함수도 학습할 수 있는 모델이다


## 토큰화 / 임베딩
문장은 그대로 모델에 바로 대입 불가능

-> 각 토큰 단위로 글자를 자르고, 숫자로 바꾼 후 각 수를 벡터공간의 벡터값으로 치환함

![alt text](image-24.png)

### 토큰화
- 문장을 토큰 단위로 나누는 과정

### 임베딩
- 각 토큰화된 단어들에게 의미를 부여하는 과정
- 의미공간의 특정 Vector값을 갖게됨
- 임베딩 모델
  - **같은 의미를 가진 단어는 유사한 벡터값을 가지도록 매핑시켜줌**
  - 임베딩 모델은 보통 트렌스포머 기반 모델로 만듦

    ![alt text](image-25.png)
    - 임베딩 모델을 거친 벡터값들은, 수학 연산으로 얼마나 유사한지 빠르게 계산 가능
  
- 예시
  - hugging face에서 `intfloat/e5-small-v2` 임베딩 모델을 불러와 사용
    ```python
    from transformers import AutoTokenizer, AutoModel
    import torch
    import numpy as np

    # 임베딩 테스트 할 단어들
    words = [
        'cat', 'dog', 'tiger', 'lion', 'wolf', 'fox', # 동물
        'apple', 'banana', 'grape', 'orange', 'peach', 'strawberry', # 과일
        'car', 'bus', 'train', 'bicycle', 'airplane', 'ship' # 탈것
    ]

    # 토크나이저 다운로드
    tokenizer = AutoTokenizer.from_pretrained("intfloat/e5-small-v2")

    # 모델 다운로드
    model = AutoModel.from_pretrained("intfloat/e5-small-v2") # 약 130MB

    # 토큰화
    #   - padding=True : 출력되는 토큰 개수를 같은 개수로 통일 (빈 공간은 0으로 채움)
    #                    모델에 입력될 때, 데이터별 입력 개수가 모두 동일해야하기 때문
    #   - return_tensors='pt' : PyTorch의 Tensor로 결과 리턴하시오.
    tokens = tokenizer(words, padding=True, return_tensors='pt')

    # 추론하기
    result = None
    with torch.no_grad():
        outputs = model(**tokens) # 추론
        embeddings = outputs.last_hidden_state.mean(dim=1) # [중요] mean pooling 수행
        result = embeddings.tolist() # 결과를 Python List 타입으로 저장

    # 결과 출력하기
    for i in range(len(words)):
        nums = result[i]
        print(f'{words[i]:10s} : 총 {len(nums)} 차원 값 --> ', end='')
        print(f'[{nums[0]:.2f}, {nums[1]:.2f}, ... , {nums[-2]:.2f}, {nums[-1]:.2f}]')
    ```
    ![alt text](image-26.png)
  
  - 유사한 단어가 유사한 벡터값을 가지는지 확인할 수 있음
    ```python
    # 위 코드를 실행 후에, 아래 코드를 실행해주세요.

    from transformers import AutoTokenizer, AutoModel
    import torch
    import numpy as np
    import plotly.graph_objects as go
    from sklearn.decomposition import PCA

    # ===
    # 1. 입력 데이터
    # ===
    words = [
        'cat', 'dog', 'tiger', 'lion', 'wolf', 'fox',                # Animal (0)
        'apple', 'banana', 'grape', 'orange', 'peach', 'strawberry', # Fruit (1)
        'car', 'bus', 'train', 'bicycle', 'airplane', 'ship'         # Vehicle (2)
    ]

    groups = [
        0, 0, 0, 0, 0, 0,   # Animal
        1, 1, 1, 1, 1, 1,   # Fruit
        2, 2, 2, 2, 2, 2    # Vehicle
    ]

    group_names = ["Animal","Fruit", "Vehicle"] # 그룹 이름

    colors = ["royalblue", "tomato", "seagreen"] # 그룹별 점 컬러


    # ===
    # 2. 임베딩 계산
    # ===
    tokens = tokenizer(words, padding=True, return_tensors='pt')

    result = None
    with torch.no_grad():
        outputs = model(**tokens)
        embeddings = outputs.last_hidden_state.mean(dim=1)
        result = embeddings.numpy()

    # ===
    # 3. PCA로 384 차원 ---> 3차원으로 축소 후 정규화
    # ===
    # PCA
    pca = PCA(n_components=3)
    reduced = pca.fit_transform(result)

    # L2 정규화 (잠시 후에 수업합니다.)
    unit_vectors = reduced / np.linalg.norm(reduced, axis=1, keepdims=True)

    # ===
    # 4. Plotly 차트 라이브러리로 시각화
    #    (이 코드는 이쁘게 만드려고 한거니, 이해하지 않아도 됩니다.)
    # ===

    # 원형 만들기
    theta = np.linspace(0, np.pi, 60)
    phi = np.linspace(0, 2 * np.pi, 120)
    theta, phi = np.meshgrid(theta, phi)
    x_sphere = np.sin(theta) * np.cos(phi)
    y_sphere = np.sin(theta) * np.sin(phi)
    z_sphere = np.cos(theta)
    sphere_points = go.Scatter3d(
        x=x_sphere.flatten(),
        y=y_sphere.flatten(),
        z=z_sphere.flatten(),
        mode='markers',
        marker=dict(size=2, color='lightblue', opacity=0.2),
        showlegend=False
    )

    # 점 찍기
    points = []
    added_groups = set()
    for i, word in enumerate(words):
        g = groups[i]
        vec = unit_vectors[i]
        is_show_legend = g not in added_groups
        added_groups.add(g)

        points.append(go.Scatter3d(
            x=[vec[0]], y=[vec[1]], z=[vec[2]],
            mode="markers+text",
            marker=dict(size=5, color=colors[g]),
            text=[word],
            name=group_names[g],
            showlegend=is_show_legend,
        ))

    # 추가 차트 세팅
    fig = go.Figure(data=[sphere_points] + points)
    fig.update_layout(
        title="3D Word Embeddings on Scatter Sphere",
        width=850, height=800,
        scene=dict(
            xaxis=dict(visible=False),
            yaxis=dict(visible=False),
            zaxis=dict(visible=False),
            camera=dict(eye=dict(x=0.9, y=0.9, z=0.9))
        ),
        showlegend=True,
        hovermode=False,
    )

    fig.show()
    ```
    ![alt text](image-27.png)
    - 384차원을 PCA를 통해 3차원으로 압축하여 그림으로 표현
    - 비슷한 의미의 단어는 비슷한 벡터값을 가지는 것을 시각적으로 확인
    - 벡터는 '방향'이 중요! 비슷한 '방향'의 벡터들이 비슷한 의미를 가짐


## 합성데이터와 데이터 증강
### 합성데이터 (Synthetic Data)
실제 세계에 없는 새로운 데이터를 생성하는 것

- 예시
  - GPT에게 “데이터를 만들어줘”라고 요청해 생성된 데이터
  - 내가 몇 개의 예시만 주고, 그걸 참고해 GPT가 새로운 문장을 만들어낸 것
  - 즉, 예시가 있어도 **"새로운 걸 만들어줘"**라는 요청이면 그건 합성 데이터
  - 프롬프트 예시
    ```
    """
    원본 데이터를 기반으로 새로운 데이터들을 만들어줘
    - 다양한 표현으로 바꾸어줘
    - 사람이 이해할 수 있는 문장이어야해
    - 원본 데이터 : "안녕, 난 철수야. KFC를 좋아하지"

    아래 예시를 보고, 같은 Format으로 만들어줘
    {
      "name": "철수",
      "age": 30,
      "height": 185,
      "messages": [
        {
          "직업": "아나운서",
          "인삿말": "안녕하십니까. 철수입니다. 저는 KFC를 참 좋아하는데요."
        },
        {
          "직업": "리포터",
          "인삿말": "안녕하세요 여러분. 철수예요. 저는 KFC를 사랑합니다. 여러분도 사랑해요."
        },
        {
          ...
        }
      ]
    }

    총 20개 문장이 되도록 만들어줘.
    최종 결과파일은 data.json 파일로 다운로드 받을 수 있게 해줘
    """
    ```
    - gpt에게 내가 예시를 주었지만, 이것은 증강이 아닌 합성 데이터임! '만들어달라'고 했기 때문!
    - 예시가 150개든, 1000개든 "참고해서 새로 만들어" 라고 하면 모두 **합성 데이터**임

### 데이터 증강 (Data Augmentation)
실제 데이터를 변형하여 데이터 셋을 확보하는 것

- 데이터의 양은 늘리되, 원본 의미는 유지함
- 주로 이미지 처리에서 많이 사용
  - 텍스트는 의미 훼손 가능성 높음 (ex. 단어 순서 바꾸면 의미가 달라질 수 있음)
  - 반면 이미지는 회전, 확대, 자르기 등의 변형에도 본질은 잘 유지됨

- 예시 코드
  - 원본 이미지
    ```python
    import requests
    from PIL import Image
    from io import BytesIO
    import matplotlib.pyplot as plt

    # 이미지 경로
    url = "https://cdn.pixabay.com/photo/2016/12/17/00/06/bowser-1912623_1280.jpg"

    # 다운로드 (request로 데이터 다운받고, 이미지 타입으로 변환)
    response = requests.get(url)
    img_origin = Image.open(BytesIO(response.content)).convert("RGB")
    print(f'타입 : type(img_origin)')

    # 출력하기
    plt.imshow(img_origin)
    plt.axis("off")
    plt.show()
    ```
    ![alt text](image-28.png)
  
  - 데이터 증강
    ```python
    from torchvision import transforms # 전처리 기능들이 모인 클래스
    from torchvision.utils import save_image

    # ===
    # 전처리 - 원본 이미지
    # ===
    base_transform = transforms.Compose([
        transforms.CenterCrop(640), # 640 x 640 이미지로 Crop (이미지 자르기)
        transforms.Resize((224, 224)), # 모델 입력 크기에 맞게 (224 x 224) 사이즈만 축소
        transforms.ToTensor()
    ])

    base_img = base_transform(img_origin) # 전처리된 이미지 저장

    # ===
    # 전처리 - 데이터 증강하기
    # ===
    aug_transform = transforms.Compose([
        transforms.CenterCrop(640),
        transforms.Resize((400, 400)),
        transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 좌우반전
        transforms.RandomRotation(degrees=10),   #  -10 ~ 10도 사이 랜덤 회전
        transforms.RandomApply([transforms.RandomGrayscale(p=1.0)], p=0.2), # 20% 확률로 흑백사진
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3), # 밝기, 대조, 노출값 랜덤하게 변경
        transforms.RandomCrop((224, 224)), # 랜덤한 위치로 Crop
        transforms.ToTensor()
    ])

    aug_imgs = []
    for i in range(5):
      aug_img = aug_transform(img_origin)
      aug_imgs.append(aug_img) # 전처리된 이미지 5개 생성

    # ===
    # 시각화 (아래 코드는 이쁘게 출력하기 위한 노력입니다.)
    # ===

    to_pil = transforms.ToPILImage()

    plt.figure()
    plt.title('Original')
    plt.imshow(to_pil(base_img))
    plt.axis("off")

    fig, axes = plt.subplots(1, 5, figsize=(7, 7))
    for i, aug in enumerate(aug_imgs):
        axes[i].imshow(to_pil(aug))
        axes[i].set_title(f'arg{i+1}')
        axes[i].axis("off")

    plt.show()

    # ===
    # 파일로 저장
    # ===
    save_image(base_img, "aug_grid.jpg")
    for i, img in enumerate(aug_imgs):
        save_image(img, f"aug_image_{i+1}.jpg")
    ```
    ![alt text](image-29.png)

※ 데이터가 부족할 때 합성 데이터 우선? 데이터 증강 우선?

| 상황               | 사용 방법                                                               |
| ---------------- | ------------------------------------------------------------------- |
| **토이 프로젝트, 연습용** | → 진짜든 가짜든 상관없으니 **합성 데이터**를 사용                                      |
| **실제 서비스, 제품**   | → 정확성이 중요하므로, **가짜 데이터 사용 불가**<br>→ 이 경우엔 **데이터 증강**으로 원본 기반 데이터 늘림 |


## CNN
### CNN (Convolutioonal Neural Network)
이미지나 영상처럼 **시각적인 데이터**의 특징을 자동으로 학습해서 **사물을 인식하거나 분류**할 수 있게 해주는 딥러닝 알고리즘

1. 컨볼루션 연산: 이미지 X 커널 = 특징 추출 
    - CNN은 이미지 전체를 한 번에 보지 않고, 작은 커널(필터)을 슬라이딩하면서 일부분씩 곱셈 연산을 함

      ![CNN](CNN.gif)
    - 이렇게 하면 테두리, 모서리, 방향 같은 특징이 강조됨
    - 이 연산 과정을 **Convolution(합성곱)** 이라고 부름
    - 즉, 이미지를 kernel에 컨볼루션 연산을 통해, 특징점을 잡는다

      ![alt text](image-30.png)
    - 커널도 학습 대상임
      - CNN은 커널함수의 값을 Weight로 취급하여 학습함

2. ReLU: 음수 제거 -> 비선형성 부여
    - Convolution 결과(특징이 뽑아진 값)에는 음수 값도 섞여 있음
    - ReLU를 통과시키면 음수는 0으로, 양수는 그대로 출력
    - 이렇게 하면 비선형성(Non-linearity) 이 생기고 모델이 더 복잡한 패턴을 학습할 수 있음
    
      ![alt text](image-31.png)

3. Max Pooling: 가장 강한 특징만 뽑아냄
    - 일정 구간(ex. 2×2)에서 가장 큰 값만 남김
    - 특징을 압축하면서도 정보는 유지
    - 연산량 줄고, 모델이 더 중요한 특징에 집중하게 됨
      
      ![maxpooling](max_pooling.gif)

### CNN 전체 구조 흐름
![alt text](image-32.png)

1. 입력(input)
    - ex. 흑백 이미지 28×28 (픽셀값 0~255)
    - CNN은 이 이미지를 숫자 행렬로 받아들임
      - 크기: `[1, 28, 28]` (1채널, 28행 28열)

2. 컨볼루션 (Convolution)
    - 작은 필터(커널)를 이미지 위에서 슬라이딩하며 곱셈 연산 수행
    - ex. 3×3 커널 하나가 이미지 위를 왼→오→아래로 움직이며 계산함
    - 이 커널이 이미지 안에서 테두리, 선, 무늬 등 특징을 감지함
    - 어떤 값으로 곱하냐?
      - 그 **곱할 값(커널 내부의 숫자)** 자체가 **학습 대상** !!
        ```
        예: 커널 (처음엔 랜덤)

        [ 0  1  0 ]
        [ 1 -4  1 ]   ← 이런 값들로 이미지 일부를 곱하고 합칩니다
        [ 0  1  0 ]
        ```
      - CNN은 처음에 이 커널 값들을 랜덤으로 시작하고
      - 학습을 거치면서 경사 하강법(Gradient Descent) 으로 이 커널 값들을 조금씩 바꾸며 더 좋은 특징을 뽑아내게 됨

3. 활성화 함수 (ReLU)
    - Convolution 결과엔 음수도 있음 ->  `ReLU(x) = max(0, x)`
      - 음수는 0으로 날리고, 양수만 남김
    - 덕분에 모델이 비선형성을 가지게 됨 (선만 학습하는 한계를 넘음)

4. 풀링 (Pooling)
    - 보통 Max Pooling (2×2) 사용
    - 일정 영역에서 가장 큰 값만 남김
    - 크기를 줄이고 (Downsampling), 가장 강한 특징만 보존

5. 다시 반복 (Convolution → ReLU → Pooling)
    - 위 과정을 여러 번 반복하면서 이미지에서 점점 더 추상적인 특징을 뽑아냄
    - 초반엔 선, 모서리 → 중반엔 윤곽 → 후반엔 숫자/사물의 모양

6. 전개 (Flatten)
    - 최종적으로 나온 특징 맵(feature map)을 일렬로 펴서 벡터로 변환
    - ex. `[4, 4, 128] → [2048]`
    - → 이제 MLP 구조에 넣을 수 있음

7. Fully Connected Layer (MLP 구조)
    - 여기서부턴 앞서 배운 MLP처럼 $y = ReLU(Wx + b)$ 구조로 예측을 수행함
    - 마지막에는 **클래스 수만큼 출력** (ex. MNIST는 10개 → 0~9)

8. Loss 계산 & 역전파
    - 예측값과 정답을 비교해서 Loss 계산
    - 역전파(backpropagation) 로 
      - MLP의 가중치뿐만 아니라 
      - 커널(필터) 안의 값들도 함께 학습됨

- 정리
  - Convolution → ReLU → Pooling → (반복) → Flatten → MLP(FC)

  - 앞부분에서 이미지의 특징을 뽑고,
  뒤에서는 MLP 구조로 분류까지 마무리함

  - 예를 들어 MNIST 숫자 이미지라면,
  → 마지막 FC 레이어에서 0~9 중 어떤 숫자인지 예측함

  - 이 구조 안에서 학습되는 파라미터는 2가지:
    - ① 커널(필터) 값들
    - ② FC 레이어의 Weight, Bias 값들 -> 이 모든 값을 **Gradient Descent** 로 학습함

※ MLP는 가중치(W)를 학습함

※ CNN은 **커널 안의 값들을 가중치로 사용하고, 그것들을 학습함**

-> 즉, CNN은 **“어떤 모양의 필터로 이미지를 살펴보는 게 좋은가?”** 를
스스로 학습해서 찾아내는 모델

### CNN 확장 (AlexNet & ResNet)
- AlexNet
  - CNN 구조를 기반으로 커널 사이즈, 필터 수, 레이어 순서 등 하이퍼파라미터를 잘 튜닝한 대표 모델
  - 이미지 분류 성능이 기존을 압도하면서 딥러닝의 붐을 이끈 모델
  - 즉, CNN을 "잘 설계한 버전"

- ResNet (Residual Network)
  - CNN 구조를 더 깊게 쌓으려 했던 시도에서 등장
  - 그러나 문제 발생
    - 왜?
    - 연산이 계속 곱해지며, 앞단에서 계산한 값의 영향력이 점점 사라지는 현상 발생
    - 이를 **기울기 소실(Vanishing Gradient)** 이라고 함
  
  - 해결 방법: **Skip Connection**
    - 앞 레이어의 출력을 그대로 뒷단으로 전달 (skip 연결)
      
      ![alt text](image-33.png)
    - 수식 표현: $y = ReLU(F(x) + x)$
    - 이렇게 하면 학습 시 정보가 사라지지 않고 더 깊은 네트워크도 안정적으로 학습 가능

### 사전학습된 CNN 모델 활용: Linear Probing
- 사전 학습 모델
  - ResNet18 같은 모델은 ImageNet이라는 대규모 데이터셋으로 이미 학습되어 있음
  - 즉, **일반적인 이미지 특징을 잘 뽑아내는 능력(백본)**이 탑재된 상태

- 파인튜닝(Fine-Tuning)
  - 기존에 사전학습된 모델을 완전히 새로 학습시키는 게 아니라, 
  - 필요한 일부 Layer만 새로 학습하거나, 전체를 조금만 조정하는 기법

  - 파인튜닝 3가지 방식
    1. Full Fine-Tuning (풀 파인튜닝)
        - 모든 파라미터(Weight)를 업데이트하는 방식
        - 기존 모델의 구조는 유지하지만, 모든 층의 가중치를 다시 조정함
        - 표현력이 매우 높아, 성능 향상이 클 수 있음
        - 그러나, 데이터가 많아야 과적합(overfitting)을 막을 수 있기 때문에 **대규모 데이터셋**이 있을 때 사용
    
    2. Partial Fine-Tuning (파셜 파인튜닝)
        - 일부 레이어만 학습시키고, 나머지는 고정 (freeze)
        - ex. `Linear Probing` → 마지막 FC Layer만 교체해서 학습
        - 실무에서 소량의 커스텀 데이터를 쓸 때 적합

    3. Efficient Fine-Tuning (효율적인 파인튜닝)
        - 기존 모델의 파라미터는 전부 고정하고, "어댑터"나 "로라(LoRA)" 등의 추가 모듈만 학습
        - 모델 내부에 작고 얇은 경로를 삽입하여 추가 학습 가능하게 함
        - 소량의 데이터 & 적은 리소스로도 성능 향상 가능
        - 최근 LLM 파인튜닝 등에서 매우 널리 사용됨

- Linear Probing
  - **전체 네트워크 중 마지막 Layer만 새로 교체**해서 학습하는 방법
  - 앞쪽 CNN Layer들은 학습된 그대로 사용
    - **가중치(Weight)를 고정**시킴
    - 백본의 특징 추출 능력은 그대로 쓰기 위함
  - 새로 만든 마지막 FC Layer만 학습하여 **원하는 분류 작업** 수행

    ![alt text](image-34.png)

- 예시: 사전학습된 ResNet으로 이미지 분류하기
  - 사전학습된 ResNet-18 로드하고, 이미지 1장을 예측
  - 사전학습된 모델을 **그대로 활용**해서 일반 이미지 분류를 하는 예시 (1000개 클래스 분류)
    ```python
    import torch
    from torchvision import models, transforms
    from PIL import Image
    from io import BytesIO
    from torchvision.models import ResNet18_Weights
    import requests
    import matplotlib.pyplot as plt

    # 모델 불러오기
    # 사전학습된 ResNet-18 모델을 불러옵니다.
    model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)
    model.eval() # 평가모드 (추론용)

    # 이미지 URL
    url = "https://cdn.pixabay.com/photo/2018/07/07/21/29/cat-3522912_1280.jpg" # 고양이
    #url = "https://cdn.pixabay.com/photo/2012/02/27/17/00/ape-17474_1280.jpg" #침팬지
    #url = "https://cdn.pixabay.com/photo/2020/02/03/07/40/cross-speed-4814978_1280.jpg" #스포츠카

    # 이미지 다운로드
    img = Image.open(BytesIO(requests.get(url).content)).convert("RGB")

    # 이미지 출력
    plt.figure(figsize=(5, 5))
    plt.imshow(img)
    plt.axis('off')
    plt.show()

    # 이미지 전처리
    preprocess = transforms.Compose([
        transforms.Resize((256, 256)), # 크기 변경 (256 x 256)
        transforms.ToTensor(), # PyTorch Tensor 타입으로 변경 + 0 ~ 1로 정규화
        transforms.Normalize( # 정규화
            mean=[0.485, 0.456, 0.406], # ImageNet 학습때 쓴 평균 값
            std=[0.229, 0.224, 0.225] # ImageNet 학습때 쓴 표준편차 값
        ),
    ])

    # 이미지 전처리 후 차원 변경
    # (3, 256, 256) -> (1, 3, 256, 256) : 배치차원 추가
    input_tensor = preprocess(img).unsqueeze(0)

    # 모델 추론
    with torch.no_grad():
        outputs = model(input_tensor)
        preds = torch.argmax(outputs, dim=1).item()

    # 예측 결과 출력
    labels = ResNet18_Weights.IMAGENET1K_V1.meta["categories"]
    print(f'예측 클래스: {labels[preds]}')
    ```
    ![alt text](image-35.png)

- 예시: Ronald McDonald 분류기 만들기 (Fine-tuning)
  - 맥도날드 이미지 다운로드
    ```python
    !git clone https://github.com/mincoding1/ronald_image.git
    ```
  
  - Linear Probing을 통한 모델 학습
    ```python
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torchvision import models, datasets, transforms
    from torch.utils.data import DataLoader
    import os

    # ===
    # 1. 기본 설정
    # ===
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("사용 장치 : ", device)

    SEED = 0
    torch.manual_seed(SEED)

    # ===
    # 2. 이미지 전처리
    # ===
    print('이미지 준비...', end='')
    transform_train = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
    ])

    # train 폴더에 이미지들 정보를 읽음 (폴더명을 클래스명으로 인식!)
    train_dataset = datasets.ImageFolder("ronald_image/train", transform=transform_train)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    print('완료')

    # ===
    # 3. 사전학습된 ResNet18 모델 준비 + 파인튜닝 준비
    # ===
    print('모델 준비...', end='')
    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)

    # [핵심코드 1] 모든 파라미터 autograd off
    for param in model.parameters():
        param.requires_grad = False

    # [핵심코드 2] 마지막 FC 변경 (새로 생성)
    model.fc = nn.Linear(512, 2) # 입력 512개, 출력 2개, 이 부분만 autograd on
    model = model.to(device)
    print('완료')

    # ===
    # 4. 학습
    # ===
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

    print('학습 시작')
    for epoch in range(5):  # Epoch 수는 자유롭게 조절 가능
        print(f' - [Epoch {epoch+1}/3] ... ',end='')
        model.train()
        total_loss = 0.0

        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f'Loss: {avg_loss:.4f}')
    print('학습 완료')
    ```
    - 사전학습된 ResNet18 불러와서, 마지막 Layer만 새로 학습
    - 나머지 파라미터는 고정 (Freeze)
    - Ronald vs Not-Ronald 분류기 학습
    - 수정한 부분
      ```python
      # 기존에 있던 마지막 분류층 (1000개의 출력) 제거
      # 대신 우리가 원하는 2개의 클래스만 분류하도록 새로 정의
      model.fc = nn.Linear(512, 2)

      # 기존의 model.fc는 512 → 1000 연결되어 있었는데,
      # 512 → 2로 바꿔서 2-class classification으로 수정함!!
      ```
  
  - 학습된 모델로 예측 및 시각화
    ```python
    import torch
    from torchvision import transforms
    from PIL import Image
    from io import BytesIO
    import requests
    import matplotlib.pyplot as plt

    # 테스트 해볼 이미지 URL들
    urls = [
        "https://cdn.pixabay.com/photo/2012/11/28/10/28/cowboy-67630_1280.jpg",  # 사람
        "https://cdn.pixabay.com/photo/2014/06/17/14/16/mcdonald-370465_1280.jpg",  # 로널드
        "https://cdn.pixabay.com/photo/2018/07/07/21/29/cat-3522912_1280.jpg",  # 고양이
        "https://cdn.pixabay.com/photo/2012/02/27/17/00/ape-17474_1280.jpg",  # 침팬지
        "https://static.wikia.nocookie.net/ronaldmcdonald/images/9/93/Ronald_Actors.jpg",  # 로널드
        "https://cdn.pixabay.com/photo/2020/02/03/07/40/cross-speed-4814978_1280.jpg",  # 스포츠카
        "https://static.wikia.nocookie.net/agk/images/d/db/Screen-Shot-2017-05-01-at-11.17.09-AM.jpg",
        "https://cdn.pixabay.com/photo/2013/06/20/04/41/fool-140229_1280.jpg",
        "https://cdn.pixabay.com/photo/2021/10/10/21/52/makeup-6698881_1280.jpg",
        "https://cdn.pixabay.com/photo/2019/11/04/01/11/cellular-4599956_1280.jpg",
    ]

    results = []
    model.eval()

    with torch.no_grad():
        for url in urls: # 이미지 하나씩 추론해봄

            # 다운로드 1 장
            img = Image.open(BytesIO(requests.get(url).content)).convert("RGB")

            # 이미지 전처리
            img_tensor = transform_train(img).unsqueeze(0).to(device)

            # 추론
            outputs = model(img_tensor)

            # 확률계산
            probs = torch.softmax(outputs, dim=1)[0].cpu().numpy()

            # 결과를 result에 저장
            pred_idx = probs.argmax()
            pred_class = train_dataset.classes[pred_idx]
            pred_prob = probs[pred_idx]

            results.append({
                "img": img,
                "pred_prob": pred_prob,
                "probs": probs
            })

    # 확률 높은 순서대로 정렬
    target_class_idx = train_dataset.classes.index('ronald')
    results_sorted = sorted(results, key=lambda x: x["probs"][target_class_idx], reverse=True)

    # 시각화
    plt.figure(figsize=(15, 4))
    for i, r in enumerate(results_sorted):
        plt.subplot(1, len(results_sorted), i + 1)
        plt.imshow(r["img"].resize((128, 128)))
        plt.axis("off")
        plt.title(f"Top-{i+1}\n{r['pred_prob']:.2f}")

    print()
    ```
    ![alt text](image-36.png)
    - 학습된 모델을 사용해 다양한 이미지를 분류
    - 로널드일 확률이 높은 순서대로 시각화


## RNN / LSTM
### RNN (Recurrent Beural Network)
연속된 데이터(시퀀스)를 처리하기 위해 만들어진 딥러닝 구조

- 입력 데이터 간의 시간적 순서나 의존성을 고려함
  - 과거 정보를 기억하며 다음 값을 예측하는 데 유리

- RNN 구조
  - 매 시점마다 입력을 순차적으로 받고
  - 이전 상태(hidden state)를 다음 입력에 반영함
  - 이전 정보를 기억하면서 다음 토큰 예측하는 구조

    ![alt text](image-37.png)

    ![alt text](image-38.png)

- RNN 문제점
  - 기억 소실 문제 (Vanishing Memory)
    - 시점이 길어질수록 과거 정보 잊어버림
    - 즉, 과거 Hidden Layer의 출력값 정보가 점점 소실됨
    - ex. 문장 초반 단어 정보가 끝에 가면 영향력 거의 없음

      ![alt text](image-39.png)

  - 기울기 소실 문제 (Vanishing Gradient)
    - 역전파 시, 반복적인 chain rule로 미분값이 점점 작아짐
    - 결국 앞쪽 Layer는 거의 학습되지 않음
    - 긴 문장 or 긴 시퀀스 학습 불가능에 가까움

      ![alt text](image-40.png)
  
  - 이 문제를 해결하려고 나온 것이 LSTM -> Transformer 구조

### LSTM(Long Short-Term Memory)
RNN 단점을 보완한 구조로, 핵심은 **게이트 구조**

-> 중요한 정보는 기억, 쓸모없는 건 지움으로써 **긴 문맥도 기억 가능**하게 됨

![alt text](image-41.png)

- 내부 구조
  - Forget Gate
    - 이전 정보 중 어떤 걸 버릴지 결정함
    - 시그모이드 -> 0에 가까우면 버림, 1에 가까우면 유지
  - Input Gate
    - 현재 입력에서 어떤 정보를 추가할지 결정
    - 시그모이드 + tanh 조합으로 새로운 기억 후보 생성
  - Cell State (셀 상태)
    - 정보가 흘러가는 고속도로 같은 역할
    - 필요 없는 건 forget gate로 지우고, 중요한 건 input gate로 추가함
  - Output Gate
    - 현재 시점에서 어떤 정보를 출력할지 결정
    - 최종 hidden state로 내보냄 -> 다음 시점의 입력으로도 사용됨

- LSTM 학습 대상
  - 각 게이트마다 고유한 Weight 있음
    - Forget Gate: `W_f`
    - Input Gate: `W_i`
    - Output Gate: `W_o`
    - Cell State 업데이트용: `W_c`
  - 이 weight 값들이 모두 학습 대상임
  - 학습하면서 "어떤 정보를 버릴지, 저장할지, 출력할지"를 결정하는 능력을 키움

- LSTM 한계점
  - RNN에서 발생하던 기억 소실 문제를 어느 정도 해결함
  - 하지만 완전히 해결한 건 아님
  - 문장이 너무 길어지면, 여전히 **과거 정보가 희미**해짐
  - 특히 문맥 안의 **모든 단어 간의 관계를 동시에 파악하기 어려움**
    - ex. 앞에서 말한 주어가 뒤에서 동사와 연결될 때, 멀리 떨어져 있으면 연결 잘 안 됨
  - 결국, 순차적으로만 정보를 처리한다는 구조적 한계 존재
  - 그래서 등장한 것이 Attention 메커니즘!


## Attention과 Transformer 모델
### Attention
- 입력데이터에 중요한 부분에 집중할 수 있게 돕는 기술
- 문맥을 이해하는 벡터를 만들어는 역할을 함







### Transformer
- 어텐션을 인코더, 디코더 형태로 활용하여 Foundation Medel 시대를 열게된 딥러닝 모델


## Transformer 기반 이미지 모델
### Transformer 기반 이미지 모델
- 트랜스포머는 Text, 이미지, 음성 등 여러 모달을 다루는 모델의 성능을 극대화 시킴



## RAG

### RAG (Retrieval-Augmented Generation)
- LLM이 신뢰성 있는 최신 정보를 참고하여 답변하게 하는 기술
- 환각 현상을 줄일 수 있음

### LangChain 사용방법과 함께 RAG 구현 방법을 실습

## PEFT
### 효율적인 파인튜닝 (PEFT)
- 모든 파라미터를 변경하지 않고, 일부 파라미터만 변경하여 파인듀닝의 효율을 극대화 시킴
- LoRA를 중심적으로 다룸

---
사실상 여기가 끝
---

# 관통 PJT에 AI 적용하기
- AI를 사용하기 위한 구조
- FastAPI 소개
- FastAPI 테스트
- Post 요청 해보기
- 허깅페이스 Local 테스트
- Fast API로 챗봇 만들기

# AI를 사용하기 위한 구조

## 추론 서버를 포함한 아키텍처
### AI를 사용하기 위한 기본 구조
- 추론서버 : AI 추론용 서버를 두고 REST API로 추론
- 백엔드에서 추론 서버를 호출

# FastAPI 소개

## FastAPI를 ML에서 가장 많이 사용되는 Web Framework 입니다.
- REST API 서버 용도로 사용됩니다.
````
from fastapi import FastAPI

app = FastAPI()

@app.get("/hello/lunch")
def get_lunch():

  # 이곳에 코드를 넣습니다.

  return {"lunch": "Big-mac"}

# 서버 실행
# uvicorn main:app --reload
````

# FastAPI 테스트

## PyCharm 환경에서 Fast API를 테스트합니다.
- SW역량평가 환경이므로, 모든 PC에 설치되어 있습니다.

# Post 요청 해보기

## FastAPI에서 Post 요청
- Post 요청시 응답을 받아 출력하는 코드를 작성해 봅니다.
  - HTML 파일에서 접근을 위한 CORS 설정이 필요합니다.
  - 우측 Front는 ChatGPT로 생성합니다.

# 허깅페이스 Local 테스트

## 허깅페이스의 Transformer Library 사용
- Qwen3-0.6B 모델을 사용합니다.
- CPU 환경에서 추론이 잘 되는지 테스트합니다.
````
while True:
  prompt = input("입력 : ")
  result = incoke(prompt)
  print(f"AI : {result}\n")
````

# Fast API로 챗봇 만들기

## Fast API로 간단한 챗봇을 만들어봅니다.
- Front
  - ChatGPT로 html 파일을 생성합니다.
  - 전송 버튼을 누를때마다 POST 요청을 합니다.
- Back
  - Fast API로 Local LLM을 동작시킵니다.

# 끝으로
- 관통 PJT 활용 아이디어
- 앞으로의 학습 방향

# 관통 PJT 활용 아이디어 1

## Image to TEXT
- 손글씨, 인쇄된 종이 문서 등 사진으로 부터 Text를 추출해주는 Application 제작

# 관통 PJT 활용 아이디어 2

## Image to Text + Voice to Text
- 영화 이미지와 음성을 Text로 출력해주는 서비스

# 관통 PJT 활용 아이디어 3

## Image to Image
- 이미지와 Mask를 넣어, 다른 이미지로 변경해주는 서비스 제작
- InstantX/Qwen-Image-ControlNet-Inpainting

# 관통 PJT 활용 아이디어 4

## Text to Image
- 나만의 이미지를 생성해주는 Web Application
- https://huggingface.co/stabilityai/stable-diffusion-sl-refiner-1.0

# 관통 PJT 활용 아이디어 5

## Text to Voice
- 텍스트를 음성으로 출력해주는 TTS 서비스
- https://huggingface.co/hexgrad/Kokoro-82M

## 허깅페이스에서 다양한 아이디어를 얻자

## 허깅페이스 모델과 데이터셋
- 다양한 AI Application을 제작할 수 있습니다.

# 앞으로의 학습 방향

## 앞으로의 학습 방향은?

## 이제부터는 프로젝트로 기술을 통합하고, Hugging Face 중심으로 포트폴리오를 만들어야 합니다.

## 추천 학습 방향
- 모달별 모델 사용해서 데모 서비스 만들기
- 모달별 파인튜닝 노하우 습득하기
- 파인튜닝과 RAG 서비스 운영하기
- Local LLM으로 지속적인 학습하기
- 나만의 모델서버 / 추론서버 운영