# 딥러닝 및 이미지 모델 — 1차시: 이미지 딥러닝 모델

## 📑 CONTENTS
1. CNN 살펴보기
   1) CNN vs. FCN
   2) 모델 구조

2. CNN 기반 모델 변천사
   1) AlexNet
   2) VGGNet
   3) ResNet
   4) MobileNet


## 🎯 학습 목표
- 이미지 기반 학습 모델의 구조와 작동 방식을 이해합니다.
- 대표적 CNN 기반 이미지 모델의 변천사를 배웁니다.
- 이미지 분류 문제에서 실습을 통해 주요 이미지 모델을 적용해봅니다.


# 0. 학습 시작(오버뷰)

## 0. 학습 시작
- CNN 모델의 기본 구조는 무엇이며, 왜 이미지 과업에서 중요한가?
  - CNN 모델의 기원 및 모델 주요 모듈 소개

- CNN의 단위 연산과 디자인 철학은 어떻게 발전했을까?
  - 계층 깊이, 스케일, 학습 방법, 경량화

- CNN 기반 주요 모델은 어떻게 변화해왔고, 왜 중요한가?
  - 깊이와 효율성을 동시에 높이는 방향으로 개선


## 0. 학습 시작 — 산업 현장 사례
- 금형 표면 스크래치 검출
- 식품/의약품의 이물질 검출
- PCB 불량 자동 판정  
  부품과 기판 사이의 접합이 불완전한 냉땜, 표면에 남은 플럭스 잔여물, 납땜부 미세 크랙 등 불량 검출

> 출처: SAIGE 제조 인사이트


## 0. 학습 시작 — 내 일상 속에서 찾아볼 수 있는 사례
- 성동구, 픽토그램 활용 ‘안심우회전시설’ 첫 설치
- 인공지능(AI) 카메라를 이용하여 차량, 보행자 등을 실시간으로 감지
- 상황에 맞게 움직이는 픽토그램을 모니터로 표출하는 방식으로 교통사고를 효율적으로 예방

> 출처: 연합뉴스


# 1. CNN 살펴보기

## 1-1. CNN vs. FCN
(섹션 제목 슬라이드)


## 1-1. FCN 모델
### 완전 연결층(Fully-Connected Layer)
- 입력을 받아서 출력으로 변환하는 신경망의 기본 모듈

### FCN 변환 예시
- 입력: 이미지가 32×32×3인 고차행렬이라고 가정  
  (이미지의 높이, 너비가 각각 32픽셀, RGB값 3개를 의미하며 RGB 값은 0~255 사이)  
  이미지의 모든 화소를 나열하여 1차원 벡터로 만들어 입력 데이터로 받는다.
- 출력: 10×1의 1차원 벡터
- 모델 상수: 모델이 학습해야 하는 파라미터(parameter)  
  상기 입력력 시, 모델상수 W는 10×3072로 입력의 모든 값이 출력에 미치는 영향을 나타냄.

- (개념 요약)  
  32×32×3 3차원 변환(Flatten) → 1×3072 1차원 → 변환된 데이터를 10개 중 1개로 분류

# 1-1. CNN 모델

## 합성곱 레이어 (Convolution Layer)

- 입력 이미지를 필터와 연산하여 특징 맵(feature map)을 뽑아내는 모듈  
- 1차원 구조로 변환하는 FCN과 달리 3차원 구조를 그대로 보존하면서 연산  
- Convolution(컨볼루션): 필터를 이미지 상에서 이동시키면서 내적을 반복 수행,  
  내적으로 구한 모든 값을 출력으로 제공  

### 예시 구조
- 입력 이미지: 3 × 32 × 32  
  - 깊이(채널): 3  
  - 높이: 32  
  - 너비: 32  

- 필터: 3 × 5 × 5  
  - 깊이(채널): 3  

> 차원을 반드시 주의!  
> Filter는 항상 입력의 깊이/채널 축과 동일한 차원을 가짐.

---

## 필터 연산 방식

- 특정 위치에서 필터와 동일한 사이즈의 이미지 영역 간 내적한 결과값을 계산  
- 필요 연산은 (3 × 5 × 5 = 75)차원 내적 곱 + bias 벡터

**수식:**  
wᵀx + b  
- x: 필터가 덮는 이미지 영역  
- w: 필터의 가중치  
- b: 편향(보정값)

---

## 활성화 맵 (Activation Map)

- 필터의 이동 경로를 따라 모든 연산 값을 모아 활성화 맵을 출력  
- 예시: 3 × 32 × 32 이미지 → 3 × 5 × 5 필터 → 출력: 28 × 28 × 1 Activation Map  

---

## 합성곱 연산 예시 (박스 필터)

- 예: 박스필터(Box Filter)  

```text
1/9 *
[[1 1 1]
 [1 1 1]
 [1 1 1]]
```

- 각 필터값과 대응되는 입력값의 곱을 모두 합산  
- 입력 가운데 위치의 출력값 계산  
  → 예: 입력 행렬 내 해당 영역의 평균값

---

### 필터 이동 과정 (시각적 개념)

| 입력 위치 | 출력 결과 |
|------------|------------|
| 필터가 처음 위치할 때 | 출력값 0 |
| 오른쪽으로 한 칸 이동 | 출력값 10 |
| 다시 한 칸 이동 | 출력값 20 |
| 다음 칸 이동 | 출력값 30 |
| 필터 이동이 끝날 때 | 출력값 30 (유지) |

즉, 필터가 입력 상을 이동하면서 각 위치의 국소 영역(local region)에 대해 합성곱을 수행하고,  
그 결과를 모아 출력 맵을 생성함.

# 1-1. CNN 모델

## 합성곱 레이어 (Convolution Layer)

- 입력 이미지를 필터와 연산하여 특징 맵(feature map)을 뽑아내는 모듈

---

## 박스 필터(Box Filter) 예시

- 입력과 필터의 곱을 이동시키며 계산  
- 필터가 이동할 때마다 입력의 국소 영역(local region)과 대응되는 값들의 곱을 모두 더함  
- 결과는 입력 중앙에 해당하는 위치에 출력됨

```text
필터: 1/9 *
[[1 1 1]
 [1 1 1]
 [1 1 1]]
```

- 필터가 입력 위를 이동하며 합성곱 수행  
- 각 위치별 출력값 예시:  
  0 → 10 → 20 → 30 → 30 → 20 → 10 → 0

---

## 출력 해상도 변화

- 입력 대비 출력의 공간 해상도가 줄어듦 (예: 32 → 28)
- 출력 해상도 계산식:

```text
출력 해상도 = 입력 해상도 - 필터 해상도 + 1
예) 32 - 5 + 1 = 28
```

- 예시: 입력 10, 필터 3 → 출력 8  
  → `8 = 10 - 3 + 1`

---

## 패딩(Padding)

- 입력과 출력의 해상도를 동일하게 유지하고 싶을 경우,
  출력값 중 정의되지 않은 영역을 0 혹은 가장 가까운 출력값으로 대체

> 예: “출력 해상도를 유지하고 싶다면, 경계에 0을 채워 넣는다.”

---

## 여러 필터 사용 시

- CNN에서는 다수의 필터를 동시에 사용하여 서로 다른 특징 맵을 추출함  
- 예: 6개의 필터를 사용할 경우  
  - 입력: 3 × 32 × 32  
  - 필터: 6 × 3 × 5 × 5  
  - 출력: 6 × 28 × 28  
  - 추가로 각 필터마다 1개의 bias(편향) 포함  
  - 전체 bias는 6×1 형태의 벡터로 구성됨

```text
입력 3x32x32 → Convolution Layer(6x3x5x5, bias 6x1) → 출력 6x28x28
```

---

## 배치 단위 처리

- CNN에서는 여러 이미지를 동시에 입력(batch)으로 처리 가능  
- 예시:
  - 입력 배치: 2 × 3 × 32 × 32  
  - 필터: 6 × 3 × 5 × 5  
  - 출력: 2 × 6 × 28 × 28  
  - bias: 6 × 1

> 즉, 배치마다 동일한 필터 세트를 공유하여 병렬 연산을 수행함.

---

## 요약

| 항목 | 설명 |
|------|------|
| 입력 구조 | (Batch, Channel, Height, Width) |
| 필터 구조 | (Num_Filter, Channel, Kernel_H, Kernel_W) |
| 출력 구조 | (Batch, Num_Filter, Output_H, Output_W) |
| 출력 해상도 | 입력 해상도 - 필터 해상도 + 1 |
| 편향(bias) | 필터마다 1개씩 존재 |
| 패딩 | 경계 손실 방지를 위해 사용 (입출력 해상도 동일 유지) |

---

📘 **정리 포인트**
- 합성곱 연산은 이미지의 공간 구조를 유지하면서 특징을 추출하는 핵심 과정  
- 출력 크기는 필터 크기에 따라 달라지며, 필요 시 패딩으로 보정  
- 여러 필터를 동시에 사용하면 다양한 특징을 병렬로 학습 가능  
- CNN의 출력은 여러 Activation Map의 집합으로 표현됨

# 1-2. 모델 구조

---

## 1-1. CNN 모델

### 합성곱 레이어 (Convolution Layer)
- 입력 이미지를 필터와 연산하여 특징 맵(feature map)을 뽑아내는 모듈  
- C_out×1 벡터로 구성된 **bias**도 함께 추가됨

**입출력 구조**
```text
입력: N × C_in × H × W   (이미지 배치)
필터: C_out × C_in × K_w × K_h
출력: N × C_out × H' × W'
```

출처: https://www.anthropic.com/claude

---

## 1-2. 모델 구조

### 합성곱 레이어 — 중첩
- 모델 상수(파라미터)를 증가시키면?
- Conv 레이어를 여러 개 중첩해 심층 구조를 형성할 수 있음.

**예시**
```text
입력: N × 3 × 32 × 32
W₁: 6×3×5×5, b₁=6
W₂: 10×6×3×3, b₂=10
W₃: 12×10×3×3, b₃=12
```

→ 첫 번째 히든 레이어: N × 6 × 28 × 28  
→ 두 번째 히든 레이어: N × 10 × 26 × 26  

---

### 합성곱 레이어 — 중첩 ②
> 어차피 Linear classifier인데 꼭 더 쌓아야 할까?

수식:
```text
W_all = W₃ W₂ W₁
```
→ 단순 선형 결합만으로는 복잡한 표현이 어려움.

---

### 합성곱 레이어 — 중첩 ③
> 비선형 블록(Conv[Linear] + ReLU[비선형])과 함께 사용 시 모델링 파워 향상

```text
구조 예시:
Conv → ReLU → Conv → ReLU → Conv → ReLU
```

이처럼 ReLU 활성화 함수를 포함하면 모델이 비선형성을 갖게 되어 더 복잡한 패턴 학습 가능.

---

## 1-2. CNN 모델 구조 — 필터의 의미

### 필터 시각화
- 학습된 필터를 시각화하면 모델이 어떤 정보를 학습했는지 이해할 수 있음.

| 모델 유형 | 필터 역할 |
|------------|------------|
| 선형모델 | 각 카테고리의 전형적 템플릿(예: 개, 자동차 등) |
| FCN | 다양한 템플릿 제공 |
| CNN | 계층별로 이미지의 지역적 템플릿(엣지, 색상 등) 학습 |

---

## 1-2. CNN 모델 구조 — 수용영역 (Receptive Field)

### 중첩과 수용영역
- CNN 레이어는 이미지의 작은 부분, 즉 “지역 정보(local feature)”를 추출하도록 설계됨.
- 하지만 전체 이미지를 이해하려면 “지역 정보 + 전체 맥락”을 함께 고려해야 함.

**수용영역이란?**
- CNN이 한 번에 볼 수 있는 영역의 크기  
- 네트워크의 ‘시야(field of view)’  
- 레이어가 깊어질수록 수용영역도 커짐 → 더 넓은 맥락 파악 가능

---

## 1-2. CNN 모델 구조 — 수용영역 (Receptive Field) ②

- 고해상도 이미지 처리 시 많은 레이어를 통과해야 함 → 연산량, 비용, 학습 난이도 증가  
- 실제 해결 방법: **입력 사이즈를 줄여서 모델에 입력**

예시:
- 입력 → 여러 단계의 컨볼루션을 거치며 수용 영역 확장  
- 출력 시 넓은 영역을 대표하는 특징을 학습

---

## 1-2. CNN 모델 구조 — 풀링 (Pooling)

### 정의
- 효율적 연산 및 위치 변화의 강건성 확보

### 주요 효과
- **위치 변화 강건성:** 입력 내 객체 위치가 약간 달라도 동일한 출력을 유지  
- **저해상도 기반 작업 가능:** 약간의 화소 이동은 무시됨

---

### CNN 레이어 연산 효율 향상
- 풀링은 CNN의 연산 효율성을 극대화함.  
- 예: 2×2 풀링으로 입력 해상도가 절반으로 줄어드는 경우

```text
입력 해상도 224×224 → 112×112로 감소
```

**연산량 비교**
```text
224×224×6 → 1.85 GFLOPS
112×112×6 → 0.46 GFLOPS
→ 약 4배 감소
```
> GFLOPS: 초당 10억 번의 연산을 처리하는 성능 단위

---

### 시각적 예시
- 입력 해상도: 224×224×64 → 풀링 후: 112×112×64  
- 공간 해상도는 절반으로 줄지만, **내용 인식은 거의 동일**

> 즉, 해상도 손실 없이 효율적 특징 추출 가능

## 📘 41p — 1-2. CNN 모델 구조 - 풀링 (Pooling)

**풀링(Pooling)**  
: 효율적 연산 및 위치 변화의 강건성 확보  

- **맥스풀링(Max Pooling)**  
  정해진 커널 사이즈(예: 2×2)로 이미지를 나누어 각 영역 내 가장 큰 값을 선택하는 연산  

**예시 (2×2 조각)**
```
입력 (단일 채널)
1 1 2 4
5 6 7 8
3 2 1 0
1 2 3 4

출력 (각 영역 최대값 선택)
6 8
3 4
```
> 모델 상수 학습 X  


---

## 1-2. CNN 모델 구조 - 스트라이드 합성곱

**스트라이드 합성곱(Strided Convolution)**  
: 풀링의 한계 개선  

- **일반 합성곱** : 필터를 한 칸씩 이동하면서 연산 수행 (출력값 계산)  
- **스트라이드 합성곱** : 필터를 스트라이드(S칸)만큼 이동 후 출력 연산  

**효과**  
- 풀링처럼 입력 해상도를 줄임 (연산 효율과 위치 강건성 확보)  
- 풀링 vs 스트라이드 합성곱  
  - 풀링: 학습 상수 X  
  - 스트라이드: **커널을 동시에 학습**  

**추가 내용**
- 해상도 저하로 인한 정보 손실이 적음  
- 고도화된 CNN에서는 스트라이드 합성곱을 풀링 대신 활용하는 경향  


---

## 스트라이드 합성곱 예시

**개념 시각화**

> 필터가 입력 이미지 위를 stride 간격으로 이동하며 합성곱 연산을 수행  
> stride 값이 커질수록 출력 feature map 크기 ↓, 연산 효율 ↑  

- stride = 1 → 겹치는 영역 많음 → 세밀한 표현  
- stride = 2 → 겹침 줄고, 더 큰 간격으로 특징 추출  

**예시 연산 (stride 2)**  
- 입력 영역의 평균을 구하는 필터(1/9 * 3×3)를 적용  
- stride 2 이동 시 출력의 공간 크기가 절반으로 감소  
- 출력 feature map의 각 위치는 입력의 국소 영역 평균값으로 계산  

→ **Pooling 없이도 Downsampling 효과** 획득  


---

## 2. CNN 기반 모델 변천사

**2010 ~ 2017년 CNN 모델 발전 흐름**

| 연도 | 모델 | 주요 특징 | Error Rate(%) | Layers |
|------|--------|------------------|----------------|---------|
| 2010 | Lin et al | Shallow | 28.2 | - |
| 2011 | Sanchez & Perronnin | Shallow | 25.8 | - |
| 2012 | **AlexNet** | ReLU, Dropout, GPU 병렬 | **16.4** | 8 |
| 2013 | Zeiler & Fergus | - | 11.7 | 8 |
| 2014 | **VGGNet** | 단순 구조 반복 | 7.3 | 19 |
| 2014 | **GoogLeNet** | Inception 구조 | 6.7 | 22 |
| 2015 | **ResNet** | Residual 구조 | 3.6 | 152 |
| 2016 | Shao et al | - | 3.0 | 152 |
| 2017 | **SENet** | 채널 주의 기법 | 2.3 | 152 |
| Human | 기준 성능 | - | 5.1 | - |

> Error Rate가 낮을수록 성능 우수  

## 2-1. AlexNet

### 5개의 합성곱 계층과 3개의 완전연결 계층으로 구성된 8계층 CNN 모델

- **2012년 Krizhevsky et al (AlexNet)**
- 8 layers
- Error Rate: 16.4%

#### 모델 구조 특징
- 5개 합성곱 레이어  
- 맥스 풀링  
- 3개 연결층 레이어  
- ReLU 비선형 활성화  
→ **총 8-레이어**

#### 입출력
- 입력: 이미지 / 3 × 227 × 227 해상도  
- 출력: 레이블 / 1 × 1024 벡터  
- 학습 시 정답은 원-핫 벡터 (원소 1개만 ‘1’이고 나머지는 ‘0’인 벡터)  
- 모델 예측값은 0~1 사이 확률 값  
- (ImageNet 기준, 각 클래스에 해당할 확률)

📊 **총 인용수: 162,207**

- 인용수 관련 비교:
  - Darwin, “On the origin of species”, 1859: **64,798**  
  - Shannon, “A mathematical theory of communication”, 1948: **156,596**  
  - Watson and Crick, “Molecular Structure of Nucleic Acids”, 1953: **19,392**

### Image input 224, 2*GPU

- 논문 그림에는 입력 크기가 224로 되어있으나, 227로 정정되었음  
- 당시 사용된 GPU는 GTX 580으로 메모리가 3GB밖에 되지 않아  
  → 2개의 GPU를 병렬 사용  
- Conv/FC 레이어를 GPU 2개에 분산 = 채널/뉴런 반씩 나눠 계산

### Image input 227, 1*GPU

- 최신 GPU (예: NVIDIA RTX 3090, A100, H100 등)  
  → 24GB ~ 80GB VRAM 보유

### AlexNet 단계별 구조

#### 입력 (Input)
- 크기: 227×227×3  
- RGB 이미지 (3채널)

#### 첫 번째 합성곱 층 (Conv1)
- Conv 11×11, stride=4, 96개 필터  
- 출력 크기: 55×55×96  
- 큰 필터와 큰 stride로 초반에 이미지를 강하게 줄이고 특징 추출 시작

#### 첫 번째 풀링층 (Max Pool1)
- Max Pool 3×3, stride=2 (Overlapping Pooling)
- 출력 크기: 27×27×96
- 해상도 절반으로 줄이면서 중요한 특징만 남김

#### 두 번째 합성곱 층 (Conv2)
- Conv 5×5, pad=2, 256개 필터
- 출력 크기: 27×27×256
- padding을 적용해 해상도 유지, 더 복잡한 특징 추출

#### 두 번째 풀링층 (Max Pool2)
- Max Pool 3×3, stride=2
- 출력 크기: 13×13×256

#### 세 번째 합성곱층 (Conv3)
- Conv 3×3, pad=1, 384개 필터  
- 출력 크기: 13×13×384

#### 네 번째 합성곱층 (Conv4)
- Conv 3×3, pad=1, 384개 필터  
- 출력 크기: 13×13×384

#### 다섯 번째 합성곱층 (Conv5)
- Conv 3×3, pad=1, 256개 필터  
- 출력 크기: 13×13×256

#### 세 번째 풀링층 (Max Pool3)
- Max Pool 3×3, stride=2  
- 출력 크기: 6×6×256  
- 마지막 공간 축소 → 이후 완전연결층으로 연결하기 쉽게 함

#### 완전 연결층 (FC Layers)
- Flatten → 벡터 크기: 9216 (6×6×256)  
- FC1: 4096 뉴런  
- FC2: 4096 뉴런

#### 출력층 (Output)
- FC3 → Softmax (1000 클래스)  
- ImageNet 1000개 클래스 확률 분포 출력

### AlexNet 출력 사이즈 계산법 [Conv1]

- 출력 채널 수 = 현재 레이어의 필터(커널) 개수  
- 출력 해상도  
  \[
  W' = \frac{W - K + 2P}{S} + 1
  \]

| Layer(층) | 입력 채널(C) | 세로/가로(H/W) | 필터 개수(F) | 커널 크기(K) | Stride(S) | Padding(P) | 출력 채널(C) | 세로/가로(H/W) |
|------------|---------------|----------------|---------------|---------------|-------------|--------------|----------------|
| Conv1 | 3 | 227 | 96 | 11 | 4 | 0 | ? | ? |

### AlexNet 출력 사이즈 계산법 [Conv1]

- 출력 채널 수 = 96  
- 출력 해상도  
  \[
  W' = \frac{227 - 11 + 2×0}{4} + 1 = 55
  \]

| Layer(층) | 입력 채널(C) | 세로/가로(H/W) | 필터 개수(F) | 커널 크기(K) | Stride(S) | Padding(P) | 출력 채널(C) | 세로/가로(H/W) |
|------------|---------------|----------------|---------------|---------------|-------------|--------------|----------------|
| Conv1 | 3 | 227 | 96 | 11 | 4 | 0 | **96** | **55** |

### AlexNet 연산비용 계산법 [Conv1]

- 출력 활성 메모리(KB) = C × H′ × W′ = 96 × 55 × 55 = 290,400  
- 출력값 메모리 사용 4 Byte  
  → 4 Byte × 290,400 = 1,161,600 Byte = 1,134 KB

| Layer(층) | 입력 채널(C) | 세로/가로(H/W) | 필터 개수(F) | 커널 크기(K) | Stride(S) | Padding(P) | 메모리(KB) | 상수(k) | 연산(FLOPs) |
|------------|---------------|----------------|---------------|---------------|-------------|--------------|-------------|-------------|---------------|
| Conv1 | 3 | 227 | 96 | 11 | 4 | 0 | 1,134 | ? | ? |

### AlexNet 연산비용 계산법 [Conv1]

- 상수 (파라미터 수)  
  총 개수 = 출력 채널 수 × 입력 채널 수 × K²(필터 웨이트) + 출력 채널 수(바이어스 웨이트)  
  → 96×3×11×11 + 96 = 34,944 (34.9K)

| Layer(층) | 입력 채널(C) | 세로/가로(H/W) | 필터 개수(F) | 커널 크기(K) | Stride(S) | Padding(P) | 메모리(KB) | 상수(k) | 연산(FLOPs) |
|------------|---------------|----------------|---------------|---------------|-------------|--------------|-------------|-------------|---------------|
| Conv1 | 3 | 227 | 96 | 11 | 4 | 0 | 1,134 | **34.9** | ? |

### AlexNet 연산비용 계산법 [Conv1]
- FLOPs(연산량) = 총 연산 (곱하기만 고려) = 출력 원소 수 × 출력 원소별 연산  
  = (H′ × W′) × (C_in × K × K × C_out)  
  → (55 × 55) × (3 × 11 × 11 × 96) × 2 = 105,415,200 (105.4M FLOPs)

| Layer(층) | C(입력 채널) | H/W(세로/가로) | F(필터 갯수) | K(커널 크기) | S(Stride) | P(Padding) | 메모리(KB) | 상수(k) | 연산(FLOPs) |
|------------|----------------|----------------|----------------|----------------|--------------|---------------|--------------|--------------|----------------|
| Conv1 | 3 | 227 | 96 | 11 | 4 | 0 | 1,134 | 34.9 | 105.4M |

### AlexNet [2Groups]
- 당시 Conv2/4/5에 그룹 합성곱 적용하여 연산량 감소 (2개의 GPU 병렬)

| Layer(층) | C(입력 채널) | H/W(세로/가로) | F(필터 갯수) | K(커널 크기) | S(Stride) | P(Padding) | C(출력 채널) | H/W(세로/가로) | 메모리(KB) | 상수(k) | 연산(FLOPs) |
|------------|----------------|----------------|----------------|----------------|--------------|---------------|----------------|----------------|--------------|--------------|----------------|
| Conv1 | 3 | 227 | 96 | 11 | 4 | 0 | 96 | 55 | 1,134 | 34.9 | 105.4M |
| Pool1 | 96 | 55 | - | 3 | 2 | 0 | 96 | 27 | 273 | 0 | 0 |
| Conv2 | 96 | 27 | 256 | 5 | 1 | 2 | 256 | 27 | 729 | 307.5 | 224M |
| Pool2 | 256 | 27 | - | 3 | 2 | 0 | 256 | 13 | 169 | 0 | 0 |
| Conv3 | 256 | 13 | 384 | 3 | 1 | 1 | 384 | 13 | 254 | 885 | 149.5M |
| Conv4 | 384 | 13 | 384 | 3 | 1 | 1 | 384 | 13 | 254 | 664 | 112.1M |
| Conv5 | 384 | 13 | 256 | 3 | 1 | 1 | 256 | 13 | 169 | 443 | 74.7M |
| Pool5 | 256 | 13 | - | 3 | 2 | 0 | 256 | 6 | 36 | 0 | 0 |
| Flatten | - | - | - | - | - | - | 9216 | - | 36 | 0 | 0 |
| FC1 | 9216 | - | - | - | - | - | 4096 | - | 16 | 37,752 | 37.7M |
| FC2 | 4096 | - | - | - | - | - | 4096 | - | 16 | 16,781 | 16.7M |
| FC3 | 4096 | - | - | - | - | - | 1000 | - | 4 | 4,097 | 4.1M |

### Memory [1Group]
- 메모리 사용은 초기 레이어에 집중

```
Conv1: 1134 KB
Conv2: 729 KB
Conv3: 254 KB
Conv4: 254 KB
Conv5: 169 KB
FC1: 16 KB
FC2: 16 KB
FC3: 4 KB
```

### Params [1Group]
- 모델 상수(K)는 연결층 레이어에 집중

```
Conv1: 35K
Conv2: 615K
Conv3: 885K
Conv4: 1327K
Conv5: 885K
FC1: 37,753K
FC2: 16,781K
FC3: 4,097K
```

### FLOPs [1Group]
- 합성곱에서 주로 발생

```
Conv1: 105.4M
Conv2: 447.9M
Conv3: 149.5M
Conv4: 224.3M
Conv5: 149.5M
FC1: 37.7M
FC2: 16.7M
FC3: 4.1M
```

---

## 2-2. VGGNet

### 5개의 합성곱 블록 + 맥스 풀링 구조

- Simonyan & Zisserman (VGG)
- 2014년 / 19 Layers / Error Rate 7.3%

### VGG-16 기준

#### 모델 구조 특징
- 5개 합성곱 블록 (각 블록당 합성곱 - ReLU 구조 반복)
- 각 블록당 맥스 풀링
- 3개 연결층 레이어
- ReLU 비선형 활성화
- 총 16개의 합성곱/연결층 레이어
- 최종 소프트맥스 수행

#### 입출력
- 입력: 이미지 / 3 × 224 × 224 해상도
- 출력: 레이블 / 1 × 1024 벡터

### 모델 비교
- AlexNet, VGG16, VGG19

```
[AlexNet]
Input → 11×11 conv, 96 → 5×5 conv, 256 → 3×3 conv, 384 → Pool → FC 4096 → FC 1000 → Softmax

[VGG16]
Input → (3×3 conv ×2) 64 → (3×3 conv ×2) 128 → (3×3 conv ×3) 256 → (3×3 conv ×3) 512 → (3×3 conv ×3) 512 → FC 4096 → FC 1000 → Softmax

[VGG19]
Input → (3×3 conv ×2) 64 → (3×3 conv ×2) 128 → (3×3 conv ×4) 256 → (3×3 conv ×4) 512 → (3×3 conv ×4) 512 → FC 4096 → FC 1000 → Softmax
```

### VGG의 레슨
- 작고 단순한 필터를 깊게 쌓으면 성능 향상  
  → 새로운 설계 철학 제시

#### 구조 비교
| Model | 주요 구성 |
|--------|-------------|
| VGG16 | Conv(3×3) ×13 + FC ×3 |
| VGG19 | Conv(3×3) ×16 + FC ×3 |

출처: Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015

## 2-2. VGGNet

### VGG의 디자인 룰
- 3×3 합성곱 / 스트라이드 1 / 패딩 1 반복 적용  
- 맥스풀링(2×2)으로 크기 줄이기  
- 풀링 후, 채널 크기 2배 적용  

#### 구조 예시
| 모델 | 구조 |
|-------|------|
| VGG16 | 3×3 conv, 512 → Pool → FC 4096 → Softmax |
| VGG19 | 3×3 conv, 512 → Pool → FC 4096 → Softmax |

출처: Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015

- 3×3 합성곱 / 스트라이드 1 / 패딩 1 반복 적용  
- 맥스풀링(2×2)으로 크기 줄이기  
- 풀링 후, 채널 크기 2배 적용  
  - 3×3 합성곱 두 개를 연달아 적용하면 5×5 합성곱과 동일 리셉티브 필드 확보  
  → 그럼에도 연산량을 크게 줄일 수 있음

| Option | 구성 | Params | FLOPs |
|--------|------|---------|--------|
| Option 1 | 합성곱(5×5, C→C) | 25C² | 25C²HW |
| Option 2 | 합성곱(3×3, C→C) ×2 | 18C² | 18C²HW |

출처: Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015

- 3×3 합성곱 / 스트라이드 1 / 패딩 1 반복 적용  
- 맥스풀링(2×2)으로 크기 줄이기  
- 풀링 후, 채널 크기 2배 적용  
  - 3×3 합성곱 두 개를 연달아 적용하면 5×5 합성곱과 동일 리셉티브 필드 확보  
  → 그럼에도 연산량을 크게 줄일 수 있음  

> AlexNet에서는 레이어별 연산량 차이가 매우 큼.  
> VGG는 해상도를 줄일 때마다 채널을 늘려 레이어별 연산량 차이를 줄임.

출처: Simonyan and Zissermann, “Very Deep Convolutional Networks for Large-Scale Image Recognition”, ICLR 2015

### AlexNet vs VGGNet : VGG는 거대한 모델

#### 메모리(Memory) 비교
- AlexNet(1.9MB) vs VGG-16(48.6MB) → 약 25배

```
Memory Usage (MB)
Conv1: 1.134 (AlexNet) vs 25.0 (VGG16)
Conv2: 0.729 vs 12.8
Conv3: 0.254 vs 6.2
Conv4: 0.254 vs 4.5
Conv5: 0.169 vs 1.2
FC6~FC8: 소폭 증가
```

#### 모델 상수(Parameters) 비교
- AlexNet(61M) vs VGG-16(138M) → 약 2.3배

```
Parameters (Millions)
Conv1: 0.035 vs 0.1  
Conv2: 0.615 vs 1.2  
Conv3: 0.885 vs 2.3  
Conv4: 1.327 vs 4.4  
Conv5: 0.885 vs 5.6  
FC6: 37.753 vs 102.8  
FC7: 16.781 vs 16.8  
FC8: 4.097 vs 7.0
```

#### 연산량(Compute) 비교
- AlexNet(0.7GFLOPs) vs VGG-16(13.6GFLOPs) → 약 19.4배

```
Compute (GFLOPs)
Conv1: 0.105 vs 1.8  
Conv2: 0.448 vs 2.8  
Conv3: 0.150 vs 2.8  
Conv4: 0.224 vs 4.6  
Conv5: 0.150 vs 1.4  
FC6~8: 0.038~0.02 수준
```

### 모델 특징
- 단순함 + 깊이의 강력한 성능을 보임  
- 단순 설계로 모델 해석에 이상적  
- 특징 추출기, 전이학습에 강력한 베이스라인  
  *파라미터가 많고 연산량 요구가 매우 큼*

---

## 2-3. ResNet

### 합성곱 블록(VGG유사)과 잔차(Residual) 블록
- 2015년, He et al (ResNet)  
- 152 layers, Error Rate: 3.6 

### 모델 구조 특징 (ResNet-50 기준)
- 합성곱 블록(VGG 유사)과 잔차(Residual) 블록  
- 잔차 블록은 블록입력을 그대로 출력에 더해주는 지름길 연결(Skip Connection) 구조  
- Inception 모델에서 차원 축소로 활용된 1×1 합성곱을 적용, 연산 효율 개선

#### 입출력
- 입력: 이미지 / 3 × 224 × 224 해상도  
- 출력: 레이블 / 1 × 1024 벡터

### ResNet의 등장 배경
- 배치 정규화 방식으로 10+ 레이어 학습 가능  
  - 20-layer 모델: 학습이 안정적으로 진행되며 에러가 꾸준히 감소  
  - 56-layer 모델: 더 깊음에도 불구하고 training error가 오히려 높음  
- 깊다고 무조건 더 잘 학습되는 것은 아님 (Degradation problem)

출처: He et al., “Deep Residual Learning for Image Recognition”, CVPR 2016

- 배치 정규화 방식으로 10+ 레이어 학습 가능  
  - 20-layer 모델: Test error가 점진적으로 낮아짐  
  - 56-layer 모델: Test error가 더 높게 유지됨  
→ 단순히 레이어를 깊게 하면 오히려 성능이 떨어짐

출처: He et al., “Deep Residual Learning for Image Recognition”, CVPR 2016

### ResNet의 아이디어
- 배치 정규화 방식으로 10+ 레이어 학습 가능  
- 작은 모델(10+레이어)의 성능은 “최소한” 누릴 수 있도록  
  작은 레이어의 추정값(입력값)을 그대로 후속 레이어에 제공하여  
  최소한 작은 레이어 수준의 성능은 보장  
→ **Residual Block(잔차 블록)**의 핵심 아이디어  

```
F(x) → 3×3 conv → ReLU → 3×3 conv → ReLU
출력: F(x) + x (입력값을 더함)
```

출처: He et al., “Deep Residual Learning for Image Recognition”, CVPR 2016

### Residual Block (잔차 블록)

#### 입력 X가 두 경로로 나뉘어 전달됨
1. **변환 경로 (F(x))**  
   → Conv(3×3) → ReLU → Conv(3×3) → ReLU → 출력 F(x)  
2. **Shortcut 경로 (Identity mapping)**  
   → 입력 X를 그대로 전달  

#### 마지막에 두 값을 더함  
Output = F(x) + x  

- 학습이 잘 안 된다면 F(x) ≈ 0 이 될 수 있고  
  → 출력은 x가 됨 → 최소 성능 보장  

> 이듬을 0으로 초기화하면 작은 모델과 동일한 예측 가능

출처: He et al., “Deep Residual Learning for Image Recognition”, CVPR 2016

### ResNet 잔차 블록 (깊게 보기)

- **Plain Residual Block(일반 잔차 블록)** vs **Bottleneck Residual Block(보틀넥 잔차 블록)**  
- 연산 효율을 위해 보틀넥 구조를 도입  
→ FLOPs 비교 : 18HWC² vs 17HWC²  

| 블록 형태 | 구조 | 전체 FLOPs |
|------------|--------|-------------|
| 일반 잔차 블록 | Conv(3×3, C→C) ×2 | 18HWC² |
| 보틀넥 잔차 블록 | Conv(1×1, C→4C) → Conv(3×3, C→C) → Conv(1×1, 4C→C) | 17HWC² |

출처: He et al., “Deep Residual Learning for Image Recognition”, CVPR 2016

- 연산 효율을 위해 보틀넥 잔차구조를 도입  
- **왜 보틀넥 잔차?**  
  → 더 깊은 모델을 더 적은 연산으로 달성  
→ FLOPs 비교 : 18HWC² vs 17HWC²  

| 블록 형태 | 구조 | 전체 FLOPs |
|------------|--------|-------------|
| 일반 잔차 블록 | Conv(3×3, C→C) ×2 | 18HWC² |
| 보틀넥 잔차 블록 | Conv(1×1, C→4C) → Conv(3×3, C→C) → Conv(1×1, 4C→C) | 17HWC² |

출처: He et al., “Deep Residual Learning for Image Recognition”, CVPR 2016

### Stem 구조
- 기존 모델의 효율성 레시피를 잘 활용  

1) **스텝 구조를 모델 초기에 도입**,  
   입력 데이터 해상도를 ¼로 줄이고 이후 잔차 블록 적용  

출처: He et al., “Deep Residual Learning for Image Recognition”, CVPR 2016

### Stem 구조
- 기존 모델의 효율성 레시피를 잘 활용  

1) 스텝 구조를 모델 초기에 도입, 입력 데이터 해상도를 ¼로 줄이고 이후 잔차 블록 적용  
2) 여러 개의 FC 레이어를 제거하고(GoogleLeNet에서 제안)  
   전역 평균 풀링(Global Average Pooling)을 사용 → 마지막 1개 FC만 적용  

출처: He et al., “Deep Residual Learning for Image Recognition”, CVPR 2016

### ResNet-18과 ResNet-34

#### ResNet-18
- Stem: 1 conv layer  
- Stage1 (C=64): 2 res. block = 4 conv  
- Stage2 (C=128): 2 res. block = 4 conv  
- Stage3 (C=256): 2 res. block = 4 conv  
- Stage4 (C=512): 2 res. block = 4 conv Linear  

#### ResNet-34
- Stem: 1 conv layer  
- Stage1 (C=64): 3 res. block = 6 conv  
- Stage2 (C=128): 4 res. block = 8 conv  
- Stage3 (C=256): 6 res. block = 12 conv  
- Stage4 (C=512): 3 res. block = 6 conv Linear  

| 모델 | 구조 특징 | Top-5 Error(%) | GFLOPs | 특징 |
|------|------------|----------------|--------|------|
| ResNet-18 | 18 layers (Residual Block) | 10.92 | 1.8 | 얕고 가볍지만 VGG보다 약간 성능 낮음 |
| ResNet-34 | 34 layers (Residual Block) | 8.58 | 3.6 | 성능 ↑, VGG보다 정확하고 가벼움 |
| VGG-16 | 16 conv layers (Plain CNN) | 9.62 | 13.6 | 성능은 괜찮지만 연산량 매우 큼 |

출처: Error rates are 224×224 single-crop testing, reported by torchvision

### 다양한 조합
- ResNet-34와 ResNet-50 차이는 **Basic 잔차 블록** vs **보틀넥 잔차 블록** 사용 여부  
- ResNet-50은 지금도 활발히 사용되는 CNN 기반 강력한 베이스라인  

| 모델-레이어(총) | Block Type | Stem layers | Stage1 Blocks | Stage1 Layers | Stage2 Blocks | Stage2 Layers | Stage3 Blocks | Stage3 Layers | Stage4 Blocks | Stage4 Layers | FC layers | GFLOPs | ImageNet top-5 error |
|----------------|-------------|--------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|--------------|--------------|----------------|
| ResNet-18 | Basic | 1 | 2 | 4 | 2 | 4 | 2 | 4 | 2 | 4 | 1 | 1.8 | 10.92 |
| ResNet-34 | Basic | 1 | 3 | 6 | 4 | 8 | 6 | 12 | 3 | 6 | 1 | 3.6 | 8.58 |
| ResNet-50 | Bottle | 1 | 3 | 9 | 4 | 12 | 6 | 18 | 3 | 9 | 1 | 3.8 | 7.13 |
| ResNet-101 | Bottle | 1 | 3 | 9 | 4 | 12 | 23 | 69 | 3 | 9 | 1 | 7.6 | 6.44 |
| ResNet-152 | Bottle | 1 | 3 | 9 | 8 | 24 | 36 | 108 | 3 | 9 | 1 | 11.3 | 5.94 |

### 잘 알려진 ResNet 학습 레시피
- 모든 Conv 레이어 적용 시 배치정규화 수행  
- Xavier 초기화(He 초기화) 적용 → 그래디언트 소실 문제 다룸  
- SGD + 모멘텀(0.9) 사용  
- 학습 비율 0.1 적용, 단 Validation 오류가 정체되면 1/10로 비율 축소  
- 미니 배치 사이즈는 256 사용  
- Weight Decay 1e-5  
- Dropout 미사용  

### 최종 요약
- 깊은 모델 학습을 위한 중요한 전진 (100+ 레이어 학습 가능)
- 깊은 모델의 강력함을 다시 확인
- 제안 당시 모든 벤치마크에서 최고 성능 달성
- 지금도 CNN 기반 구조 중 가장 활발하게 활용

**MSRA @ ILSVRC & COCO 2015 Competitions**
- ImageNet Classification: “Ultra-deep” 152-layer nets  
- ImageNet Detection: 16% better than 2nd  
- ImageNet Localization: 27% better than 2nd  
- COCO Detection: 11% better than 2nd  
- COCO Segmentation: 12% better than 2nd  

---

## 2-4. MobileNet

### 목표: 모바일/임베디드 환경에서 구동 가능
- 기존 합성곱은 공간(H×W)과 채널(C)을 동시에 처리하여 연산량이 과도하게 요구됨  
- **MobileNet의 핵심 아이디어:** 공간과 채널을 두 단계로 분리하여 처리  
  - 깊이별 합성곱: 각 채널별 독립적 3×3 합성곱 수행  
  - 화소별 합성곱: 1×1 합성곱을 채널 방향으로 적용  

**효과**
- 연산량 기존 대비 9배 가량 절감  
- 모델 상수 대폭 감소  

### 깊이별 합성곱 / 화소별 합성곱 예시

**일반 합성곱**
```
Conv(3×3, C→C)
BatchNorm
ReLU
연산량: 9C²HW
```

**깊이별(Depthwise) / 화소별(Pointwise) 합성곱**
```
Conv(3×3, C→C, groups=C)   → 깊이별 합성곱
BatchNorm
ReLU
Conv(1×1, C→C)             → 화소별 합성곱
BatchNorm
ReLU
연산량: 9CHW + C²HW
```

### 깊이별 합성곱 / 화소별 합성곱 예시
- 일반 합성곱: 9C²HW  
- 깊이별 + 화소별 합성곱: 9CHW + C²HW  

### 깊이별 합성곱 / 화소별 합성곱 예시 (연산량 절감 비율)
```
연산량 절감 비율 = 9C²HW / (9CHW + C²HW)
                 = 9C / (9 + C)
                 ≈ 9 (as C → ∞)
```

즉, **약 9배 효율적인 연산 구조**를 가짐.

### 이후 모델에 영향력
- 가볍고 빠른 모델로 엣지 디바이스에서 딥러닝 모델 실시간 실행 가능함을 보임  
- 효율형 모델 구조의 표준 제시:  
  → 깊이별 합성곱과 보틀넥 구조가 이후 경량 모델에서 지속적으로 채택됨  

---

## 확인문제

1️⃣ CNN 기반 모델 변천사를 순서대로 나열하면?  
A. MobileNet  
B. VGG  
C. AlexNet  
D. GoogLeNet  
E. ResNet  

---

## 확인문제 정답 및 해설

**정답:** C → B → D → E → A

**해설:**
- **AlexNet (2012)** → ILSVRC 대회 우승, 딥러닝 CNN의 본격적인 시작점  
- **VGG (2014)** → 3×3 작은 필터를 깊게 쌓은 단순하지만 강력한 구조  
- **GoogLeNet / Inception (2014)** → Inception 모듈 도입, 효율적인 연산 적용  
- **ResNet (2015)** → Residual Connection(잔차 연결) 도입, 기울기 소실 문제 해결  
- **MobileNet (2017)** → 모바일·임베디드 환경 최적화, Depthwise Separable Convolution 도입  

> 각 모델은 깊이, 효율성, 연결 구조, 경량화 등 CNN의 한계를 단계적으로 극복하며 발전했다.

## 확인문제 ②

**CNN 모델의 특징과 가장 먼 것은?**

① 이미지의 모든 화소를 나열하여 1차원 벡터로 만들어 입력 데이터를 활용한다.  
② 기본 구조는 합성곱 → 활성화 → 풀링 → 완전연결층으로 이루어져 있다.  
③ 지역적인 특징을 먼저 학습하고, 층이 깊어질수록 추상적 특징을 학습한다.  
④ 위치 변화나 노이즈에 대해 강건하다.  
⑤ 층이 깊어질수록 지역 수용 영역(Receptive Field)이 커져, 더 넓은 맥락을 이해할 수 있다.

---

## 확인문제 정답 및 해설 ②

**정답:** ①  

**해설:**  
CNN에서도 마지막 분류 단계에서 FC Layer(완전연결층)을 쓰기 때문에 feature map을 1차원 벡터로 변환하긴 하지만,  
이는 **출력 단계**에서만 일어나는 보조 과정이다.  
CNN의 핵심은 입력 이미지의 2차원 구조를 보존한 채 합성곱과 풀링으로 지역적 특징을 추출하는 것.  

따라서  
> "이미지의 모든 화소를 나열하여 1차원 벡터로 입력한다"  
는 설명은 CNN의 특징과 거리가 멀다.

---

## 확인문제 ③

**CNN에서 풀링(Pooling)의 주요 목적이 아닌 것은?**

① 차원 축소 및 연산량 감소  
② 위치 변화에 대한 불변성 확보  
③ 네트워크의 파라미터 수 증가  
④ 지역적 특징을 요약하여 다음 층에 전달  

---

## 확인문제 정답 및 해설 ③

**정답:** ③  

**해설:**  
① Pooling은 보통 Max Pooling이나 Average Pooling을 사용하여 입력의 크기를 줄이는 역할을 한다.  
   → 연산량 감소, 학습 속도 향상, 메모리 사용량 절감 효과가 있다.  

② Max Pooling은 특정 맵의 가장 두드러진 값을 취함으로써,  
   입력 이미지가 이동하거나 노이즈가 추가되어도 중요한 특징을 유지하게 해준다.  
   → 위치 변화에 대한 불변성을 확보.  

④ Pooling은 인접한 픽셀 정보를 요약하여 지역적 특징을 압축된 형태로 전달.  
   이후 합성곱 층이 더 넓은 맥락(Context)을 이해할 수 있게 돕는다.  

③ Pooling 연산은 **비학습적 연산**이며 학습해야 할 파라미터가 존재하지 않는다.  
   오히려 feature map의 크기를 줄여 **파라미터 수를 감소시킨다.**  
   따라서 “네트워크의 파라미터 수 증가”는 목적과 거리가 멀다.

---

## 강의 정리

### 오늘 공부한 내용 요약 및 정리

#### NN의 기본 구조
- 합성곱 → 활성화 → 풀링 → 완전연결층  
- 지역 특징 추출 / 다중 레이어 기반의 계층적 학습  

#### CNN 단위 연산과 특징
- 필터, 리셉티브 필드, 패딩, 스트라이드  
- 풀링과 스트라이드 합성곱의 차이  
- 1×1 합성곱을 활용한 효율적 연산  

#### 주요 모델 변천사
- **AlexNet → VGG → GoogLeNet → ResNet → MobileNet**  
- 각 모델의 설계 철학과 혁신 포인트

---

# 딥러닝 및 이미지 모델  
## “2차시: 다양한 신경망 모델”

---

## 목차 (CONTENTS)

### 1️⃣ CNN의 한계
1. CNN의 구조 및 장점  
2. 한계  

### 2️⃣ 시퀀스 데이터: RNN
1. 순차 데이터 처리 방식  
2. 단순 RNN 한계  
3. 대안 모델  

### 3️⃣ 긴거리 의존성: 어텐션 / ViT
1. 어텐션 메커니즘  
2. ViT 위치 인코딩  

---

## 학습 목표

- CNN의 한계를 이해하고, 이미지 인식에서 왜 새로운 접근이 필요한지 확인한다.  
- RNN, Attention 기반 Transformer의 핵심 아이디어와 구조를 파악하고,  
  각 모델이 해결하는 문제를 이해한다.  
- 모델별 설계 철학과 특징을 비교하며,  
  데이터 유형(이미지, 시퀀스, 긴 맥락의 중요성)에 따라 적절한 모델을 선택할 수 있다.

---

## 0. 학습 시작 (오버뷰)

### 왜 CNN만으로는 부족할까?
- 순서가 중요한 데이터 처리 불가  
- 긴 거리 의존성(Long-range Dependency) 부족  

---

### 시퀀스 데이터 처리에 적절한 방법은? → RNN!
- RNN의 기본 아이디어  
- RNN의 한계  

---

### 긴 거리 의존성을 확보할 방법은 없을까? → Attention! ViT!
- 어텐션 메커니즘 개념  
- 비전 트랜스포머 (ViT)

# 1. CNN의 한계

---

## 1-1. CNN의 구조 및 장점

### 합성곱 구조의 특성
- **지역적인 특징 학습에 특화**  
  → 작은 필터로 학습하여 패턴/경계선 등의 특징을 포착하기 용이함  
- **파라미터 효율성 우수**  
  → 지역적 특징 추출 시 이미지 블록별 가중치를 공유하여 연산량 절감  
- **위치 변화와 노이즈에 견고함**  
  → Pooling / Stride 합성곱으로 세부 차이는 줄이고 이미지 전체 의미에 집중  
- **이미지 기반 과업에서 표준 모델로 활용**  
  → 이미지 분류, 탐지, 인식 등 다양한 비전 분야에서 핵심 기반  

**구성 요소**
- 합성곱 레이어 (Convolution Layer)  
- 풀링 (Pooling)  
- 비선형 함수 (Non-linearity)  
- 정규화 (Normalization)  
- 연결층 (Fully Connected Layer)  

> [그림 2-1] CNN의 핵심 구조

---

## 1-2. 한계

---

### CNN의 핵심 한계 ① — 데이터 순서(order) 무시

#### CNN은 데이터의 순서를 반영하기 어려움
- **한계:** 순차적(sequential) 데이터의 시간적 흐름(텍스트/음성 등)을 처리하기 어려움  
- **예시:**  
  - “약을 먹고 잠을 자고” vs “잠을 자고 약을 먹고”  
    → 같은 단어라도 **순서**가 바뀌면 의미가 달라짐  
  - “I am a boy not a girl” vs “I am a girl not a boy”  
    → 단어 조합은 같지만 순서가 반대이면 의미가 완전히 다름  

> CNN은 순서에 따라 문맥이 달라지는 데이터를 적절히 처리하기 어려움.

---

### CNN의 핵심 한계 ② — 긴 거리 의존성 부족

- **한계:**  
  CNN은 **국소적(로컬)** 특징 추출에는 강하지만,  
  이미지나 문장에서 **멀리 떨어진 요소 간의 관계**를 잘 학습하지 못함.  

- **이미지 예시:**  
  - 이미지의 특성상 반복되는 패턴이 발생함  
  - 멀리 떨어져 있지만 유사한 구조를 가진 부분을 인식하는 것이 어려움  

> [그림 2-2] 이미지 내에서 발생하는 중복 패턴

---

### CNN의 핵심 한계 ③ — 픽셀 단위 복원 한계

- **한계:**  
  CNN은 이미지 전체 정보를 압축(예: 라벨 분류)에 효과적이지만,  
  위치 정보는 손실됨. (Pooling / Stride 효과)  

→ **픽셀 단위 복원**(예: 세그멘테이션)이나  
   **영상 생성**(예: 생성, 편집, 개선 등) 문제를 다루기 위한  
   **새로운 구조의 필요성**이 제기됨.  

**예시:**
- 동일한 고양이 이미지를 좌/우 반전해도 CNN은 “Cat”으로 인식함.  
- 하지만 세밀한 위치 복원이 필요한 문제에서는 정확한 위치 정보 손실이 문제됨.  

> 출처: [Quora - What is shift invariance in CNN?](https://www.quora.com/What-is-shift-invariance-in-a-convolutional-neural-network-CNN)

---

# 2. 시퀀스 데이터 처리: RNN

---

## 2-1. RNN (Recurrent Neural Nets)

### 순차적(sequential) 데이터를 처리하기 위해 고안된 신경망 구조

#### 순차적 정보 반영
- 이전 단계의 데이터를 **은닉 상태(hidden state)** 로 전달  
- 다음 단계로 이어지며 **시간/순서의 흐름**을 반영할 수 있음  

#### 시계열 / 언어 데이터 적합
- 문장, 음성신호, 센서 데이터 등 시계열 데이터 처리에 강력한 귀납 편향 제공  

#### 예시
- **문장 생성**  

**학습 단계 예시**

| Input Chars | Input Layer | Hidden Layer | Output Layer | Target Chars |
|--------------|--------------|---------------|---------------|---------------|
| “h” | [1, 0, 0, 0] | [0.3, -0.1, 0.9] | [1.0, 2.2, -3.0, 4.1] | “e” |
| “e” | [0, 1, 0, 0] | [1.0, 0.3, 0.1] | [0.5, 0.3, -1.0, 1.2] | “l” |
| “l” | [0, 0, 1, 0] | [0.1, -0.5, -0.3] | [0.1, 0.5, 1.9, -1.1] | “l” |
| “l” | [0, 0, 1, 0] | [-0.3, 0.9, 0.7] | [0.2, -1.5, -0.1, 2.2] | “o” |

> 순차 입력(“h” → “e” → “l” → “l”)을 거치며 은닉 상태가 전달되고  
> 다음 출력 단계에 반영됨.

## 순차적(sequential) 데이터를 처리하기 위해 고안된 신경망 구조

- **순차적 정보 반영:**  
  이전 단계 데이터를 은닉 상태(hidden state)로 다음 단계에 전달  
  → 시간/순서의 흐름을 반영할 수 있다.

- **시계열/언어 데이터 적합:**  
  문장, 음성신호, 센서 데이터 등 시계열 데이터 처리에 관한 강력한 귀납 편향 제공

- **예시:** 문장 생성  

> [실행단계]  
> One-hot encoding → Input Layer → Hidden Layer → Output Layer → Target Chars

---

## Input Chars

- 학습하려는 문장의 문자열들  
- 각 문자를 숫자로 표현하기 위해  
  ‘One-hot encoding’ 같은 방식으로 변환되어 Input Layer로 들어감  

---

## Input Layer

- 1과 0으로 된 벡터, 길이는 알파벳 집합 개수와 동일  
- RNN이 문자를 수치적으로 다룰 수 있게  
  각 문자를 벡터화(One-hot 벡터)한 것  
- **W_xh:** 입력 → 은닉 변화  

---

## Hidden Layer

- 현재 입력(Input) + 이전 상태의 은닉 상태를 합쳐 만든 내부 메모리  
- Hidden state가 바로 “이전까지의 문맥(Context)”을 담고 다음 단계로 전달  
- **W_hh:** 이전 은닉 → 현재 은닉  

---

## Output Layer

- Hidden state를 출력 벡터로 변환한 것  
- 각 숫자는 다음 문자가 될 확률을 계산하기 위한 점수  
- **W_hy:** 은닉 → 출력 변화  

---

## Target Chars

- 각 시점에서 예측해야 하는 정답 문자  
- 이 정답과 Output Layer 확률을 비교한 오차로 가중치 업데이트  

## RNN 셀
- 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할  
  - x: 입력층의 입력 벡터  
  - y: 출력층의 출력 벡터  
  - cell: 이전의 값을 기억하려고 하는 일종의 메모리 역할 수행  

- 은닉 상태(hidden state):  
  메모리 셀이 출력층 방향 또는 다음 시점인 t+1의 자신에게 보내는 값  

## RNN 표현 방식
- 재귀(왼쪽) == 사이클(오른쪽)

## 시계열 의존성
- \( h_t \) 가 이전 상태 \( h_{t-1} \) 에 의존하고 있음  
- 데이터의 시계열 특징을 반영하는 모델이나,  
  RNN 상수는 시계열(혹은 순서)에 무관, 동일하게 학습됨.  

\[
h_t = f_W(h_{t-1}, x_t)
\]

| 기호 | 의미 |
|------|------|
| \( h_t \) | 현재 은닉 상태 |
| \( h_{t-1} \) | 이전 은닉 상태 |
| \( f_W \) | 모델의 가중치 W가 포함된 RNN 함수 |
| \( x_t \) | 현재 입력 데이터 |

## 순수(vanilla) RNN
- 선형변환, 바이어스를 모델상수/비선형 (tanh) 함수 추가로 구성된 단순한 형태  

### 은닉 상태 계산
- \( h_t \): 현재 은닉 상태  
- \( h_{t-1} \): 이전 은닉 상태  
- \( x_t \): 현재 입력 데이터  

\[
h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t)
\]

> tanh(·): 비선형 변환을 통해 더 복잡한 패턴 학습 가능

## 순수(vanilla) RNN
- 선형변환, 바이어스를 모델상수/비선형 (tanh) 함수 추가로 구성된 단순한 형태  

### 출력 계산
- 은닉 상태를 기반으로 최종 출력 생성 (예: 다음 문자 예측)

\[
h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t)
\]
\[
y_t = W_{hy}h_t
\]

> 최종 출력 생성

## RNN 입출력 구조
- **One-to-Many:** 하나의 입력에서 시간축으로 출력이 생성  
- **Many-to-One:** 여러 시점 입력이 하나의 출력으로 압축  
- **Many-to-Many:** 입력과 출력이 시간축을 따라 1:1 혹은 n:m으로 매핑  

| 구조 | 설명 | 대표 예시 |
|------|------|------------|
| One-to-One | 단일 입력 → 단일 출력 | 이미지 → 라벨 (CNN 분류) |
| One-to-Many | 단일 입력 → 시퀀스 출력 | 이미지 → 캡션 생성, 음악 생성, 문장 생성 |
| Many-to-One | 시퀀스 입력 → 단일 출력 | 문장 → 감정 분류(긍/부정), 음성 → 라벨, 비디오 → 행동 분류 |
| Many-to-Many | 시퀀스 입력 → 시퀀스 출력 | 기계 번역(영어→한국어), 음성 → 텍스트 |

> [표 2-1] RNN 입출력 구조들

---

# 2-1. RNN 계산 그래프

## 그래프 시각화
- 초기 은닉 상태: \( h_0 \)  
- 입력: 첫 번째 시계열 데이터 / 시퀀스 데이터  

## 그래프 시각화
- 동일 함수 \( f_w \) 를 모든 시퀀스에 적용  

## 그래프 시각화
- 반복 적용  

## 그래프 시각화
- 최종 은닉 상태 → 출력  

```
h0 → f_w → h1 → f_w → h2 → f_w → h3 → … → h_t
         ↑       ↑       ↑
         x1      x2      x3
```

## 그래프 시각화
- W 학습: 역전파 시 모든 은닉 상태에 발생한 기울기를 더하여 연산  

```
h0 → f_w → h1 → f_w → h2 → f_w → h3 → … → h_t
         ↑       ↑       ↑
         x1      x2      x3
```

## 그래프 시각화
- 시퀀스 입력 - 단일 출력  

```
h0 → f_w → h1 → f_w → h2 → f_w → h3 → … → h_t → y
         ↑       ↑       ↑
         x1      x2      x3
```

## 그래프 시각화
- 단일 입력 - 시퀀스 출력  

```
h0 → f_w → h1 → f_w → h2 → f_w → h3 → … → h_t
         ↑       ↑       ↑
          x     y1      y2       y3 … y_t
```

## 그래프 시각화
- 시퀀스 입력 - 시퀀스 출력  

```
시퀀스 입력 - 단일 출력 (Many-to-One)
h0 → f_w → h1 → f_w → h2 → f_w → h3
         ↑       ↑       ↑
         x1      x2      x3

시퀀스 입력 - 시퀀스 출력 (Many-to-Many)
h_t → f_w' → h1 → f_w' → h2 → f_w' → h3 …
         ↑       ↑       ↑
         y1      y2      y3
```

## 예시
- 언어 생성  
  단위 데이터: [h, e, l, o]

```
"h" → [1, 0, 0, 0] → [0.3, -0.1, 0.9] → [1.0, 2.2, -3.0, 4.1] → [0.03, 0.13, 0.00, 0.84] → “e”
Input → Input Layer → Hidden Layer → Output Layer → Softmax → Sample
```

## 예시
- 언어 생성  

```
"h" → [1, 0, 0, 0] → [0.3, -0.1, 0.9] → [1.0, 2.2, -3.0, 4.1] → [0.03, 0.13, 0.00, 0.84] → “e”
"e" → [0, 1, 0, 0]
```

## 예시
- 언어 생성  

```
"h" → [1, 0, 0, 0] → [0.3, -0.1, 0.9] → [1.0, 2.2, -3.0, 4.1] → [0.03, 0.13, 0.00, 0.84] → “e”
"e" → [0, 1, 0, 0] → [1.0, 0.3, 0.1] → [0.5, 0.3, -1.0, 1.2] → [0.25, 0.20, 0.05, 0.50] → “l”
```

## 예시
- 언어 생성  

```
"h" → [1, 0, 0, 0] → [0.3, -0.1, 0.9] → [1.0, 2.2, -3.0, 4.1] → [0.03, 0.13, 0.00, 0.84] → “e”
"e" → [0, 1, 0, 0] → [1.0, 0.3, 0.1] → [0.5, 0.3, -1.0, 1.2] → [0.25, 0.20, 0.05, 0.50] → “l”
"l" → [0, 0, 1, 0] → [0.1, -0.5, -0.3] → [0.1, 0.5, 1.9, -1.1] → [0.11, 0.17, 0.68, 0.03] → “l”
"l" → [0, 0, 1, 0] → [-0.3, 0.9, 0.7] → [0.2, -1.5, -0.1, 2.2] → [0.11, 0.02, 0.08, 0.79] → “o”
```

---

# 2-2. 단순 RNN 한계

---

## 기울기 폭발 혹은 소실 발생
- 역전파의 시퀀스 데이터가 적용되면서 선형가중치 W가 기울기 계산 시, 계속 곱셈으로 반영됨.  
- |W|의 특이값 ↑ → 곱할수록 무한히 커짐 → 기울기 **폭발**  
- |W|의 특이값 ↓ → 곱할수록 0에 수렴 → 기울기 **소실**  
- 이상적으로 W의 특이값이 1이면 온전한 학습 가능!  

```
W — tanh
↑   ↓
stack
```

각 상태에서의 기울기는 모두 선형 가중치의 곱으로 표현  
(→ 순전파(Forward), 역전파(Backward))

---

## 강제 절삭! 그러나..
- 폭발하지 않도록 기울기 값을 절삭할 수 있음.  
- 그러나, 결국 잘못된 기울기 값을 양산하는 문제  
- 기울기 소실은 해결 불가!  
→ **다른 모델!**

---

# 2-3. 대안 모델

---

## LSTM (Long Short-Term Memory)

- **Long Short Term Memory**: 셀 상태, 게이트 유닛으로 구성  
  - **셀 상태 (Cell State)**: 중요한 정보를 유지/기억하는 역할  
  - **게이트 (Gate)**: 정보의 흐름을 제어  
    - 입력은 얼마나 저장, 과거 정보는 얼마나 버릴지  
    - 셀 상태를 얼마나 은닉정보에 반영할지  
  - **구동철학**: 긴 시퀀스 데이터에서 필요한 정보는 유지, 불필요한 정보는 지워 안정적 학습 유도  

\[
\begin{pmatrix} i \\ f \\ o \\ g \end{pmatrix}
=
\begin{pmatrix} \sigma \\ \sigma \\ \sigma \\ \tanh \end{pmatrix}
W \begin{pmatrix} h_{t-1} \\ x_t \end{pmatrix}
\]
\[
c_t = f \odot c_{t-1} + i \odot g
\]
\[
h_t = o \odot \tanh(c_t)
\]

(노란색 강조: 셀 상태 / 은닉 상태)

---

## LSTM (Long Short-Term Memory)

- **입력 게이트 (i)**: 새로운 정보의 저장  
- **망각 게이트 (g)**: 과거 정보를 제거  
- **출력 게이트 (o)**: 셀 상태를 현재 은닉 상태로 출력  

(노란색 강조: 셀 상태 / 은닉 상태)

---

## LSTM (Long Short-Term Memory)

- **입력 게이트 (i)**: 새로운 정보의 저장  
  → 새로운 정보를 적절히 추가해 학습 안정화  
- **망각 게이트 (g)**: 과거 정보를 제거  
  → 필요 없는 정보는 줄여 셀 상태가 폭발하지 않도록 제어  
- **출력 게이트 (o)**: 셀 상태를 현재 은닉 상태로 출력  
  → 필요한 정보만 은닉 상태로 전달  

> ⊕ 기호: 기울기 소실 방지  

---

## LSTM (Long Short-Term Memory)

- **셀 상태 (Cell state[C])**라는 새로운 모듈을 도입  
- 기울기 전파 시 선형가중치의 곱 이외에 덧셈 형태로 전달하여  
  기울기를 장기적으로 보존 가능  

> 각 상태에서의 기울기를 덧셈과 곱셈 경로로 분리 가능 (소실 방지)  
> → : LSTM 덧셈 경로 전달

---

## LSTM (Long Short-Term Memory)

- **셀 상태 (Cell state[C])**라는 새로운 모듈을 도입  
- 기울기 전파 시 선형가중치의 곱 이외에 덧셈 형태로 전달하여  
  기울기를 장기적으로 보존 가능  

**BUT, 정보 희석 문제의 한계**
- 망각 게이트가 과거의 정보를 조금씩 줄임  
- 누적 효과로 과거 중요한 정보가 점차 희석되어  
  오래된 정보가 점차 약화됨  

---

# 3. 긴거리 의존성: 어텐션 / ViT

---

# 3-1. 어텐션 메커니즘

## 이미지를 통해 어텐션 메커니즘 이해하기
- 픽셀 거리와 상관없이 유사한 패치가 이미지에 존재  
  - 패치(Patch): 이미지를 일정 크기로 잘라낸 작은 조각 단위

---

## 이미지를 통해 어텐션 메커니즘 이해하기
- 픽셀 거리와 상관없이 유사한 패치가 이미지에 존재  
- 관련 **높은 패치 / 낮은 패치 정보**를 최종 결정에 반영하자!  
  - 쿼리(Q): 집중하려는 대상  
  - 키(K): 다른 패치  
  - 값(V): 키 패치의 정보  
  - 어텐션(a): 쿼리와 키의 유사도 (확률 분포 형태)  
  - 출력(o): 어텐션으로 가감된 값의 총합  

```
쿼리(Q) | 키(K) | 값(V) | 임베딩
```

---

## 어텐션 메커니즘

### 이미지에서 해석
- 쿼리(Q): 질의 대상 (입력 토큰 / 입력 이미지 패치)  
- 키(K): 특성 (입력 내 기타 토큰)  
- 값(V): 키의 실제 정보  
- 어텐션(a): 쿼리와 키의 유사도 (확률 분포 형태)  
- 출력(o): 어텐션으로 가감된 값의 총합  

\[
q = xW_q, \quad k = xW_k, \quad v = xW_v
\]
\[
a_{ij} = \text{softmax}\left(\frac{q_j \cdot k_i}{\sqrt{D}}\right)
\]
\[
y_j = \sum_i a_{ij} v_i
\]

> x: 입력, W: 쿼리/키/값으로 변환하는 선형 변환  
> D: 입력의 차원, y: 출력  
> 쿼리와 키가 유사하면, 해당 값에 가중치를 높게

---

# 3-1. 자기 어텐션 (Self-Attention)

## 자기 어텐션(Self-Attention)이란
- 하나의 입력 안에서 패치 정보(쿼리/키/값)를 정의  
- 같은 이미지 내 유사도 (쿼리와 키 간 유사도) 반영  
- 입력 내부 패치 간 연결망 구축 → 클러스터링 효과  

> 클러스터링(Clustering): 서로 비슷한 특성의 패치끼리 모여 그룹 형성

---

# 3-1. 교차 어텐션 (Cross-Attention)

## 교차 어텐션(Cross-Attention)이란
- 둘 이상의 이종 데이터에서 패치 정보(쿼리/키/값)를 정의  
  - 예시: Text-To-Image 모델에서 이미지-텍스트 간 연결  
  - 쿼리(Q)는 이미지 패치, 키(K)는 텍스트 키의 정보  
- 이종 데이터 간 유사도 (쿼리와 키 간 유사도) 반영  
  - 쿼리와 키를 비교할 수 있는 공간 필요  
- 서로 다른 입력 간 연결망 구축  

> 교차 어텐션 시각화 예시: “여기”, “고양이”, “싱크대” 단어 → 이미지 내 영역과 연결

---

# 3-2. ViT 위치 인코딩

## 패치 순서(위치) 정보 제공
- 각 입력 토큰에 위치 정보를 담은 벡터를 추가  
  - 이 패치가 이미지 (2,2)에 위치함을 이해 가능

---

## 사인파 인코딩
- 데이터에 알맞게 최적화  
  - (+) 데이터 해상도가 바뀌어도 동일한 룰 적용  
  - (–) 고정이라 데이터/과업에 특화된 방식이 아님

---

## 학습 가능한 위치 인코딩
- 데이터에 알맞게 최적화  
  - (+) 데이터 해상도가 바뀌어도 동일한 룰 적용  
  - (–) 해상도가 바뀌면 다시 학습 필요  

```
패치별 위치 인코딩 → ViT 인코더 → 연결층 헤드 → 라벨 (새, 공, 자동차)
```
> 이미지 패치에 선형가중치 적용

---

# 3-2. 모델 별 특성 정리

| 모델 | 특성 요약 |
|------|------------|
| **CNN** | 지역적(작은 범위) 특성에 집중<br>장거리 맥락 이해 떨어짐<br>→ **데이터 순서 반영 불가** |
| **RNN** | 데이터 순서를 반영<br>→ **장거리 맥락 학습 어려움 (기울기 소실/폭발)** |
| **LSTM** | 데이터 순서 반영<br>장거리 맥락 이해 가능<br>→ **정보 희석 문제** |
| **Attention** | 장거리 맥락에 탁월<br>정보 희석 없음<br>→ **데이터 순서? (위치 인코딩 필요)** |

# 3-2. ViT 인코딩

---

## 패치 순서(위치) 정보 제공
- 각 입력 토큰에 위치 정보를 담은 벡터를 추가  
  - 이 패치가 이미지 (2,2)에 위치함을 이해 가능  

## 상대 위치 인코딩
- 절대 위치가 아닌, 상대적 위치를 인코딩  
  - (+) 해상도 변화에도 적용 가능  
  - (–) 절대적 위치(예: 좌우 코너) 고려가 어려움  

```
이미지 패치에 선형가중치 적용 → 패치별 위치 인코딩 → ViT 인코더 → 연결층 헤드 → 라벨(새, 공, 자동차)
```

---

# 3-2. ViT 인코더

---

## 인코더 내부 구조
- 정규화 - 다중헤드 어텐션 - 정규화 - 연결층으로 구성  
  - **정규화(Normalization)**: 입력을 균일하게 맞춤  
  - **다중헤드 어텐션(Multi-Head Attention)**: 여러 관점에서 패치 간 관계를 학습  
    (예: 어떤 패치는 형태, 다른 패치는 색, 또 다른 패치는 위치)  
  - **연결층(MLP)**: 어텐션 정보를 조합·변환해 더 풍부한 특징 생성  

- 인코더 L개 통과 (일반적으로 12개 이상)

---

## 전역 관계 학습
- CNN: 지역적 영역에서 수용영역을 넓혀 전역 이해  
- ViT: 수용영역을 넓히지 않아도 전역 맥락 이해 가능  

---

# 3-2. ViT

---

## 이미지를 이미지 토큰 집합으로 변환
- 위치 인코딩으로 패치 토큰의 순서를 학습에 반영  

## ViT 인코더
- 정규화 - 다중헤드 어텐션  
  - 정규화 - 연결층으로 구성  

## 최종 출력
- MLP head - 최종 예측 수행  

---

# 3-2. ViT vs ResNet

---

## ViT가 늘 CNN 모델보다 좋을까? ❌  
- ImageNet의 경우, **ResNet 성능이 더 우수!**  
- ImageNet-21K에서 사전 학습하고 ImageNet 미세조정한 경우, ViT도 우수  
  → (ViT-L/16이 큰 ResNet 수준)

---

## ViT는 거대 데이터셋 사전학습 시 우수  
- JFT-300M은 300M개의 표기된 데이터셋 (거대)  
- JFT 사전학습, 이후 ImageNet 미세조정하는 경우  
  → ViT 성능이 확실한 개선을 보이기 시작  

```
B = Base
L = Large
H = Huge
/32, /16, /14 : 패치 사이즈 의미 (작을수록 큰 모델)
```

---

# 3-2. ViT 학습 시 유의점

---

## ViT 학습 기술이 중요!
- 막대한 상수를 보유  
- 정규화 기법 / 데이터 증강 기술이 매우 중요!  

(ResNet 변종, 데이터 증강 강화 별 정확도 비교표 포함)

---

## 증류학습 기술이 효과적 (DeiT 2021)
- 상당한 개선 효과 확인  
(ViT-B/16 @ ImageNet)

- 원본 ViT-B/16: 약 78%
- 증류 적용 시: 약 84%  

출처: *Training data-efficient image transformers & distillation through attention, ICML 2021.*

---

## 증류학습 기술이 효과적 (DeiT 2021)
- **증류학습 방식**

**1단계:**  
“선생님 모델”을 이미지-레이블 활용해서 학습  
→ P(고양이)=0.9, P(개)=0.1  
→ 크로스 엔트로피 손실 (Cross Entropy Loss)

**2단계:**  
“학생 모델”은 "선생님 모델"의 예측을 따라가도록 학습  
→ KL 발산 손실 (KL Divergence Loss)

---

## 증류학습 기술이 효과적 (DeiT 2021)
- **증류학습 방식**

**1단계:**  
“선생님 모델”을 이미지-레이블 활용해서 학습

**2단계:**  
“학생 모델”은 "선생님 모델"의 예측을 따라가도록 학습  
(정답도 함께 배우도록 학습)

---

## 증류학습 기술이 효과적 (DeiT 2021)
- **증류학습 방식**

**1단계:**  
“선생님 CNN 모델”을 이미지-레이블 활용해서 학습

**2단계:**  
“학생 ViT 모델”은 "선생님 CNN 모델"의 예측을 따라가도록 학습  
(정답도 함께 배우도록 학습)

---

## 증류학습 기술이 효과적 (DeiT 2021)
- **증류학습 방식**

학생 ViT 모델은 학습 가능한 토큰을 추가로 입력받아,  
선생님 CNN의 **logit 출력과 동일하게 예측하도록 학습됨!**

---

## 증류학습 기술이 효과적 (DeiT 2021)
- **증류학습 기술이 효과적 (DeiT 2021)**  
- **상당한 개선 효과 확인**

(그래프: 원본 ViT-B/16 대비 증류 후 Top1 정확도 향상)

---

## 증류학습 기술이 효과적 (DeiT 2021)
- 상당한 개선 효과 확인  
- ViT-B/16 @ ImageNet

| 모델 | 설명 | Top1 정확도 |
|------|------|--------------|
| 원본 ViT-B/16 | 기본 | 약 78% |
| + 증류 | Teacher-Student 방식 추가 | 약 82% |
| + 길게 학습 | 300 → 1000 epochs | 약 84% |
| + 고해상도 입력 | 224×224 → 384×384 | 약 85% |

> 출처: *Training data-efficient image transformers & distillation through attention, ICML 2021.*

---

# 3-2. Vision Transformer의 트렌드

---

## 다양한 변종이 존재

- CNN처럼 이미지의 **계층적 이해를 활용**할 수 없을까?

### Swin Transformer
- 이미지 전역에 직접 어텐션 수행하는 것은 너무 고비용, 비효율  
- 따라서 **윈도우(Window) 영역**을 지정하여 그 내부의 어텐션에 집중  
  1) 윈도우 영역 지정, 내부 어텐션 집중  
  2) 윈도우를 비껴가게 한 번 더 설정  
  → (1)과 (2)가 중첩되면서 최종적으로 이미지 전역을 고려하는 효과!

**Block L:** 내부 윈도우  
**Block L+1:** 비껴간 윈도우  

> Swin 계열은 특히 **검출/분할과 같은 화소별·공간 이해에서 좋은 성능**을 보임.

출처: *Liu et al., “Swin Transformer: Hierarchical Vision Transformer using Shifted Windows”, CVPR 2021.*

---

## Vision 백본 vs 데이터 확장성 vs 멀티모달 모델

### ✅ Vision 백본
- **ViT-22B** (220억 상수, 구글 리서치)  
  - 지도학습 채택: 대규모 비공개 레이블 데이터 기반 (JFT-3B)  
  - 일반화 성능이 우수 / 모델 상세 비공개 (재현 불가)

### ✅ 자기지도 Vision 백본
- **DINOv2** (최대 10억 상수, 메타 AI)  
  - 공개 웹 데이터 기반 **14억장 이미지** 활용  
  - 비지도 학습 채택: 자기지도 학습 + 대조학습/증류  
  - 공개 모델, 현재 가장 많이 활용되는 백본

출처: *Oquab et al., “DINOv2: Learning Robust Visual Features without Supervision”, TMLR 2024.*

---

# 3-2. Vision Transformer의 트렌드 (2)

---

## Vision 백본 vs 데이터 확장성 vs 멀티모달 모델

### ✅ 비전-언어 모델
- **EVA-CLIP** (상하이 AI)  
  - 지도학습 채택: 이미지-텍스트 대조 학습 + 마스킹 기반 사전학습 결합 (LAION)  
  - 제로샷 분류 및 멀티모달 검색 강력 / 모델 상수 공개  

### ✅ 범용 시각 모델
- **InternImage-H** (상하이 AI Lab)  
  - 대규모 지도 학습 채택  
  - Conv + Transformer 하이브리드 백본  
  - 탐지/분할에서 최고 수준 성능 달성  
  - 모델/코드 공개

---

# 3-2. ViT 장단점

---

## ✅ 장점
- 전역 문맥을 한 번에 고려 가능  
- 시계열/시퀀스 데이터의 경우, **순서 고려 가능 (위치 인코딩)**  
  - 순서 없는 데이터의 경우, 선택적으로 제거도 가능  

---

## ⚠️ 단점
- 대규모 학습 자원 필요  
  - 대규모 데이터 및 GPU 자원 요구 (메모리)  
- 학습 데이터 규모가 작을 경우, CNN보다 성능 저하  
- 한 번에 고려할 수 있는 토큰 수 제한적  

---

## 💡 활용
- 이미지 분류 / 탐지 / 분할 / 생성 모두에서 **SoTA 성능 달성**  
- 멀티모달 모델의 **기준 아키텍처**로 활용  
- 다양한 **기존 모델의 백본(backbone)** 역할 수행 중  

---

# 확인문제

---

## ① CNN 구조의 장점이 아닌 것은?

1️⃣ 지역적인 특징 학습에 특화되어 작은 필터로 패턴이나 경계선을 잘 포착한다.  
2️⃣ 파라미터 효율성이 뛰어나 이미지 블록별 가중치를 공유함으로써 연산량을 줄인다.  
3️⃣ Pooling과 Stride 연산으로 인해 위치 변화와 노이즈에도 견고하다.  
4️⃣ 이미지 분류, 탐지 등 다양한 이미지 기반 과업에서 표준 모델로 활용된다.  
5️⃣ **텍스트나 음성과 같은 순차적 데이터의 순서 정보를 효과적으로 반영한다.**

---

## ✅ 정답 및 해설

- **정답:** ⑤  
- **해설:**  
  CNN은 이미지 처리에 최적화된 구조로,  
  지역적 특징 학습과 파라미터 효율성, 노이즈 견고함 등의 장점을 가진다.  
  그러나 텍스트나 음성과 같은 **순차적(Sequential) 데이터의 시간·순서 의존성**을 반영하는 데는 한계가 있다.  
  이러한 데이터에는 주로 **RNN(Recurrent Neural Network)**이나 **Transformer 계열 모델**이 적합하다.

---

# 확인문제

---

## ② 다음 중 RNN의 특징으로 옳지 않은 것은?

1️⃣ 은닉 상태(hidden state)를 통해 이전 시점의 정보를 다음 시점으로 전달한다.  
2️⃣ 순차적 데이터를 처리하는 데 적합하다.  
3️⃣ 입력 데이터의 시계열 의존성을 반영할 수 있다.  
4️⃣ **병렬 연산이 쉬워 대규모 데이터 처리에서 속도가 빠르다.**  
5️⃣ 언어 모델링, 기계 번역, 음성 인식 등에 활용된다.  

---

## ✅ 정답 및 해설

- **정답:** ④  
- **해설:**  
  RNN은 입력 데이터를 **순차적으로 처리**해야 하므로 병렬 연산이 어렵고,  
  대규모 데이터 학습 시 속도가 느려지는 단점이 있다.  
  이 문제는 **Transformer 계열 모델**의 등장으로 크게 개선되었다.

# 0. 학습 시작(오버뷰)

---

## 같은 모델 구조로 다른 성능이 나오는 이유는 무엇일까?
- 좋은 구조를 선택해도 **학습 실패 가능**
- **안정적 학습을 위한 핵심 기법**이 중요

---

## 학습 레시피? 재료손질, 양념 그리고 손맛!
- **학습 전:** 구조 이외에 결정이 필요한 모델 구성 요소 / 하이퍼 상수 선택  
  - *하이퍼 상수:* 학습을 시작하기 전에 사람이 직접 설정해야 하는 값
- **학습 과정:** 주요 학습 전략

---

# 확인문제 ③

---

## 다음 중 빈칸에 들어갈 단어로 알맞은 것을 모두 고르시오.

RNN의 대표적인 한계는 **______ 문제와 장기 의존성(Long-term Dependency)** 학습의 어려움이다.

| 보기 | 선택지 |
|------|----------|
| A | 기울기 소실 |
| B | 기울기 폭발 |
| C | 과적합 |
| D | 병렬화 어려움 |

---

### ✅ 정답 및 해설
- **정답:** A, B  

**해설:**  
RNN은 시간 역전파(BPTT) 시 동일 가중치가 반복 곱해지면서 **기울기 소실/폭발 문제**가 발생한다.  

- **기울기 소실:** 시간이 멀어질수록 기울기가 0에 가까워져 장기 의존성 학습이 어려움  
- **기울기 폭발:** 기울기가 비정상적으로 커져 학습이 불안정  
- **과적합:** 모든 머신러닝 모델에 공통된 일반적 문제  
- **병렬화 어려움:** 순차 처리 구조 특성상 GPU 병렬화가 어려우나, 대표적 한계로는 거리가 멀다.

---

# 확인문제 ④

---

## 다음 중 Vision Transformer(ViT)의 특징에 대한 설명으로 옳지 않은 것은?

1️⃣ 패치를 나눈 후 위치 인코딩을 통해 순서(위치) 정보를 추가한다.  
2️⃣ 인코더는 정규화, 다중헤드 어텐션, 연결층(MLP)으로 반복적으로 구성된다.  
3️⃣ CNN과 달리 수용영역을 넓히지 않아도 전역 맥락을 학습할 수 있다.  
4️⃣ **데이터가 작을 때도 CNN보다 항상 높은 성능을 보장한다.**  
5️⃣ 대규모 학습 데이터와 GPU 자원이 필요하다.  

---

### ✅ 정답 및 해설
- **정답:** ④  

**해설:**  
ViT는 **대규모 학습 데이터**가 있을 때 강력한 성능을 발휘하지만,  
데이터가 적은 경우에는 오히려 **CNN보다 성능이 떨어질 수 있다.**  
따라서 “항상 높은 성능을 보장한다”는 설명은 잘못이다.

---

# 강의 정리

---

## 오늘 공부한 내용 — 요약 및 정리

---

### ✅ CNN의 강점과 한계
- **강점:** 지역적 특징 추출에 뛰어남  
- **한계:** 긴 거리 의존성과 픽셀 단위 복원에서 한계 존재  

---

### ✅ RNN의 역할과 발전
- **순차 데이터 처리 가능**
- **기울기 소실/폭발 문제** 존재 → 이를 **LSTM 구조**로 보완  

---

### ✅ Attention과 ViT의 혁신
- **Attention:** 전역적 맥락(Global Context)을 고려  
- **ViT(Vision Transformer):** 이미지 과업에서 새로운 패러다임 제시  

---

# 딥러닝 및 이미지 모델
### “3차시: 이미지 모델 학습 전략”

---

# 📑 CONTENTS

---

## 1. 학습 전략의 중요성
1. 학습 불안정, 과적합, 수렴 속도 문제  
2. 실제 적용에서는 **일반화 성능과 효율성**이 핵심 → “훈련 전략” 필요  

---

## 2. 모델 구성: 악마는 디테일에 있다
1. 데이터 전처리 / 활성화 함수 선택  
2. 가중치 초기화 전략 / 정규화 중요성  

---

## 3. 학습 과정
1. 학습 비율 조정 / 스케줄링  
2. 하이퍼 상수 최적화 / 데이터 증강  

---

# 학습 목표

---

- **딥러닝 모델 학습의 안정성, 효율성, 일반화 성능을 높이기 위해**  
  모델 구조의 개선 못지않게 학습 방법론이 중요함을 이해한다.  

- **모델 학습 전·과정에 필요한 핵심 전략을 습득**한다.

# 1. 학습 전략의 중요성

---

## 1-1. 모델 구조

### ▪ 지난 1차/2차 강의의 내용 요약
- CNN 구조와 발전 (AlexNet → ResNet → MobileNet)  
- CNN의 한계 → RNN, Transformer (ViT)

### ▪ CNN의 구성
- 합성곱 레이어  
- 풀링  
- 비선형 함수  
- 정규화  
- 연결층 레이어  

---

## 1-2. 좋은 모델 구조 = 우수한 성능?

### ▪ 좋은 구조만으로는 성능 보장 불가!
- 동일한 구조, 동일한 데이터셋을 사용해도 성능 차이가 큼  
- 이유: 학습 과정에서 발생하는 불안정성, 과적합, 수렴 속도 문제  

### ▪ 학습에서 자주 겪는 문제들
- **학습 불안정:** 손실 폭발, 학습 멈춤, 수렴 실패  
  - 손실 폭발: 학습 과정에서 손실 값이 급격히 커져 모델이 수렴하지 못하는 현상  
  - 학습 멈춤: 손실 값이 더 이상 줄지 않고 학습이 정체되는 상황  
  - 수렴 실패: 최적의 값에 도달하지 못하고 학습이 무의미하게 끝나는 현상  
- **과적합:** 훈련 데이터에는 맞지만 검증·실전 성능 저하  
  - 원인: 데이터 부족, 모델 복잡도 과다, 정규화 부족 등  
- **느린 수렴:** 최적점에 도달하기까지 불필요하게 많은 epoch 소모로 인한 학습 효율 저하  

---

## 1-3. 학습 전략

### ▪ 본 강의의 해결 방향
- 구조 설계만큼 중요한 훈련 전략 제시  
- 세부 방법 (예시)  
  - 학습률 스케줄링, 정규화, 초기화, 데이터 전처리 등  

### ▪ 적용 시 기대 효과
- 일반화 성능 + 학습 효율성 확보  

---

> **좋은 구조만으로는 충분하지 않다 → 훈련 전략이 필요하다**

---

# 2. 모델 구성: 악마는 디테일에 있다

---

## 2-1. 활성화 함수

### ▪ 정의
- 입력 신호의 총합을 출력 신호로 변환하는 함수  
  - 예시: 뉴런이 ‘켜질지 / 얼마나 반응할지’를 결정하는 스위치  

---

## ▪ 역할
- 신경망에 비선형성 부여 → 복잡한 패턴 학습 가능  
  - 단순 패턴은 선형 분류가 가능하지만, 복잡한 패턴은 직선만으로 분류 불가능  
  - 활성화 함수를 통해 비선형성을 부여함으로써 복잡한 패턴 학습 가능  

---

## ▪ 활성화 함수 선택

### ▪ 역할
- 신경망에 비선형성 부여 → 복잡한 패턴 학습 가능  
- 활성화 함수가 없다면 단순 선형모델과 동일  
- 단, 활성화 함수의 특성에 따라 학습 안정성과 성능이 크게 좌우됨  
  - 활성화 함수 종류: Sigmoid, Tanh, ReLU, Leaky ReLU 등  

# 대표 활성화 함수 : Sigmoid

---

## Sigmoid
- 0~1 사이 값으로 출력 값을 제어 → 확률 값처럼 해석 가능
  - 분류 문제(특히 이진 분류)에서 “이 클래스일 확률”로 활용 가능
  - 이진 분류 예시: 합격/불합격, 참/거짓

### ▪ 함수  
\( a(x) = \frac{1}{1 + e^{-x}} \)  
- 출력은 항상 0~1 사이  

## 문제①
- \( a(x) = \frac{1}{1 + e^{-x}} \)
- Sigmoid 함수는 입력이 매우 크거나 매우 작으면 출력이 거의 0이나 1에 고정  
  → 더 이상 역전파 전달 X  
  - 역전파: 정답과 예측의 차이(오답률)를 모델에 전달하며 가중치를 수정하는 과정  
  - 출력이 0이나 1인 구간에서 역전파 신호가 0이 되어 신경망이 더 이상 학습을 못함  

## 문제②
- \( a(x) = \frac{1}{1 + e^{-x}} \)
- Sigmoid 출력 범위는 항상 양수  
  → 학습 과정에서 편향된 업데이트 발생  
  - 기울기의 평균이 한쪽(양수)으로 치우쳐 학습 비효율 발생  

## 문제③
- 계산식에 지수 함수(exp)가 들어감  
  → ReLU처럼 단순한 max 연산보다 계산 비용이 더 큼  

### ▪ Sigmoid 정의식  
\( a(x) = \frac{1}{1 + e^{-x}} \)

### ▪ ReLU 정의식  
\( f(x) = \max(0, x) \)

---

# 대표 활성화 함수 : Tanh

---

## 출력 범위
- [-1, 1]

## 역할
- 데이터가 중심(0)을 기준으로 대칭 → 학습 안정성 ↑  
  - 음수/양수 모두 표현 가능 → Sigmoid보다 효과적  

## 문제①
- 기울기 소실 문제 여전 → 학습 정체  
  - 출력이 -1이나 1에 가까워져 신경망이 더 이상 학습 불가  

## 문제②
- exp 연산 포함 → 계산 비용이 큼  

### ▪ Tanh 정의식  
\( \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \)

---

# 대표 활성화 함수 : ReLU

---

## ReLU (Rectified Linear Unit)
\( f(x) = \max(0, x) \)

### ▪ 장점
- 양의 영역에서 포화되지 않음 → Sigmoid/tanh처럼 기울기 소실 문제 크게 줄어듦  
  - 입력이 음수면 0, 양수면 그대로 출력함  
- 계산 효율성 높음 → 단순 max(0, x) 연산, 매우 빠름  
- 학습 속도 빠름 → 실제로 Sigmoid, tanh 대비 6배 이상 빠른 수렴 보고됨  

## 문제점
- 죽은 뉴런 문제 → 입력이 계속 음수면 뉴런이 0만 출력 → 영구적으로 “죽은 뉴런” 발생  
  - 죽은 뉴런은 가중치 업데이트(학습)를 멈춤  
- ‘0’을 기준으로 비대칭  

---

# 대표 활성화 함수 : Leaky ReLU

---

## Leaky ReLU
\( f(x) = \max(0.01x, x) \)

### ▪ 장점
- ReLU의 모든 장점 수용  
- 죽지 않는 뉴런 (Dead ReLU 문제 해결)  
  → 음수 입력에도 작은 기울기를 부여해 뉴런이 완전히 죽지 않음  

## 문제점
- 음수 영역 기울기 값(0.01) 설정의 임의성  
  - 너무 작으면 여전히 기울기 소실, 너무 크면 출력 왜곡 가능  
- ‘0’ 중심 대칭이 아님  
  - 신호가 한쪽(양수)으로 치우치는 경향 발생 가능  
- 항상 ReLU보다 우세하지 않음  
  - 일부 문제에서는 성능 차이가 거의 없음  

# 대표 활성화 함수 : ELU

---

## Exponential Linear Unit (ELU)
\[
f(x) = 
\begin{cases}
x & \text{if } x > 0 \\
\alpha(\exp(x) - 1) & \text{if } x \le 0
\end{cases}
\]

---

### ▪ 장점
- ReLU의 모든 장점 수용!  
- 평균 출력이 0 근처에 위치  
- 음수 영역에서의 포화 구간 제공 → 기울기 소실은 막고, 왜곡 문제도 해결  

---

## Exponential Linear Unit (ELU)
\[
f(x) = 
\begin{cases}
x & \text{if } x > 0 \\
\alpha(\exp(x) - 1) & \text{if } x \le 0
\end{cases}
\]

---

### ▪ 문제점
- 계산 복잡성 증가  
- 하이퍼파라미터 α 설정 필요  
  → 음수 영역 기울기의 크기를 결정하는 α 값에 따라 성능 차이 발생  

---

# 다양한 활성화 함수

---

## 다양한 활성화 함수

| 함수 | 수식 |
|------|------|
| Sigmoid | \( \sigma(x) = \frac{1}{1+e^{-x}} \) |
| tanh | \( \tanh(x) \) |
| ReLU | \( \max(0, x) \) |
| Maxout | \( \max(w_1^T x + b_1, w_2^T x + b_2) \) |
| Leaky ReLU | \( \max(0.1x, x) \) |
| ELU | \( \begin{cases} x & x \ge 0 \\ \alpha(e^x - 1) & x < 0 \end{cases} \) |
| SELU | \( \begin{cases} \lambda x & x < 0 \\ \lambda(\alpha e^x - \alpha) & \text{otherwise} \end{cases} \) |

---

# 결론은?

---

## ▪ 큰 고민 말고, 일단 ReLU부터 시도!
\[
\text{ReLU: } \max(0, x)
\]

- 성능 경쟁 중이라면, ReLU의 개선판을 시도  

| 함수 | 수식 |
|------|------|
| ReLU | \( \max(0, x) \) |
| Leaky ReLU | \( \max(0.1x, x) \) |
| ELU | \( \begin{cases} x & x \ge 0 \\ \alpha(e^x - 1) & x < 0 \end{cases} \) |
| SELU | \( \begin{cases} \lambda x & x < 0 \\ \lambda(\alpha e^x - \alpha) & \text{otherwise} \end{cases} \) |

- Sigmoid나 Tanh는 전문가 영역!  

| 함수 | 수식 |
|------|------|
| Sigmoid | \( \sigma(x) = \frac{1}{1 + e^{-x}} \) |
| tanh | \( \tanh(x) \) |

---

# 2.2. 데이터 전처리

---

# 데이터 형식 통일

---

## ▪ 이미지 조건을 일치
- 학습/검증/테스트에 모두 동일하게 적용  
  - 이미지 해상도  
  - 색상 (예: RGB or BGR or Grey)  
  - 밝기  
  - 정규화  

---

## ▪ 일반적 정규화 방식
- 평균 0, 표준편차 1로 정규화 — 중요!!!

---

# 데이터 형식 통일

---

## ▪ 이미지 조건을 일치

### 색상 채널 통일 단계 예시  
3-channel 이미지 → 모델 입력 크기 고정  

```python
basic_rgb_pipeline = transforms.Compose([
    transforms.Resize(256, interpolation=InterpolationMode.BICUBIC),  # 크기 통일
    transforms.CenterCrop(224),  # 센터 크롭 (선택)
    transforms.ToTensor(),  # 텐서화
    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)  # 정규화
])
```

Grayscale 이미지 → 모델 호환 위해 3-channel로 복제  

```python
grayscale_pipeline = transforms.Compose([
    transforms.Resize(256, interpolation=InterpolationMode.BICUBIC),
    transforms.CenterCrop(224),
    transforms.Grayscale(num_output_channels=3),  # (H,W) -> (3,H,W)
    transforms.ToTensor(),
    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)
])
```

# 모델별 전처리 방식 예시

---

## ▪ AlexNet
- **평균 이미지를 빼기**
  - 학습 데이터 전체의 평균 이미지를 입력 이미지에서 이를 빼줌
  - 평균 이미지는 크기가 [32, 32, 3] 배열 형태

---

## ▪ VGG
- **채널별 평균 빼기**
  - R, G, B 각 채널의 평균 값을 계산, 입력에서 각 채널별 평균값을 빼줌

---

## ▪ ResNet
- **평균 빼기 + 채널별 표준편차로 나누기**
  - R, G, B 각 채널의 평균을 빼고, 표준편차로 나눔

---

# 2.3. 모델 상수 초기화

---

# 아이디어1: 0으로 초기화

---

## ▪ 모든 가중치(W)와 편향(b)을 0으로 초기화하면 무슨 일이 생길까?

- 모든 출력은 0  
  \[
  y = W \cdot x + b
  \]
- 뉴런 별 출력 \( y \) 동일, 기울기 \( \frac{\partial L}{\partial y} \) 동일  
  → 학습이 이뤄지지 않는다  

---

# 아이디어2: 임의(랜덤) 초기화

---

## ▪ 랜덤 초기화는?
- 작은 랜덤 숫자 (0.01 곱 → 0.01 분산): Gaussian 분포를 따름, 0을 중심으로  

### ▪ Gaussian 분포
- 종 모양의 정규 분포로, 평균 0 근처 값이 가장 자주 발생  
  → 값이 너무 크거나 작아지는 걸 막을 수 있음  

```python
W = 0.01 * np.random.randn(Din, Dout)
```

---

## ▪ 깊지 않은 모델에서는 동작하는 전략!

- \( W = 0.01 * np.random.randn(Din, Dout) \)
- 작은 랜덤 값으로 초기화하면 적절히 동작  
- 그러나 깊은 모델에서는 다른 문제가 발생  

---

## ▪ 깊은 모델은?

- \( W, b \)가 작은 수(<1)에 분포함  
  → 역전파 시 기울기를 구하는 과정에서 W를 계속 곱하면서 **기울기가 0으로 수렴**
- 반대로 상수가 크면(>1) **기울기는 폭발**

---

# 아이디어3: 자비에 초기화

---

## ▪ 자비에(Xavier) 초기화
- 가중치 초기 분포의 분산을 입력 차원으로 맞춤  
  \[
  \text{std} = \frac{1}{\sqrt{Din}}
  \]
  ```python
  W = np.random.randn(Din, Dout) / np.sqrt(Din)
  ```

---

## ▪ 분산을 입력 차원(뉴런 수) 증가를 고려하여 설계
- 모델 상수 분산이 입력 차원과 같으면, 선형연산 이후 출력의 분산 ≈ 입력의 분산  

```python
W = np.random.randn(Din, Dout)/np.sqrt(Din)
x = np.tanh(x.dot(W))
```

---

# 아이디어4: 허 초기화

---

## ▪ 자비에 초기화 + ReLU의 궁합?
- 가중치 초기 분포의 분산을 입력 차원으로 맞춤  
  \[
  \text{std} = \frac{1}{\sqrt{Din}}
  \]
  ```python
  W = np.random.randn(Din, Dout)/np.sqrt(Din)
  x = np.maximum(0, x.dot(W))
  ```

---

### ▪ 자비에에서는 입력력 분산을 맞추는 전략은 입력력이 대칭적 분포를 보인다는 가정 하에서 설계
- ReLU는 양의 분포만 남김 → **비대칭 분포**
- 입력 분포의 절반만 활성화 → 분산 불균형 발생  

# 모델별 전처리 방식 예시

---

## ▪ AlexNet
- **평균 이미지를 빼기**
  - 학습 데이터 전체의 평균 이미지를 입력 이미지에서 이를 빼줌
  - 평균 이미지는 크기가 [32, 32, 3] 배열 형태

---

## ▪ VGG
- **채널별 평균 빼기**
  - R, G, B 각 채널의 평균 값을 계산, 입력에서 각 채널별 평균값을 빼줌

---

## ▪ ResNet
- **평균 빼기 + 채널별 표준편차로 나누기**
  - R, G, B 각 채널의 평균을 빼고, 표준편차로 나눔

---

# 2.3. 모델 상수 초기화

---

# 아이디어1: 0으로 초기화

---

## ▪ 모든 가중치(W)와 편향(b)을 0으로 초기화하면 무슨 일이 생길까?

- 모든 출력은 0  
  \[
  y = W \cdot x + b
  \]
- 뉴런 별 출력 \( y \) 동일, 기울기 \( \frac{\partial L}{\partial y} \) 동일  
  → 학습이 이뤄지지 않는다  

---

# 아이디어2: 임의(랜덤) 초기화

---

## ▪ 랜덤 초기화는?
- 작은 랜덤 숫자 (0.01 곱 → 0.01 분산): Gaussian 분포를 따름, 0을 중심으로  

### ▪ Gaussian 분포
- 종 모양의 정규 분포로, 평균 0 근처 값이 가장 자주 발생  
  → 값이 너무 크거나 작아지는 걸 막을 수 있음  

```python
W = 0.01 * np.random.randn(Din, Dout)
```

---

## ▪ 깊지 않은 모델에서는 동작하는 전략!

- \( W = 0.01 * np.random.randn(Din, Dout) \)
- 작은 랜덤 값으로 초기화하면 적절히 동작  
- 그러나 깊은 모델에서는 다른 문제가 발생  

---

## ▪ 깊은 모델은?

- \( W, b \)가 작은 수(<1)에 분포함  
  → 역전파 시 기울기를 구하는 과정에서 W를 계속 곱하면서 **기울기가 0으로 수렴**
- 반대로 상수가 크면(>1) **기울기는 폭발**

---

# 아이디어3: 자비에 초기화

---

## ▪ 자비에(Xavier) 초기화
- 가중치 초기 분포의 분산을 입력 차원으로 맞춤  
  \[
  \text{std} = \frac{1}{\sqrt{Din}}
  \]
  ```python
  W = np.random.randn(Din, Dout) / np.sqrt(Din)
  ```

---

## ▪ 분산을 입력 차원(뉴런 수) 증가를 고려하여 설계
- 모델 상수 분산이 입력 차원과 같으면, 선형연산 이후 출력의 분산 ≈ 입력의 분산  

```python
W = np.random.randn(Din, Dout)/np.sqrt(Din)
x = np.tanh(x.dot(W))
```

---

# 아이디어4: 허 초기화

---

## ▪ 자비에 초기화 + ReLU의 궁합?
- 가중치 초기 분포의 분산을 입력 차원으로 맞춤  
  \[
  \text{std} = \frac{1}{\sqrt{Din}}
  \]
  ```python
  W = np.random.randn(Din, Dout)/np.sqrt(Din)
  x = np.maximum(0, x.dot(W))
  ```

---

### ▪ 자비에에서는 입력력 분산을 맞추는 전략은 입력력이 대칭적 분포를 보인다는 가정 하에서 설계
- ReLU는 양의 분포만 남김 → **비대칭 분포**
- 입력 분포의 절반만 활성화 → 분산 불균형 발생  

---

## ▪ He 초기화
- 가중치 초기 분포의 분산을 **2 × 입력 차원**으로 맞춤  
  \[
  \text{std} = \sqrt{\frac{2}{Din}}
  \]
  ```python
  W = np.random.randn(Din, Dout) * np.sqrt(2) / np.sqrt(Din)
  x = np.maximum(0, x.dot(W))
  ```

- **ReLU 비대칭성**을 고려하여, 유사 분산이 레이어 후반에도 유지됨  

---

# RestNet의 등장 배경

---

## ▪ RestNet (Residual Network)
- **잔차 연결(skip connection)** 을 사용하여 매우 깊은 신경망도 안정적으로 학습할 수 있는 구조  
  - 입력을 출력에 더해주는 “지름길(skip)”로, 네트워크가 전체를 새로 배우지 않고 **변화량만 학습**  

---

## ▪ 깊은 모델의 문제점
- 깊어질수록 **기울기 소실 / 폭발 문제** 발생 → 학습 어려움  
- RestNet은 이를 완화하는 구조로 설계됨  

---

# 주의사항: ResNet

---

## ▪ ResNet 철학 — 전층의 기울기 전달
- Skip connection으로 입력이 그대로 더해져, 깊은 층에서도 **기울기가 끊기지 않음**
- 그러나 **초기화가 잘못되면**, 후반 층의 출력 분포값이 계속 증폭 가능  
  → 안정적 학습이 어려워짐  

---

# 주의사항: ResNet (2)

---

## ▪ ResNet 철학 — 전층의 기울기 전달
- ReLU를 사용하는 구조이므로 **He 초기화** 사용  
- 분산 유지 조건:
  \[
  Var(x + F(x)) = Var(x)
  \]
  → 안정적인 학습을 유도  

- 두 번째 conv는 **0으로 초기화**  
- 첫 번째 conv는 **He 초기화**

---

# 주의사항: ResNet (3)

---

- 사실상 초반에는 skip에만 의존하여 안정적 학습 유도  
- 초기 입력의 분포를 그대로 유지해야 학습이 안정적으로 시작됨  

```python
첫 번째 conv → He 초기화
두 번째 conv → 0으로 초기화
```

---

# 2-4. 모델 정규화

---

# 데이터 처리 / 초기화 이후 학습 가능

---

## ▪ 학습 과정에서 드디어 오차 감소!
- Train loss ↓, Accuracy ↑  

그러나...  
### 검증 오차(val loss)가 오히려 오른다면?
→ **과적합(Overfitting)** 의 신호  

---

## ▪ 정규화를 통해 검증 오차를 줄이자!
- **정규화(Regularization)**: 모델 복잡도를 제어하여 훈련과 검증 성능의 차이를 줄이는 방법  
- **검증 오차**: 모델이 학습하지 않은 새로운 데이터에서 보이는 실제 성능 지표  

---

# 아이디어1: 가중치 감소 (Weight Decay)

---

## ▪ 목적
- 학습 과정에서 특정 가중치가 지나치게 커지는 현상 방지  

### ▪ 원인
- 가중치가 너무 크면 입력에 과도하게 반응 → 훈련 데이터에 과적합  

---

## ▪ 방법
- 가중치 크기에 비례한 패널티 적용  
- “큰 가중치는 과적합을 부른다 → 줄여주자”  

---

# 아이디어1: 가중치 감소 (2)

---

## ▪ 만약 가중치 감소만으로 완벽히 만족한다면?
- 모든 가중치가 0인 모델 → 아무것도 학습하지 않음  
- 따라서 **원본 손실(예: Cross Entropy 등)** 과 함께 사용해야 의미가 있음  

> 균형이 핵심:  
> 훈련 오차도 줄이면서 + 가중치도 적당히 작게 유지  

---

# 아이디어1: 가중치 감소 (3)

---

## ▪ L2 정규화 (Ridge, Weight Decay)
- 큰 가중치에 제곱으로 패널티 → 극단적 큰 값 방지  
- 결과: **모든 가중치가 골고루 작아짐**  
  \[
  R(W) = \sum_k \sum_l W_{k,l}^2
  \]

---

## ▪ L1 정규화 (Lasso)
- 모든 가중치에 **절댓값 크기만큼 동일한 패널티** 부여  
- 결과: **중요하지 않은 가중치는 완전히 0이 됨 → 희소한 모델**  
- 장점: 자동으로 중요한 특성만 선택 → 단순한 모델  
  \[
  R(W) = \sum_k \sum_l |W_{k,l}|
  \]

---

- **L1 정규화**는 가능한 한 더 작은 모델을 선호하는 경우 선택  
  (즉, 특성의 희소성을 유도하고 싶을 때)

# 아이디어1: 가중치 감소

---

## Elastic net (L1 + L2)

- 변수가 많고 상관관계가 높을 때 특히 효과적
- L1과 L2의 장점을 모두 활용: 특성 선택 + 안정적인 학습

\[
R(W) = \sum_k \sum_l \beta W_{k,l}^2 + |W_{k,l}|
\]

### 작동 방식
- β 파라미터로 L1과 L2의 비중 조절
- β가 크면 → L2 효과 강함 (모든 가중치 작게)
- β가 작으면 → L1 효과 강함 (희소한 모델)

---

# 아이디어2: 드롭아웃

---

## 개념
- 학습 과정에서 일부 뉴런을 확률적으로 끔(0으로 설정)
- 매 학습 스텝마다 다른 네트워크 구조가 샘플링되는 효과
- 직관: “무작위로 뉴런을 비워서 특정 뉴런/특징에 과도하게 의존하지 않게 함”

---

## 장점
- 과적합 방지 → 네트워크가 특정 패턴에 의존하지 않고 일반화 성능 ↑  
  - 훈련 시 매번 다른 뉴런 조합으로 학습하므로 특정 뉴런에 과도하게 의존하지 않음
- 여러 작은 모델을 합친 것과 비슷한 효과 (성능 개선을 위한 앙상블 방식과 유사)  
  - 앙상블: 여러 개의 서로 다른 모델을 학습시킨 후, 예측할 때 모든 모델의 결과를 평균내거나 투표하여 최종 결정
- 간단하면서도 매우 강력

---

## 단점
- 학습시간 증가
- 최적의 드롭 비율을 찾는 것이 중요
- 모델에 따라 비선호 되기도 함  
  - RNN/LSTM 등 시계열 데이터에서는 변형된 기법 필요  
  - 배치 정규화 진행된 모델에서는 비선호 (이미 강력한 정규화가 적용)

---

## 실행 단계에서 동작
- 학습 중에는 뉴런을 확률적으로 끔.  
  - 예: 드롭아웃 확률 p=0.5 (뉴런을 끌 확률) → 뉴런 절반만 살아있음 (1-0.5=0.5)
  - 실질적인 출력값이 줄어듦. (예: 평균적으로 절반 출력)

### 문제점
- 테스트 시에는 뉴런을 전부 사용
- 그대로 쓰면, 학습할 때보다 출력 크기가 2배 → 학습-추론 간 분포 차이가 생겨 성능 저하

---

## 실행단계 해결책 – Inverted 드롭아웃
- 학습 시 살아남은 뉴런의 출력을 1/(1-p) 배 스케일링  
  - 예: p=0.5 면, 활성 뉴런에 ×2

## 실행단계 해결책 – 기존 방식
- (1-p)를 출력에 곱하여 평균치를 맞춤.  
  - 예: p=0.5면 ×0.5를 테스트 출력에 스케일링

### 단점: 테스트 시 추가 연산 필요
- 매번 (1-p) 곱셈 연산  
- 실제 서비스에서 속도 저하

---

# 3. 학습 안정성 전략

---

# 3.1. 학습 비율 조정

---

# 학습 비율은 가장 중요한 하이퍼상수!

---

## 같은 모델, 같은 전처리, 같은 모듈을 사용해도 학습 비율에 따라 다른 결과!
- 잘못된 학습 비율 선택으로 학습이 완전히 실패할 수도 있음  

![loss 그래프]
- 학습률을 너무 높게 잡거나, 낮게 잡았을 경우  
  Loss(손실)가 충분히 줄어들지 않거나, 폭발하는 모습을 볼 수 있음  

---

## 학습 비율을 어떻게 선정해야 할까?
- 기본적으로 사용하는 전략: 학습 비율을 큰 값에서 시작하여, epoch 지날수록 작은 값으로 조정함
- 학습 비율을 줄이는 다양한 전략이 있음  

![loss 그래프]
Good learning rate의 경우,  
Loss(손실)가 안정적으로 줄어드는 모습을 볼 수 있음

# 학습률 계단식 변경

---

## 일정 에폭(Epoch)이 지날 때 마다 계단식으로 줄이는 방식

- 에폭: 학습 데이터를 1번 다 외움을 의미함
- K 에폭마다, 감소계수*초기 학습률 만큼 감소하는 전략임  
  - 예: η₀ = 0.1, γ = 0.1, k = 30
  - 아래 그래프는 0~29 에폭 → 0.1, 30~59 에폭 → 0.01, 60~89 에폭 → 0.001 의 학습률을 적용한 경우임

![Training Loss & Learning Rate 그래프]

---

# 학습률 코사인 변경

---

## 계단형 방식은 변경 지점을 여러개 선정, 복잡하다는 단점이 있음
- 학습 곡선이 불안정할 수 있음

### 코사인 파형에 따른 변경
- 학습률을 코사인 함수 곡선처럼 점점 줄여가는 방식  
- 직관: “처음엔 크게, 나중엔 미세하게 조율”

\[
n_t = n_0 \cdot \frac{1}{2}\left(1 + \cos\left(\frac{\pi t}{T}\right)\right)
\]

![Training Loss & Learning Rate 그래프]

---

# 학습률 선형 변경

---

## 직선을 따라 변경
- 학습률을 학습이 진행될수록 선형(직선)으로 줄여가는 방식  
- 직관: “처음엔 크게 배우고, 끝날수록 일정하게 줄여 수렴”
- 사전학습/미세조정에서 주로 쓰임

\[
n_t = n_0 \cdot \left(1 - \frac{t}{T}\right)
\]

![Training Loss & Learning Rate 그래프]

---

# 그 밖의 변경

---

## 다양한 감소 방식 가능

- 역제곱 꼴: 처음에 크게, 이후에 느리게 줄임  
  - 대규모 학습에는 부적합함  

- 일정값 유지: 동일한 속도로 학습함  
  - 데이터가 많고 과적합 위험이 없을 때 활용  

\[
\eta_t = \frac{\eta_0}{\sqrt{t}} \quad \text{or} \quad \eta_t = \eta_0
\]

![Learning Rate 그래프 2개]

---

# 학습 종료

---

## 과적합 방지를 위한 또 다른 방법

- 빠른 종료(Early Stopping) 기법  

![Loss & Accuracy 그래프]

모델이 학습하는 동안 검증 데이터셋을 통해 검증 손실을 모니터링함. (그래프 상 주황색 곡선)  
학습을 진행하면서 더 이상 성능이 좋아지지 않고, 오히려 나빠지기 시작하면 학습을 조기에 멈춤.

---

# 3.2. 하이퍼파라미터 선정

---

# 파라미터 vs 하이퍼파라미터

---

## 파라미터
- 학습을 통해 모델 스스로 얻는 값  
  - 예: 뉴럴 네트워크의 가중치, 편향 등

## 하이퍼파라미터
- 학습 전에 사용자가 정해야 하는 값
- 학습이 진행되는 동안 고정됨
- 하이퍼파라미터에는 어떤 종류가 있을까?

---

# 주요 하이퍼파라미터

---

## 학습 관련
- 학습률 → 가장 중요!, 디케이 방식 (스텝, 코사인, 선형 등)
- 배치 사이즈 (작으면 정규화 효과, 크면 안정성 효과)
- 에폭 수 (조기 종료 여부)

## 최적화 관련
- 최적화 툴 선택 (SGD, Adam, AdamW)
- 모멘텀, 최적화 계수
- 가중치 감소 (L2, L1)

## 모델 구조 관련
- 네트워크 깊이
- 채널 수/드롭아웃 비율
- 정규화 (BN, LN, GN 등)

---

## 모델 학습 성능은 구조, 학습 방법론 못지 않게 하이퍼파라미터 선택에 크게 의존
- 어떤 값의 조합이 최적일지 알 수 없어 탐색이 필요

### 하이퍼파라미터 값을 탐색하는 방식

#### ▪ 그리드 탐색
- 하이퍼파라미터 후보들을 격자처럼 전부 조합해보는 방식임

#### ▪ 랜덤 탐색
- 하이퍼파라미터 공간에서 무작위로 값을 선택해 탐색함  
- 성능에 크게 영향을 주지 않는 불필요한 축의 값에 경우의 수를 실험하지 않아도 되어 시간을 낭비하지 않을 수 있음

![Grid Layout vs Random Layout 이미지]

---

# 하이퍼파라미터 선택 비법

---

## 모든 조합을 다 실험할 수 없다면?
- 빅테크에서조차 모든 조합을 실험하지 않음.
  - 그들도 많은 프로젝트를 진행하고 있으며, 모델이 매우 큼.
- 몇 가지 규칙과 직관을 활용하면 GPU가 많지 않아도 좋은 성능 도출 가능

# 체크리스트 1

---

## 데이터 입력 등 문제가 없을까?

### ▪ 초기 손실값을 확인하자
- 데이터 손실을 측정할 때, 분류 문제 등에서 많이 사용되는 손실 함수인 CE 손실이라면,  
  초기 손실은 log(C)와 유사한 수준이어야 정상임!
- 만약 예상치를 크게 벗어난다면 데이터 로딩/라벨 문제를 확인해야 함.

---

# 체크리스트 2

---

## 학습률과 초기값 확인

- 작은 샘플을 활용, 과적합 시켜보자.
  - 어떻게? 정확도 100% 달성까지 작은 학습 샘플에 대해 학습 진행  
    (정규화 X, 고의로 과적합 야기)

---

# 체크리스트 2

---

## 학습률과 초기값 확인

- 작은 샘플을 활용, 과적합 시켜보자.
  - 정확도 100% 달성까지 작은 학습 샘플에 대해 학습 진행  
    (정규화 X, 고의로 과적합 야기)

### ▪ 손실이 줄지 않는다면?
- 학습률/초기화 문제
- 모델 구조의 오류 가능성
- 활성화 함수 이슈
- 최적화 툴 이슈/코드 버그

---

## 학습률과 초기값 확인

- 작은 샘플을 활용, 과적합 시켜보자.
  - 정확도 100% 달성까지 작은 학습 샘플에 대해 학습 진행  
    (정규화 X, 고의로 과적합 야기)

### ▪ 손실이 폭발한다면?
- 학습률을 줄이고, 초기화 변경 (가장 흔한 이슈)
- 네트워크 깊이가 깊거나, 구조적 이슈 (RNN)

---

# 체크리스트 3

---

## 체크리스트 2에서 얻은 구조, 최적화 툴 활용, 학습률을 찾자.

- 모든 학습 데이터를 활용, 작은 Iteration (~100 iter) 빠르게 손실이 줄어드는 학습률을 우선 탐색함  
  - 100 iter만 학습하여 빠르게 경향을 파악

- 일반적 탐색 범위: 로그 스케일로 1e-1, 1e-2, 1e-3, 1e-4  
  → 손실이 실제로 줄어들기 시작하는 좋은 학습률 시작점을 다수 확보!

- 직관: “손실이 실제로 줄어드는 학습률부터 시작하라.”

---

# 체크리스트 4

---

## 체크리스트 3에서 얻은 후보 학습률 + 후보 가중치 변형 방식 중, 좋은 조합을 탐색

- 상기 조합을 활용, 1~5 에폭 동안 모델을 학습시켜 성능 비교

### ▪ 왜 이런 방식이 유효할까?
- 모든 조합을 끝까지 학습할 경우, 연산자원이 낭비됨.
- 좋은 조합은 초기 몇 에폭 내에 손실이 줄어듦.
- 모두 훈련하지 않고, 짧은 학습만으로 경향성 관찰 가능.
- 단, 검증 세트에서의 성능으로 관찰할 것.  
  (학습 손실만 보면 일반화 성능을 알 수 없음)

---

# 체크리스트 5

---

## 체크리스트 4에서 얻은 좋은 조합들을 활용, 10~20 에폭까지 추가 학습

- 학습률 변경(줄이기)는 적용하지 않음.

### ▪ 왜 학습률 변경을 제외할까?
- 학습률 변경은 성능을 더 개선하지만, 학습 속도는 느려짐.  
  - 탐색과정에서 속도 늦추는 방식을 쓰는 것은 비효율!

- 순수한 조합의 성능만으로 판단

---

# 체크리스트 6

---

## 학습률에 따른 손실곡선 관찰

![손실의 이동평균 그래프 / train-val 정확도 그래프]

- 손실의 이동평균 관찰  
- 학습데이터/검증데이터 모두 확인

---

## 학습률에 따른 손실곡선 관찰

![초반 손실값 유지 그래프]

- 초반 손실값이 유지되는 경우: **초기값이 문제!**

![손실 완만 그래프]

- 손실이 크게 떨어지지 않고 유지되는 경우:  
  **학습률 변경(줄이기) 시도하기**

---

학습률 변경을 적용하였는데, 이후 학습이 진행되지 않는 경우:  
**정체가 오기 전, 너무 일찍 학습률을 줄였다고 판단.**

(그래프: 손실이 급격히 줄다가 평평하게 유지)

---

## 학습률에 따른 정확도 곡선  

---

정확도가 증가할 경우:  
→ **계속 학습할 것!**

(그래프: 학습 데이터 / 검증 데이터 모두 상승 추세)

---

학습/검증 정확도가 크게 벌어진 경우:  
→ **과적합된 것! 정규화 방식을 도입해야 함.**

(그래프: 학습 데이터는 상승하지만 검증 데이터는 하락)

---

학습데이터 / 검증데이터 결과의 차이가 매우 적다면?  
→ **모델의 힘이 너무 약한 것!**  
큰 모델을 도입하여 더 오래 학습해야 함.

(그래프: 두 곡선이 거의 겹침)

---

# 확인문제  

---

① 좋은 모델 구조를 골랐는데도 학습이 실패할 수 있는 이유는?  
② Sigmoid 함수의 주요 문제는?  
③ Leaky ReLU가 ReLU와 다른 점은 무엇인가?  
④ RestNet에서 사용하는 전처리 방식은?

---

# 확인문제 정답 및 해설  

---

① 구조가 좋아도 학습률, 정규화, 초기화 같은 훈련 전략이 잘못되면 성능이 떨어짐  
② 출력이 0이나 1이면 기울기가 0이 되어 역전파가 끊김  
③ ReLU는 음수 입력에서 완전히 기울기가 0이 되지만,  
   Leaky ReLU는 기울기 소실을 막기 위해 작은 기울기를 둠  
④ 단순 평균 빼기보다 한 단계 더 나아가 **스케일까지 맞춰 학습 효율성을 높임**

---

# 확인문제  

---

① 드롭아웃의 문제점을 설명하시오.  
② 계단식 학습 비율 감소 방식의 단점은?  
③ 학습 비율 선정의 기본 전략은?  
④ 하이퍼파라미터를 선정하는 두 가지 방식은?

---

# 확인문제 정답 및 해설  

---

① 학습 시 실질적 출력 값이 줄어드나, 테스트 시에는 뉴런을 전부 사용하기 때문에  
   학습-추론 간 분포 차이가 생겨 성능이 저하됨  

② 변경 지점을 여러 개 선정하여 복잡하여 학습 곡선이 불안정할 수 있음  

③ 학습 초반에는 큰 학습 비율로 손실 함수 표면의 넓은 영역을 빠르게 탐색하고,  
   이후 안정적 수렴을 위해 학습 비율을 줄임  

④ **그리드 탐색**은 후보들을 격자처럼 전부 조합하는 방식,  
   **랜덤 탐색**은 무작위로 값을 선택하여 효율적으로 시간 절약

---

# 강의 정리  

---

## 오늘 공부한 내용  
요약 및 정리

### 핵심 메시지
- 좋은 모델 구조만으로는 성능 보장 X  
- 학습 전략이 안정적이고 효율적인 학습의 열쇠  

### 이번 강의에서 다룬 학습 전략
- 활성화 함수 선택  
- 가중치 초기화  
- 정규화 기법  
- 데이터 전처리  
- 하이퍼파라미터 탐색  

---

# 감사합니다 🙏  