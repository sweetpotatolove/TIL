# AI & 기계학습 기초
## AI, ML, DL
- AI (Artificial Intelligence)
  - 주어진 환경/데이터를 인지·학습·추론을 통해 목표 달성을 하도록 예측·행동 선택·계획하는 시스템

- ML (Machine Learning)
  - AI 범주 내에서 데이터로부터 학습하여 목적을 달성하는 접근 방법론
  - ex. 언어 모델, 이미지 분류 모델, 추천 시스템

- DL (Deep Learning)
  - ML 범주 내에서 신경망(Neural Network) 함수를 사용한 학습 방법론

- AI – ML (ML이 아닌 AI 시스템)의 예
  - 규칙 기반 시스템
  - 휴리스틱 기반(최적화) 알고리즘

![alt text](image.png)


## 데이터와 학습의 이해
### 데이터 구성 요소 (Feature/Label)
- 데이터의 중요성
  - 머신러닝은 규칙을 직접 코딩하지 않고 (하드코딩XX), 데이터에서 규칙을 학습함
  - 데이터(Feature, Label)의 분포와 관계가 머신러닝의 학습 결과를 결정

- Feature (피처, 특성)
  - 모델이 예측에 사용하는 입력 정보 -> `input`
  - 예측, 판단의 근거/단서

- Label (라벨, 목표값)
  - 모델이 예측하려는 정답 -> `output`
  - 학습의 목표값

  ![alt text](image-1.png)

### ML 실생활 예시
- 예시1: 유튜브 추천
  - Feature
    - 각 영상들의 정보(장르, 크리에이터, 조회수, 좋아요 수 등)
    - 사용자 정보(시청 이력, 구독 채널 등)
  - Label
    - 영상에 대한 사용자 피드백(시청 여부, 좋아요 클릭 여부)

    ![alt text](image-2.png)

- 예시2: 스팸메일 분류
  - Feature
    - 메일 제목, 발신자, 단어 빈도
  - Label
    - 스팸 / 정상


## 단일 피쳐 기반 학습
### 1D 피쳐 기반 학습 (단일 피쳐 학습)
- 1D = 1차원
- Feature가 하나일 때 머신러닝이 학습하는 가장 단순한 형태
- 수식
  - $Income_i​=f^∗(Years of Education_i​)+ε_i​$
  - 데이터셋 `D` : 30명의 **Years of Education**(피처)과 **Income**(라벨) 쌍
    - $D = \{(Years\ of\ Education_i,\ Income_i)\}_{i=1}^{30}$

      ![alt text](image-3.png)
    - 빨간색 데이터 포인트들을 가지고, X와 Y의 **함수적인 관계를 가정**할 것임
  - $f^*$ : 미지의 참 함수
    - Feature와 Label 사이의 실제 평균 관계
    - 하지만 직접 관찰할 수 없음 (우리가 정확히 알 수 없음)
    - 오차가 포함된 데이터(접근만 가능)
  - $ε$ : 측정 오차
    - 데이터에는 주로 측정 오차가 섞여 있음
    - ex. 측정 기기의 한계, 환경적 요인 등
    - 따라서, `데이터 = 참 함수 + 오차 (f* + ε)`

- ※ 미지의 참 함수 $f^*$와 측정오차 $ε$는 관측 불가능
  - 관측 가능한 것은 데이터 포인트(빨간점) 뿐

    ![alt text](image-5.png)
    - "빨간 점들을 보면서 feature와 label 사이의 평균적인 관계를 잘 나타내는 함수는 무엇일까?"를 학습하는 것
    - 무엇이 관측 가능하고, 무엇이 관측 가능하지 않은지 확실히 알아야 함

- 피처와 라벨의 관계를 잘 나타낸 함수 $f$는 무엇일까?
  - 여기서의 $f$는 미지의 참 함수 $f^*$가 아님XX
    - $f$가 미지의 참 함수와 동일하다면 그것이 가장 이상적인 상황
  - 데이터를 설명하는 여러 함수 후보가 존재
  - 어떤 함수가 가장 잘 맞는지 **학습**해야 함

    ![alt text](image-4.png)

### 모델과 가설공간
- 학습 (Learning)
  - "입력(Feature) → 출력(Label)" 관계를 찾는 과정
  - 평균 관계를 하나의 함수로 표현함
  - 하지만 관계를 표현할 수 있는 함수는 무수히 많음

- 가설 공간 (Hypothesis Space)
  - 관계를 표현할 수 있는 모든 후보 함수들의 모음
  - **피처 공간**과 **라벨 공간** 위에서 정의된 함수들의 집합 $\mathcal{F}$

- 모델 (Model)
  - 가설공간 $\mathcal{F}$에 속한 특정 함수 $f$

    ![alt text](image-6.png)
    - 가설 공간을 '선형 함수'로 정했다면
    - 모델은 특정 선형 함수를 뜻함
  
- 선형함수/비선형함수 가설공간

  ![alt text](image-7.png)

### 학습 (Learning)
주어진 데이터와 성능 척도를 바탕으로
**가설공간 $\mathcal{F}$** 의 후보들 중 최적의 모델을 선택하는 과정

- 즉, 데이터 $D$ → 가설공간 $\mathcal{F}$ → 선택된 모델 $f$

  ![alt text](image-8.png)
  - 학습 전: 데이터만 존재 (관계 미정)
  - 학습 중: 가설공간 $\mathcal{F}$ 안의 여러 후보 함수 탐색
  - 학습 후: 최적의 모델 $f$ 선택


## 복수 피쳐 기반 학습
### 2D 피쳐 기반 학습 (다차원 피처 기반 학습)
- 수식
  - $Income=f^∗(Years of Education, Seniority)+ϵ$
    - $f^∗$ : 미지의 참 함수 (입력과 출력을 이어주는 숨겨진 진짜 함수)

- 예시: 학력과 수업 데이터

  ![alt text](image-9.png)
  - 파란색 Surface (= 미지의 참 함수 $f^*$) 는 관측 불가능
  - 빨간색 점들 (= 데이터) 만 관측 가능
  - 학습 전
    - 어떤 가설공간 $\mathcal{F}$ 을 사용할까?
  - 학습 후
    - 데이터를 활용하여 어떤 모델 $f$ 을 선택해야 할까?

- 모델 복잡도에 따른 학습 예시

  ![alt text](image-10.png)
  - $\mathcal{F}$: 선형함수 가설공간
    - 단순한 선형 관계만을 표현할 수 있음
    - 데이터(빨간 점)를 완벽히 설명하기 어려움
    - 모델(보라색 곡면)은 전체적인 경향만 반영
  - $\mathcal{F}$: 비선형함수 가설공간
    - 더 복잡한 형태의 관계를 표현할 수 있음
    - 데이터의 세밀한 패턴까지 학습 가능
    - 단, 과적합(Overfitting) 가능성 증가
  - 무한이 많은 선형함수, 비선형함수들 중 적절한 함수를 특정해서 찾는 것은 매우 어려움
    - "어떻게 하면 효율적으로 찾을 수 있을까?"가 학습 테스크

### 일반적 용어 정리 및 모델 가정
- $Income = f^*(Years\ of\ Education,\ Seniority,\ ...)\ +\ \epsilon \ \rightarrow \ Y = f^*(X) + \epsilon$
  - `Income`: 우리가 예측하려는 라벨(반응/목표) 변수 → $Y$ 로 표기
  - `Years of Education`: 첫 번째 피처(입력/예측) 변수 → $X_1$ 로 표기
  - `Seniority`: 두 번째 피처(입력/예측) 변수 → $X_2$ 로 표기
    - 다른 $i$번째 피처가 있다면 역시 $X_i$ 로 표기
  - 일반적인 $p$차원 피처(총 $p$개의 피처) 벡터:
    - $\mathbf{X} = [X_1, X_2, ..., X_p]^T \in \mathbb{R}^p$
  - 모델(함수형): $f^*: \mathbb{R}^p \rightarrow \mathbb{R}$ ,  $Y = f^*(\mathbf{X}) + \epsilon$
  - 측정오차 $\epsilon$:
    - 피처 $\mathbf{X}$와 독립
    - $E[\epsilon] = 0$ (**평균이 0인 오차로 가정**)

### 왜 $f(\cdot)$를 학습하는가?
- 예측 (Prediction)
  - 잘 학습된 $f$가 있으면, 새로운 입력값 $X = x$에서 반응/목표 $Y$ 를 예측할 수 있음

- 중요 특성 파악 (Feature Importance)
  - 피처들 $X = (x_1, x_2, ..., x_p)$ 의 **어떤 특성**이 $Y$를 설명하는 데 **중요**한지,
  - 그리고 어떤 것은 **덜 중요(무관)** 한지를 알 수 있음
  - 예시:
    - 근속연수(Seniority), 교육기간(Years of Education)은 소득(Income)에 큰 영향을 줄 수 있지만
    - 혼인 여부(Marital Status)는 영향이 거의 없을 것임

- 해석 가능성 (Interpretability)
  - $f$의 **복잡도**에 따라 각 구성요소 $x_j$가 $Y$에 **어떻게 영향을 미치는지** (증가/감소 방향, 민감도 등)을 이해할 수 있음


# AI & 기계학습 기초 2
## 지도학습(supervised learning)
![alt text](image-11.png)

- 데이터
  - 입력(특성)과 정답(라벨)이 쌍으로 있는 데이터

- 목표
  - 새 입력이 들어오면 정답을 잘 맞추는 규칙을 학습

- 지도학습 종류
  - **회귀** : 예측값이 숫자(가격, 점수, 온도)
  - **분류** : 예측값이 범주(스팸/정상, 질병 유/무)

### 지도학습 용어
- 특성 (Feature, $x$)
  - 예측에 쓰이는 설명 변수
  - ex. 집값 예측 {지역, 평수, 방수, 연식} / 이메일 스팸 필터링 {제목, 내용, 송신인}

- 라벨 (Label, $y$)
  - 맞춰야 하는 정답
  - ex. 집값, 스팸/정상 이메일

- 예측값 ($\hat{y}$)
  - 모델이 내놓은 결과 (숫자 또는 범주)

- 오류 (Error)
  - 예측값($\hat{y}$)과 라벨($y$)의 차이: $\hat{y}$ - $y$
  - 측정오차 $ε$와 다른거!!

- 예시

  ![alt text](image-12.png)


## 회귀(Regression)
### 회귀 문제
- "입력(Feature)으로부터 숫자(output)를 얼마나 정확히 예측할까?"에 대한 문제
  - ex. Feature: 면적, 방수, 연식 -> Label: 집값
  - ex. Feature: 매체별 광고비(TV/라디오/온라인) -> Label: 매출액

- 라벨 및 예측 모델의 출력: **연속적인 수치**

  ![alt text](image-13.png)

### 회귀 오류: 평균제곱오차(MSE)
- 평균제곱오차 (Mean Squared Error, MSE)
  - 각 데이터에서 정답($y_i$) 과 예측값($\hat{y_i}$) 의 평균 제곱 차이값
  - 수식: $MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2$

- 해석
  - 큰 오차를 더 크게 벌주므로, 전체 오차 수준을 한눈에 파악할 수 있음

- 참고
  - 데이터와 같은 단위로 표현하고 싶다면 RMSE (MSE의 제곱근) 을 사용
    - R은 root를 의미함
  - 수식: $RMSE = \sqrt{MSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2}$

### 회귀 설명력: $R^2$ (결정계수)
- 결정계수
  - 라벨의 분산 중에서 특성으로 설명되는 비율
  - "평균만 쓰는 단순한 예측"보다 나의 모델링 결과가 얼마나 더 잘 맞추는지를 `0~1` 사이로 나타낸 값
  - 수식: $R^2 = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}$
    - $\bar{y}$ : $y_i$들의 평균값
    - 분모: 예측값을 $y$의 평균으로 했을 때($\bar{y}$) 발생하는 에러
    -  

- 해석
  - $R^2$이 1에 가까울수록 설명력이 높고, 낮을수록 설명력이 낮음

※ $R^2$가 음수가 나올 수 있을까?

-> 나올 수 있음. 예측값($\hat{y_i}$)들이 평균값($\bar{y}$) 보다도 못하다면!! 분자의 에러가 분모보다 커져서 $R^2$가 음수될 수 있음

## 분류(Classification)
### 분류 문제
- "입력(Feature)으로부터 범주(output)를 얼마나 정확히 가려낼까?"에 대한 문제
  - ex. Feature: 메일 내용, 보낸이 이메일 주소 -> Label: 스팸/정상
  - ex. Feature: 종양 반경, 면적 -> Label: 악성/양성

- 라벨 및 예측 모델의 출력: **범주 라벨(이진/다중)**

  ![alt text](image-15.png)

### 분류 정확도(Accuracy)
- 정확도 (Accuracy)
  - 전체 중 맞춘 비율
  - 수식: $Accuracy = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(y_i = \hat{y_i})$
    - $\mathbb{I}$ : 지시(Indicator) 함수
    - $\mathbb{I}(A) = 1$ if $A$ is true
    - 올바르게 예측한 값(1, true)을 다 더해서 n으로 나눔
    - 즉, 내가 예측한 값들 중 얼만큼의 비율이 정확히 맞췄는지 계산

- 정확도만 보면 발생하는 문제
  - 불균형 데이터(예: 양성 1%, 음성 99%)에서는 **전부 음성**이라고 예측해도 **정확도 99%** 로 보일 수 있음

- 결론
  - 정확도만 보지 말고, 다른 지표(Precision, Recall 등) 도 함께 봐야 안전함

### 혼동행렬(Confusion Matrix)
- 혼동행렬
  - 예측과 실제 값 사이의 관계를 행렬 형태로 표현
    | 구분              | 예측 Positive         | 예측 Negative         |
    | :-------------- | :------------------ | :------------------ |
    | **실제 Positive** | True Positive (TP)  | False Negative (FN) |
    | **실제 Negative** | False Positive (FP) | True Negative (TN)  |

  - 용어
    - **TP** (True Positive): 실제 양성, 예측도 양성
    - **TN** (True Negative): 실제 음성, 예측도 음성
    - **FP** (False Positive): 실제는 음성인데 양성이라 함 (오탐)
    - **FN** (False Negative): 실제는 양성인데 음성이라 함 (누락)
      
      ![alt text](image-16.png)
      - FP(오탐), FN(누락)은 줄이고 TP, TN은 높여야함

- 정밀도 (Precision)
  - "양성이라 **판정한 것** 중" 진짜 양성의 비율
  - 수식: $Precision = \frac{TP}{TP + FP}$

- 재현율 (Recall, Sensitivity)
  - "진짜 양성 **가운데**" 잡아낸 예측 양성의 비율
  - 수식: $Recall = \frac{TP}{TP + FN}$

- F1-score
  - **정밀도와 재현율의 조화평균**
  - 수식: $F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}$

## 학습의 목적
### 학습의 목적은 "테스트 예측 (일반화)"
- **학습 모델**의 성능 평가는 모델이 **처음 보는(학습에 사용되지 않은) 데이터**로 평가
  - **일반화(generalization)** 오류의 최소화 지향
- 훈련 데이터에서 성능이 아무리 좋아도, 새로운 데이터에서 성능이 떨어지면 실전에서 사용 불가
- **일반화 성능을 추정**(검증/교차검증) 방법을 알아야함

### 오버피팅(overfitting)
- 오버피팅
  - 훈련 데이터의 **우연한 패턴/잡음**까지 외워버려서(초록색 함수) 훈련에서는 잘 맞지만 **테스트에서는 성능이 나빠지는** 현상

    ![alt text](image-17.png)
  - 현상
    - **훈련 오류 급격히 낮음** -> train data의 Accuracy는 100%가 될 것임
    - 테스트 오류 **높음/요동**

- 오버피팅이 안좋은 이유
  - **표본(sample) 의존/불안정**
    - 훈련 데이터는 모집단의 일부 표본이라 우연한 잡음이 섞임
    - 이것에만 과하게 맞추어 학습하면, 샘플 몇 개만 바뀌어도 예측이 크게 흔들림 (분산 ↑)
  - **일반화 실패**
    - 보지 못한 데이터(테스트) 에서 오류가 커지고, 모집단(population) 전체 성능과의 격차가 벌어짐
  
### 오버피팅 ≠ 분포 변화(Distribution Shift)로 인한 에러 증가
- 분포 변화로 인한 오류
  - 훈련 데이터 분포와 테스트 **분포가 다름으로 인해** (환경, 계절, 센서 변화 등) 성능이 떨어지는 현상
  - 예시

    ![alt text](image-18.png)
    - 훈련은 실제 사진으로 했는데, 테스트는 카툰 이미지로 했다면?
    - 데이터가 아예 변했기 때문에 모델은 구분 잘 못함
    - 이런 경우가 "데이터의 분포가 변한 것"
  - **분포 변화**로 인한 에러 증가는 **모델이 과적합하지 않아도 발생** 가능
  - 오버피팅은 train, test 데이터가 **"같은 분포에서 오는 데이터"** 인데, 데이터 포인트가 다른 것
    - (★ 차이점 꼭 기억하기 ★)

### 오버피팅 vs 언더피팅
- 오버피팅
  - 모델이 너무 복잡 -> 잡음까지 학습 (테스트 성능 나쁨)

- 언더피팅
  - 모델이 단순하거나 학습이 완료되지 않음 -> 중요한 패턴 놓침 (오류 큼)

  ![alt text](image-19.png)

- 오버피팅 vs 언더피팅 (균형잡기) 해결 방법론
  - 더 많은 데이터 사용
  - 테스트 데이터를 활용한 모델 선정
  - 테스트 데이터를 추정해가며 교차검증

# AI & 기계학습 기초 3
## 테스트 성능 평가
### 훈련오류 vs 테스트 오류
- 훈련오류
  - 모델을 학습시킨 **같은 데이터**에 다시 적용해 계산한 오류
- 테스트 오류
  - 학습에 쓰지 않은 **새 관측치**에 대해 모델을 적용했을 때의 평균 예측오류

  ![alt text](image-20.png)

- 보통 훈련 오류는 테스트 오류와 다름
  - 특히 훈련 오류는 테스트 오류를 (심하게) 과소평가하는 경우가 많음
  - 즉, 테스트에서는 성능이 안좋을텐데 마치 훈련오류 안에서는 잘하는 것처럼 보이는 것
  - ex. 암기 vs 응용 시험
    - 족보를 암기했을 때, 훈련 오류는 매우 낮을 것임
    - 족보를 암기했다고 해서 중간고사를 잘 볼 것이라는 보장 없음(즉, 테스트 오류 또한 낮을 것이라는 보장이 없음)
    - 지금까지 봐왔던 시험들을 바탕으로 훈련 오류가 매우 낮았다고 해서, 이를 바탕으로 "나는 새로운 테스트에서 잘하겠지"라고 생각하는 것이 **훈련 오류로 테스트 오류를 과소평가하는 경우**임

- 훈련 오류 vs 테스트 오류

  ![alt text](image-21.png)
  - x축: 모델 복잡도 / y축: 예측 오류 
  - 파란선: 훈련 오류 / 빨간선: 테스트 오류
  - 훈련 오류는 계속 ↓, 테스트 오류는 U자형
    - 모델 복잡도가 충분히 커져서 (아주 유연한 함수여서), 모든 훈련 데이터의 노이즈마저 학습할 수 있다면 훈련 오류는 현저히 작아짐
    - 그렇다고 해서 우리가 낮추고자 하는 테스트 오류(일반화 성능)가 항상 낮아지는 것은 아니다
  - 훈련 오류가 매우 작아졌을 때 테스트 오류가 오히려 높아질 수 있음
    - 이 상태가 오버피팅 상태!!
    - 훈련 오류와 테스트 오류가 차이가 난다해서 무조건 오버피팅이 아니라,
    - train data에 과적합하여 노이즈까지 학습함으로써 훈련 오류가 작고, 테스트 오류가 클 때 '오버피팅' 된거임
  - 훈련 오류도 높고, 테스트 오류도 높은 구간은 '언더피팅'
    - 데이터에 있는 주요 패턴을 아직 잘 학습하지 못해 두 오류 다 높게 나타나는 상태

  - 목적
    - 테스트 오류의 U자형이 바닥이 되도록하는 적절한 모델 찾기
    - 즉, 테스트 오류가 가장 낮아지는 모델 찾는 것이 목적

### 테스트 예측 오류 계산
- 테스트 오류 계산하는 이상적인 케이스
  - 충분히 큰 별도의 테스트 데이터셋 존재 -> 현실에선 구하기 어려움
  - 현실에서는 테스트만을 위한 데이터를 갖기에 데이터 자체가 부족할 수 있음

- 대안: **재표본화(resampling)를 통한 테스트 오류 추정**
  - **데이터를 나눠** 여러번 "훈련 -> 평가"를 반복해 테스트 오류를 가늠
  - 방법
    - 검증셋 (hold-out), K겹 교차검증 (K-fold Cross-Validation)
  - 장점
    - 별도의 테스트 데이터 없이 **데이터를 더 효율적으로 사용**하여 일반화 오차 추정
  
  
## 검증셋(validation set) 접근
### 검증셋 방법
![alt text](image-22.png)
- 가용 샘플들을 무작위로 **훈련셋과 검증셋(hold-out)** 으로 분할
- 훈련셋으로 **모델 적합**, 검증셋으로 **예측 후 검증 오류**를 계산
- 검증 오류는 보통 **정량 반응은 MSE, 범주 반응은 오분류율(또는 F1-score)** 을 측정함

### 검증셋 절차
- 데이터 순서 무작위 **셔플링** 후 두 부분으로 분할
  - 왼쪽(파랑) = 훈련셋 / 오른쪽(주황) = 검증셋

    ![alt text](image-23.png)
  - 학습은 훈련셋에서, 성능평가는 검증셋에서 수행

- 예시: 자동차 데이터
  - 목표
    - 선형(1차) 모델부터 고차항(다항식) 모델 비교

      ![alt text](image-24.png)
      - 알아두어야 할 것
        - 현재 테스트 데이터 없음
        - 만약 테스트 데이터가 주어졌을 때, 테스트 데이터의 에러가 얼만큼일까?를 알려고 하는 것
  - 392개 데이터를 무작위로 196개 훈련셋 / 196개 검증셋으로 분할
    - 일반적으로 반으로 나눌 필요는 없음

  - **죄측 패널**: 단 한 번 분할 시의 MSE 곡선
    - 테스트 에러를 구한 것은 아님(테스트 데이터 없으니까)
    - 테스트 에러를 '추정한 값'을 그래프로 나타냄
  - **우측 패널**: 여러번 (셔플 후) 다른 분할의 MSE 곡선들
    - 어떻게 분할하냐에 따라 테스트 에러 추정치가 여러 값으로 다르게 나옴
    - 2차항까지 고려한 모델을 가지고 테스트 에러를 추정했을 때 추정한 테스트 에러 값은 몇이라고 할 수 있을까? -> 추정치의 분산이 큼
    - 추정치의 분산이 큰 것은 매우 나쁜 것임 -> 검증셋 방법의 단점

### 검증셋 접근의 한계
- 예시에서 보이는 검증셋 방법의 단점
  - 어떤 표본이 훈련/검증에 들어가느냐에 따라 **검증 기반 테스트 오류 추정치가 매우 가변적**
  - 검증 접근에서는 훈련셋 (= 전체의 일부)만으로 모델을 **적합**하므로, 전체 데이터로 학습했을 때보다 **성능이 낮게 추정(즉, 테스트 오류를 과대 추정)** 될 수 있음

- 전체 데이터로 학습한 모델의 **테스트 오류를 과대 추정하는 경향**이 있는 이유
  - "학습에 데이터를 부분만 사용하기 때문"

3차시 23분
## K-겹 교차검증(K-fold Cross-Validation)
### K-겹 교차검증
94p
