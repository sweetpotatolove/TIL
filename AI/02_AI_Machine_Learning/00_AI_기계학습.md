# AI & 기계학습 기초
## AI, ML, DL
- AI (Artificial Intelligence)
  - 주어진 환경/데이터를 인지·학습·추론을 통해 목표 달성을 하도록 예측·행동 선택·계획하는 시스템

- ML (Machine Learning)
  - AI 범주 내에서 데이터로부터 학습하여 목적을 달성하는 접근 방법론
  - ex. 언어 모델, 이미지 분류 모델, 추천 시스템

- DL (Deep Learning)
  - ML 범주 내에서 신경망(Neural Network) 함수를 사용한 학습 방법론

- AI – ML (ML이 아닌 AI 시스템)의 예
  - 규칙 기반 시스템
  - 휴리스틱 기반(최적화) 알고리즘

![alt text](image.png)


## 데이터와 학습의 이해
### 데이터 구성 요소 (Feature/Label)
- 데이터의 중요성
  - 머신러닝은 규칙을 직접 코딩하지 않고 (하드코딩XX), 데이터에서 규칙을 학습함
  - 데이터(Feature, Label)의 분포와 관계가 머신러닝의 학습 결과를 결정

- Feature (피처, 특성)
  - 모델이 예측에 사용하는 입력 정보 -> `input`
  - 예측, 판단의 근거/단서

- Label (라벨, 목표값)
  - 모델이 예측하려는 정답 -> `output`
  - 학습의 목표값

  ![alt text](image-1.png)

### ML 실생활 예시
- 예시1: 유튜브 추천
  - Feature
    - 각 영상들의 정보(장르, 크리에이터, 조회수, 좋아요 수 등)
    - 사용자 정보(시청 이력, 구독 채널 등)
  - Label
    - 영상에 대한 사용자 피드백(시청 여부, 좋아요 클릭 여부)

    ![alt text](image-2.png)

- 예시2: 스팸메일 분류
  - Feature
    - 메일 제목, 발신자, 단어 빈도
  - Label
    - 스팸 / 정상


## 단일 피쳐 기반 학습
### 1D 피쳐 기반 학습 (단일 피쳐 학습)
- 1D = 1차원
- Feature가 하나일 때 머신러닝이 학습하는 가장 단순한 형태
- 수식
  - $Income_i​=f^∗(Years of Education_i​)+ε_i​$
  - 데이터셋 `D` : 30명의 **Years of Education**(피처)과 **Income**(라벨) 쌍
    - $D = \{(Years\ of\ Education_i,\ Income_i)\}_{i=1}^{30}$

      ![alt text](image-3.png)
    - 빨간색 데이터 포인트들을 가지고, X와 Y의 **함수적인 관계를 가정**할 것임
  - $f^*$ : 미지의 참 함수
    - Feature와 Label 사이의 실제 평균 관계
    - 하지만 직접 관찰할 수 없음 (우리가 정확히 알 수 없음)
    - 오차가 포함된 데이터(접근만 가능)
  - $ε$ : 측정 오차
    - 데이터에는 주로 측정 오차가 섞여 있음
    - ex. 측정 기기의 한계, 환경적 요인 등
    - 따라서, `데이터 = 참 함수 + 오차 (f* + ε)`

- ※ 미지의 참 함수 $f^*$와 측정오차 $ε$는 관측 불가능
  - 관측 가능한 것은 데이터 포인트(빨간점) 뿐

    ![alt text](image-5.png)
    - "빨간 점들을 보면서 feature와 label 사이의 평균적인 관계를 잘 나타내는 함수는 무엇일까?"를 학습하는 것
    - 무엇이 관측 가능하고, 무엇이 관측 가능하지 않은지 확실히 알아야 함

- 피처와 라벨의 관계를 잘 나타낸 함수 $f$는 무엇일까?
  - 여기서의 $f$는 미지의 참 함수 $f^*$가 아님XX
    - $f$가 미지의 참 함수와 동일하다면 그것이 가장 이상적인 상황
  - 데이터를 설명하는 여러 함수 후보가 존재
  - 어떤 함수가 가장 잘 맞는지 **학습**해야 함

    ![alt text](image-4.png)

### 모델과 가설공간
- 학습 (Learning)
  - "입력(Feature) → 출력(Label)" 관계를 찾는 과정
  - 평균 관계를 하나의 함수로 표현함
  - 하지만 관계를 표현할 수 있는 함수는 무수히 많음

- 가설 공간 (Hypothesis Space)
  - 관계를 표현할 수 있는 모든 후보 함수들의 모음
  - **피처 공간**과 **라벨 공간** 위에서 정의된 함수들의 집합 $\mathcal{F}$

- 모델 (Model)
  - 가설공간 $\mathcal{F}$에 속한 특정 함수 $f$

    ![alt text](image-6.png)
    - 가설 공간을 '선형 함수'로 정했다면
    - 모델은 특정 선형 함수를 뜻함
  
- 선형함수/비선형함수 가설공간

  ![alt text](image-7.png)

### 학습 (Learning)
주어진 데이터와 성능 척도를 바탕으로
**가설공간 $\mathcal{F}$** 의 후보들 중 최적의 모델을 선택하는 과정

- 즉, 데이터 $D$ → 가설공간 $\mathcal{F}$ → 선택된 모델 $f$

  ![alt text](image-8.png)
  - 학습 전: 데이터만 존재 (관계 미정)
  - 학습 중: 가설공간 $\mathcal{F}$ 안의 여러 후보 함수 탐색
  - 학습 후: 최적의 모델 $f$ 선택


## 복수 피쳐 기반 학습
### 2D 피쳐 기반 학습 (다차원 피처 기반 학습)
- 수식
  - $Income=f^∗(Years of Education, Seniority)+ϵ$
    - $f^∗$ : 미지의 참 함수 (입력과 출력을 이어주는 숨겨진 진짜 함수)

- 예시: 학력과 수업 데이터

  ![alt text](image-9.png)
  - 파란색 Surface (= 미지의 참 함수 $f^*$) 는 관측 불가능
  - 빨간색 점들 (= 데이터) 만 관측 가능
  - 학습 전
    - 어떤 가설공간 $\mathcal{F}$ 을 사용할까?
  - 학습 후
    - 데이터를 활용하여 어떤 모델 $f$ 을 선택해야 할까?

- 모델 복잡도에 따른 학습 예시

  ![alt text](image-10.png)
  - $\mathcal{F}$: 선형함수 가설공간
    - 단순한 선형 관계만을 표현할 수 있음
    - 데이터(빨간 점)를 완벽히 설명하기 어려움
    - 모델(보라색 곡면)은 전체적인 경향만 반영
  - $\mathcal{F}$: 비선형함수 가설공간
    - 더 복잡한 형태의 관계를 표현할 수 있음
    - 데이터의 세밀한 패턴까지 학습 가능
    - 단, 과적합(Overfitting) 가능성 증가
  - 무한이 많은 선형함수, 비선형함수들 중 적절한 함수를 특정해서 찾는 것은 매우 어려움
    - "어떻게 하면 효율적으로 찾을 수 있을까?"가 학습 테스크

### 일반적 용어 정리 및 모델 가정
- $Income = f^*(Years\ of\ Education,\ Seniority,\ ...)\ +\ \epsilon \ \rightarrow \ Y = f^*(X) + \epsilon$
  - `Income`: 우리가 예측하려는 라벨(반응/목표) 변수 → $Y$ 로 표기
  - `Years of Education`: 첫 번째 피처(입력/예측) 변수 → $X_1$ 로 표기
  - `Seniority`: 두 번째 피처(입력/예측) 변수 → $X_2$ 로 표기
    - 다른 $i$번째 피처가 있다면 역시 $X_i$ 로 표기
  - 일반적인 $p$차원 피처(총 $p$개의 피처) 벡터:
    - $\mathbf{X} = [X_1, X_2, ..., X_p]^T \in \mathbb{R}^p$
  - 모델(함수형): $f^*: \mathbb{R}^p \rightarrow \mathbb{R}$ ,  $Y = f^*(\mathbf{X}) + \epsilon$
  - 측정오차 $\epsilon$:
    - 피처 $\mathbf{X}$와 독립
    - $E[\epsilon] = 0$ (**평균이 0인 오차로 가정**)

### 왜 $f(\cdot)$를 학습하는가?
- 예측 (Prediction)
  - 잘 학습된 $f$가 있으면, 새로운 입력값 $X = x$에서 반응/목표 $Y$ 를 예측할 수 있음

- 중요 특성 파악 (Feature Importance)
  - 피처들 $X = (x_1, x_2, ..., x_p)$ 의 **어떤 특성**이 $Y$를 설명하는 데 **중요**한지,
  - 그리고 어떤 것은 **덜 중요(무관)** 한지를 알 수 있음
  - 예시:
    - 근속연수(Seniority), 교육기간(Years of Education)은 소득(Income)에 큰 영향을 줄 수 있지만
    - 혼인 여부(Marital Status)는 영향이 거의 없을 것임

- 해석 가능성 (Interpretability)
  - $f$의 **복잡도**에 따라 각 구성요소 $x_j$가 $Y$에 **어떻게 영향을 미치는지** (증가/감소 방향, 민감도 등)을 이해할 수 있음

---
p.49
# AI & 기계학습 기초 2
## 지도학습



## 회귀(Regression)


## 분류(Classification)



## 학습의 목적