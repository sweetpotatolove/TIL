# ë°ì´í„° EDA ë° ëª¨ë¸ í•™ìŠµ
## ğŸ”¹ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)
### EDAì˜ ëª©ì 
EDA(Exploratory Data Analysis)ëŠ” ëª¨ë¸ë§ ì „, ë°ì´í„°ë¥¼ ê¹Šì´ ì´í•´í•˜ê¸° ìœ„í•œ ì²« ë‹¨ê³„

-> ë°ì´í„°ì˜ êµ¬ì¡°Â·ë¶„í¬Â·ìƒê´€ê´€ê³„Â·ê²°ì¸¡ì¹˜Â·ì´ìƒì¹˜ë¥¼ íŒŒì•…í•¨ìœ¼ë¡œì¨, ì´í›„ ì „ì²˜ë¦¬ ë°©í–¥ê³¼ ëª¨ë¸ë§ ì „ëµì„ ê²°ì •í•  ìˆ˜ ìˆìŒ

### ë°ì´í„° ë¡œë”© ë° êµ¬ì¡° íŒŒì•…
- `load_wine` ë°ì´í„°ì…‹ ë¡œë“œ
  ```python
  from sklearn.datasets import load_wine
  import pandas as pd
  import numpy as np

  df, y = load_wine(as_frame=True, return_X_y=True)
  df["quality"] = y
  ```

- ë°ì´í„° êµ¬ì¡° íŒŒì•… í•¨ìˆ˜
  | í•¨ìˆ˜                           | ì„¤ëª…                       | ì˜ˆì‹œ                                                 |
  | ---------------------------- | ------------------------ | -------------------------------------------------- |
  | `len(df)`                    | ì „ì²´ ìƒ˜í”Œ ìˆ˜(í–‰ ê°œìˆ˜)ë¥¼ ë°˜í™˜        | `sample_count = len(df)`                           |
  | `df.shape`                   | (í–‰, ì—´) í˜•íƒœë¥¼ íŠœí”Œë¡œ ë°˜í™˜        | `feature_count = df.shape[1]`                      |
  | `pd.Series().nunique()`      | ê³ ìœ  ê°’(í´ë˜ìŠ¤ ë“±)ì˜ ê°œìˆ˜ ê³„ì‚°       | `class_count = pd.Series(y).nunique()`             |
  | `pd.Series().value_counts()` | ê° í´ë˜ìŠ¤ë‚˜ ê°’ì˜ ë¹ˆë„ ê³„ì‚°          | `class_distribution = pd.Series(y).value_counts()` |
  | `.sort_index()`              | ì¸ë±ìŠ¤ ê¸°ì¤€ ì •ë ¬ (í´ë˜ìŠ¤ ë²ˆí˜¸ ìˆœì„œ ë§ì¶¤) | `class_distribution.sort_index()`                  |

- ê·¸ë£¹ë³„ í†µê³„ ê³„ì‚° í•¨ìˆ˜
  | í•¨ìˆ˜                       | ì„¤ëª…                        | ì˜ˆì‹œ                                     |
  | ------------------------ | ------------------------- | -------------------------------------- |
  | `pd.DataFrame.groupby()` | íŠ¹ì • ê¸°ì¤€(ì˜ˆ: í´ë˜ìŠ¤ë³„)ìœ¼ë¡œ ë¬¶ì–´ ì—°ì‚° ìˆ˜í–‰ | `df.groupby('y')['alcohol'].mean()`    |
  | `.mean()`                | í‰ê·  ê³„ì‚°                     | `malic_mean = df['malic_acid'].mean()` |
  | `.std()`                 | í‘œì¤€í¸ì°¨ ê³„ì‚°                   | `malic_std = df['malic_acid'].std()`   |
  | `.idxmin()`              | ìµœì†Œê°’ì„ ê°€ì§„ í–‰ì˜ ì¸ë±ìŠ¤ ë°˜í™˜         | `min_ash_idx = df['ash'].idxmin()`     |

- ì¡°ê±´ë¶€ ë¹„ìœ¨ ë° êµ¬ê°„ ê³„ì‚°
  | í•¨ìˆ˜               | ì„¤ëª…                               | ì˜ˆì‹œ                                     |
  | ---------------- | -------------------------------- | -------------------------------------- |
  | `(ì¡°ê±´).mean()`    | ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¹„ìœ¨ ê³„ì‚° (True=1, False=0) | `(df["color_intensity"] >= 10).mean()` |
  | `.quantile(q)`   | ë¶„ìœ„ìˆ˜ ê³„ì‚° (ì˜ˆ: ìƒìœ„ 10%, ì¤‘ê°„ê°’ ë“±)        | `df["magnesium"].quantile(0.9)`        |
  | `np.histogram()` | ì—°ì†í˜• ë³€ìˆ˜ì˜ ë¹ˆë„(ë„ìˆ˜) ê³„ì‚°                | `np.histogram(df["proline"], bins=20)` |

- ìƒê´€ê´€ê³„ ë¶„ì„
  | í•¨ìˆ˜                 | ì„¤ëª…                          | ì˜ˆì‹œ                                 |
  | ------------------ | --------------------------- | ---------------------------------- |
  | `df.corr()`        | ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ Pearson ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ìƒì„± | `corr = df.corr()`                 |
  | `.drop("alcohol")` | ìê¸° ìì‹  ì œì™¸                    | `corr["alcohol"].drop("alcohol")`  |
  | `.abs().idxmax()`  | ì ˆëŒ“ê°’ ê¸°ì¤€ ìƒê´€ê³„ìˆ˜ê°€ ê°€ì¥ í° ë³€ìˆ˜ëª… ë°˜í™˜    | `corr_with_alcohol.abs().idxmax()` |

### ìƒê´€ê´€ê³„ ë¶„ì„ (Correlation Analysis)
- Pearson ìƒê´€ê³„ìˆ˜ (r): ë‘ ë³€ìˆ˜ ê°„ ì„ í˜• ê´€ê³„ ê°•ë„ë¥¼ `[-1, 1]`ë¡œ í‘œí˜„
  - +1 â†’ ì–‘ì˜ ìƒê´€ê´€ê³„
  - -1 â†’ ìŒì˜ ìƒê´€ê´€ê³„
  - 0 â†’ ê´€ê³„ ì—†ìŒ

- ì£¼ì˜: ìƒê´€ê´€ê³„ â‰  ì¸ê³¼ê´€ê³„ (Correlation â‰  Causation)
  - ex. ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰ê³¼ ìµì‚¬ ì‚¬ê³ ëŠ” ëª¨ë‘ ì—¬ë¦„ì— ì¦ê°€í•˜ì§€ë§Œ ì¸ê³¼ê´€ê³„ëŠ” ì—†ë‹¤

- ì½”ë“œ ì˜ˆì‹œ
  ```python
  corr = df.corr()
  fig, ax = plt.subplots(figsize=(10, 7))
  mask = np.triu(np.ones_like(corr, dtype=bool))  # ìƒì‚¼ê°í–‰ë ¬ ë§ˆìŠ¤í¬

  sns.heatmap(data=corr, annot=True, fmt=".2f",
              cmap="coolwarm", mask=mask)
  ax.grid(False)
  plt.title("Correlation between features")
  plt.show()
  ```
  ![alt text](image-144.png)

### ë³€ìˆ˜ ë¶„í¬ ì‚´í´ë³´ê¸° (Distribution Visualization)
- `histplot`: ë³€ìˆ˜ì˜ ë¶„í¬ì™€ ì»¤ë„ ë°€ë„(KDE) ì‹œê°í™”
- `scatterplot`: ë‘ ë³€ìˆ˜ì˜ ê´€ê³„ë¥¼ ì ìœ¼ë¡œ í‘œí˜„
- `pairplot`: ì—¬ëŸ¬ ë³€ìˆ˜ ìŒì˜ ê´€ê³„ë¥¼ í•œ ë²ˆì— íƒìƒ‰

- ì—¬ëŸ¬ í˜•íƒœ ë¶„í¬ ì‹œê°í™”
  ```python
  fig, ax = plt.subplots(figsize=(18, 5), ncols=3)

  # 1. ì„¸ë¡œí˜• íˆìŠ¤í† ê·¸ë¨
  sns.histplot(data=df, y="flavanoids", bins=20, kde=True, ax=ax[0])

  # 2. í´ë˜ìŠ¤ë³„ ë¶„í¬
  sns.histplot(data=df, x="flavanoids", hue="quality", bins=20, kde=True, ax=ax[1])

  # 3. ì´ë³€ëŸ‰ ë¶„í¬ (2D)
  sns.histplot(data=df, x="flavanoids", y="total_phenols", hue="quality",
              bins=20, ax=ax[2])
  ```
  ![alt text](image-145.png)

- ì‚°ì ë„ì™€ í˜ì–´í”Œë¡¯ ì‹œê°í™”
  ```python
  sns.scatterplot(data=df, x="flavanoids", y="total_phenols", hue="quality")

  # qualityì™€ ìƒê´€ ë†’ì€ ìƒìœ„ 5ê°œ feature ì¶”ì¶œ
  corr_with_quality = corr["quality"].abs().sort_values(ascending=False)
  top_features = corr_with_quality.index[1:6]

  sns.pairplot(data=df[top_features.tolist() + ['quality']], hue="quality", corner=True)
  ```
  ![alt text](image-146.png)

### ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ íƒìƒ‰ (Missing & Outlier Detection)
- ê²°ì¸¡ì¹˜(Missing Value): ë°ì´í„°ê°€ ë¹„ì–´ ìˆëŠ” ê²½ìš°
  - ì²˜ë¦¬ ë°©ë²•
    1. í–‰/ì—´ ì‚­ì œ (`dropna()`)
    2. í‰ê· , ì¤‘ì•™ê°’, ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´ (`fillna()`)
    3. ì˜ˆì¸¡ ê¸°ë°˜ ëŒ€ì²´ (ê³ ê¸‰ ê¸°ë²•)

- ì´ìƒì¹˜ íƒìƒ‰ (Outlier Detection)
  - IQR(Interquartile Range) ê¸°ë°˜ íƒìƒ‰
    - Q1 = 25%, Q3 = 75%
    - IQR = Q3 - Q1
    - ì´ìƒì¹˜: `Q1 - 1.5Ã—IQR`ë³´ë‹¤ ì‘ê±°ë‚˜, `Q3 + 1.5Ã—IQR`ë³´ë‹¤ í° ê°’
  
  - ì´ìƒì¹˜ ì²˜ë¦¬ ì˜ˆì‹œ ì½”ë“œ
    ```python
    def detect_outliers_iqr(data, column):
        Q1, Q3 = data[column].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR
        return data[(data[column] < lower) | (data[column] > upper)]

    outliers = detect_outliers_iqr(df_missing, 'alcohol')

    # ì´ìƒì¹˜ ì œê±°
    df_no_outliers = df_filled[~df_filled.index.isin(outliers.index)]
    ```

### ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ì²˜ë¦¬
ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ ì²˜ë¦¬ëŠ” ë‹¨ìˆœíˆ ê°’ë§Œ ì±„ìš°ê±°ë‚˜ ì œê±°í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì‹œê°„ì  íë¦„ì„ ê³ ë ¤í•´ì•¼ í•¨

- ê²°ì¸¡ì¹˜ ì²˜ë¦¬
	-	Forward Fill (ffill)
    - ì´ì „ ì‹œì ì˜ ê°’ì„ ê·¸ëŒ€ë¡œ ì±„ì›€
    - ì£¼ê°€Â·ì˜¨ë„ì²˜ëŸ¼ ê°’ì´ ê¸‰ë³€í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì í•©
	-	Backward Fill (bfill)
    - ë‹¤ìŒ ì‹œì ì˜ ê°’ì„ ì‚¬ìš©
	-	ë³´ê°„(Interpolation)
    - ì„ í˜•Â·ë‹¤í•­Â·ìŠ¤í”Œë¼ì¸ ë³´ê°„ ë“±ìœ¼ë¡œ ì‹œê³„ì—´ì˜ ì¶”ì„¸ë¥¼ ë°˜ì˜í•´ ì±„ì›€
	-	ëª¨ë¸ ê¸°ë°˜ ì˜ˆì¸¡
    - ARIMA, Prophet ë“± ì‹œê³„ì—´ ëª¨ë¸ë¡œ ê²°ì¸¡ êµ¬ê°„ì„ ì˜ˆì¸¡í•˜ì—¬ ì±„ì›€

- ì´ìƒì¹˜ ì²˜ë¦¬
	-	í†µê³„ì  ë°©ë²•
    - ì´ë™í‰ê· Â·ì´ë™í‘œì¤€í¸ì°¨ë¡œ ì •ìƒ ë²”ìœ„ë¥¼ ì„¤ì •í•˜ê³  ë²—ì–´ë‚œ ê°’ ì œê±°/ìˆ˜ì •
	-	ê³„ì ˆì„± ê³ ë ¤
    - ê³„ì ˆ/ì£¼ê¸° íŒ¨í„´ì„ ë¶„ë¦¬í•œ ë’¤ ì”ì°¨ê°€ ì¼ì • ê¸°ì¤€ì„ ë„˜ìœ¼ë©´ ì´ìƒì¹˜ë¡œ íŒë‹¨

â€» í•µì‹¬ì€ ì‹œê°„ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ ê²°ì¸¡ì¹˜Â·ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒ

-> ë¬´ì‘ì • ì‚­ì œí•˜ë©´ ì‹œê³„ì—´ íŒ¨í„´ì´ ì™œê³¡ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜ í•„ìš”


## ğŸ”¹ ë¨¸ì‹ ëŸ¬ë‹ ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµÂ·ê²€ì¦
### ë°ì´í„° ì „ì²˜ë¦¬ (Preprocessing)
- ì „ì²˜ë¦¬ ëª©ì 
  - ëª¨ë¸ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë°ì´í„° ìŠ¤ì¼€ì¼ì„ ë§ì¶”ê³  í•™ìŠµìš©(train)/í‰ê°€ìš©(test) ë°ì´í„°ë¥¼ ë¶„ë¦¬
  - ë°ì´í„° ëˆ„ìˆ˜(Data Leakage) ë°©ì§€ ë° ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ ëª©ì 

- ë°ì´í„° ë¶„í•  (`train_test_split`)
  - ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ í‰ê°€ìš©ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜
    ```python
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.3,
        random_state=42,
        stratify=y
    )
    ```
    - ì£¼ìš” íŒŒë¼ë¯¸í„°
      - `test_size`: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (ì˜ˆ: 0.3 â†’ 30%)
      - `random_state`: ë‚œìˆ˜ ê³ ì • (ì¬í˜„ì„± í™•ë³´)
      - `stratify`: í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ (ë¶ˆê· í˜• ë°©ì§€)

- íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (`StandardScaler`)
  - ê° featureë¥¼ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ì •ê·œí™”
  - í•™ìŠµ ë°ì´í„°ë¡œ fit, í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ transformë§Œ ìˆ˜í–‰ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
  - ëª¨ë¸ì˜ ìˆ˜ë ´ ì†ë„ ë° ì•ˆì •ì„± í–¥ìƒ
    ```python
    from sklearn.preprocessing import StandardScaler

    scaler = StandardScaler()
    X_train_norm = scaler.fit_transform(X_train)
    X_test_norm = scaler.transform(X_test)
    ```
    - ì£¼ìš” ë©”ì„œë“œ
      - `fit()`: í‰ê· , í‘œì¤€í¸ì°¨ ê³„ì‚°
      - `transform()`: ê³„ì‚°ëœ ê°’ìœ¼ë¡œ ë³€í™˜ ì ìš©
      - `fit_transform()`: í•œ ë²ˆì— ìˆ˜í–‰(train ì „ìš©)

### ì§€ë„í•™ìŠµ (Supervised Learning)
- ë¡œì§€ìŠ¤í‹± íšŒê·€ (LogisticRegression)
  - ì„ í˜• ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜
  - ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ í†µí•´ í™•ë¥  ì˜ˆì¸¡ ìˆ˜í–‰ (ì´ì§„ ë¶„ë¥˜ìš©)
    ```python
    from sklearn.linear_model import LogisticRegression

    clf = LogisticRegression()
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    ```
    - `fit()`: ëª¨ë¸ í•™ìŠµ
    - `predict()`: ì˜ˆì¸¡ ìˆ˜í–‰
    - `predict_proba()`: í´ë˜ìŠ¤ë³„ í™•ë¥  ë°˜í™˜
    - `ConvergenceWarning`: ë°˜ë³µ íšŸìˆ˜ ë¶€ì¡± ì‹œ ë°œìƒ â†’ max_iter ì¡°ì •

- ì„±ëŠ¥ í‰ê°€ (Evaluation Metrics)
  - ëª¨ë¸ì˜ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë‹¤ê°ë„ë¡œ ê²€ì¦
    ```python
    from sklearn.metrics import confusion_matrix, classification_report

    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    ```
    - ì£¼ìš” ì§€í‘œ
      - ì •ë°€ë„(Precision): `TP / (TP + FP)`
      - ì¬í˜„ìœ¨(Recall): `TP / (TP + FN)`
      - F1-score: ì •ë°€ë„Â·ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· 
      - Accuracy: ì „ì²´ ì¤‘ ë§ì¶˜ ë¹„ìœ¨

- ROC-AUC ë¶„ì„ (`roc_curve`, `roc_auc_score`)
  - ì„ê³„ê°’ ë³€í™”ì— ë”°ë¥¸ TPR(FN ë¹„ìœ¨)â€“FPR ê´€ê³„ ì‹œê°í™”
  - AUCê°’ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¶„ë¥˜ê¸° ì„±ëŠ¥ì´ ìš°ìˆ˜
    ```python
    from sklearn.metrics import roc_curve, roc_auc_score
    import matplotlib.pyplot as plt

    y_score = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test, y_score)
    auc = roc_auc_score(y_test, y_score)

    plt.plot(fpr, tpr, label=f"AUC={auc:.3f}")
    plt.plot([0, 1], [0, 1], '--', label="Random")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend()
    plt.show()
    ```
    ![alt text](image-147.png)

### êµì°¨ ê²€ì¦ (Cross Validation)
í•œ ë²ˆì˜ ë°ì´í„° ë¶„í•  ê²°ê³¼ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ„ì–´ í‰ê·  ì„±ëŠ¥ì„ ê³„ì‚° 

-> ê³¼ì í•© ë°©ì§€

- ì£¼ìš” í•¨ìˆ˜: `cross_val_score`
  - ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ êµì°¨ê²€ì¦ìœ¼ë¡œ ì¸¡ì •
    ```python
    from sklearn.model_selection import cross_val_score

    f1_scores = cross_val_score(
        estimator=clf,
        X=X_train,
        y=y_train,
        cv=5,
        scoring='f1'
    )
    print("Average F1-score (CV):", f1_scores.mean())
    ```

### ì°¨ì›ì¶•ì†Œ (PCA)
- PCAì˜ ëª©ì 
  - ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì£¼ì„±ë¶„ ì¶•ìœ¼ë¡œ íˆ¬ì˜
  - ì •ë³´ ì†ì‹¤ ìµœì†Œí™”í•˜ë©´ì„œ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ì••ì¶•
  - ì‹œê°í™”ë‚˜ ë…¸ì´ì¦ˆ ì œê±°, ë³€ìˆ˜ ê°„ ìƒê´€ì„± ì™„í™”ì— í™œìš©

- ì½”ë“œ
  ```python
  from sklearn.decomposition import PCA

  pca = PCA(n_components=2)
  X_pca = pca.fit_transform(X)
  ```
  - ì£¼ìš” ë©”ì„œë“œ
    - `fit()`: ì£¼ì„±ë¶„ ë°©í–¥ ê³„ì‚°
    - `transform()`: ì›ë³¸ ë°ì´í„°ë¥¼ ìƒˆ ì¶•ìœ¼ë¡œ íˆ¬ì˜
    - `fit_transform()`: í•œ ë²ˆì— ìˆ˜í–‰

### êµ°ì§‘í™” (Clustering)
- K-Means ì•Œê³ ë¦¬ì¦˜ (`KMeans`)
  - ì¤‘ì‹¬ì (centroid)ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”
  - ë¹„ì§€ë„ í•™ìŠµ â†’ ë ˆì´ë¸” ì—†ì´ ë°ì´í„° íŒ¨í„´ íŒŒì•…
  - `n_clusters`: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ì§€ì •

- K-means ì½”ë“œ
  ```python
  from sklearn.cluster import KMeans
  import seaborn as sns
  import matplotlib.pyplot as plt

  kmeans = KMeans(n_clusters=2, random_state=42)
  clusters = kmeans.fit_predict(X_pca)

  fig, ax = plt.subplots(figsize=(12, 4), ncols=2)
  sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette="Set2", ax=ax[0])
  ax[0].set_title("Labels inferred by K-Means")

  sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette="Set2", ax=ax[1])
  ax[1].set_title("Actual Labels on PCA")
  plt.show()
  ```
  ![alt text](image-148.png)
  - ì™¼ìª½: K-Meansê°€ ë¹„ì§€ë„ ë°©ì‹ìœ¼ë¡œ ë‚˜ëˆˆ êµ°ì§‘ ê²°ê³¼
  - ì˜¤ë¥¸ìª½: ì‹¤ì œ í´ë˜ìŠ¤ ë¼ë²¨ ê¸°ë°˜ì˜ ë¶„í¬
  - ë‘ ê·¸ë˜í”„ê°€ ìœ ì‚¬í• ìˆ˜ë¡ K-Meansê°€ ì‹¤ì œ êµ¬ì¡°ë¥¼ ì˜ í¬ì°©í•œ ê²ƒ

### íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (Regression Modeling & Evaluation)
- Linear Regression (ì„ í˜• íšŒê·€)
  - ì…ë ¥ ë³€ìˆ˜ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ íƒ€ê¹ƒ(ì—°ì†í˜• ê°’)ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸
  - ìˆ˜ì‹

    ![alt text](image-149.png)

  - `scikit-learn` ì‚¬ìš© ì‹œ
    - `fit()` : íšŒê·€ê³„ìˆ˜ ğ‘¤, ì ˆí¸ ğ‘ í•™ìŠµ
    - `predict()` : ìƒˆë¡œìš´ ë°ì´í„°ì˜ ì˜ˆì¸¡ê°’ ê³„ì‚°
      ```python
      from sklearn.linear_model import LinearRegression
      reg = LinearRegression()
      reg.fit(X_train, y_train)
      y_pred = reg.predict(X_test)
      ```

- íšŒê·€ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ (Regression Metrics)
  | ì§€í‘œ                                    | ì„¤ëª…                                | í•¨ìˆ˜                                       |
  | ------------------------------------- | --------------------------------- | ---------------------------------------- |
  | **RMSE (Root Mean Squared Error)**    | ì˜¤ì°¨ ì œê³±ì˜ í‰ê· ì— ë£¨íŠ¸ë¥¼ ì”Œìš´ ê°’. í° ì˜¤ì°¨ì— ë¯¼ê°     | `mean_squared_error(..., squared=False)` |
  | **MAE (Mean Absolute Error)**         | ì ˆëŒ€ ì˜¤ì°¨ì˜ í‰ê· . ì´ìƒì¹˜ì— ëœ ë¯¼ê°              | `mean_absolute_error()`                  |
  | **RÂ² (Coefficient of Determination)** | ëª¨ë¸ì´ íƒ€ê¹ƒ ë³€ë™ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€(1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ) | `r2_score()`                             |
  ```python
  from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

  test_rmse = mean_squared_error(y_test, y_pred, squared=False)
  test_mae  = mean_absolute_error(y_test, y_pred)
  test_r2   = r2_score(y_test, y_pred)
  ```

  