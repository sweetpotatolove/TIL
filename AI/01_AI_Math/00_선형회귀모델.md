# NumPy를 이용한 선형 회귀 모델 구현
## 🔹 데이터 정규화 및 준비
### 데이터 전처리의 중요성
모델 학습 이전 단계에서 데이터 스케일링(정규화·표준화) 은 필수적인 과정으로, 입력값의 크기 차이를 줄여 학습 안정성과 수렴 속도를 높임

- **표준화(Standardization)**: 평균을 0, 표준편차를 1로 맞추는 방식
- **정규화(Normalization)**: 특정 범위(보통 0~1)로 값을 조정
- **브로드캐스팅(Broadcasting)**: 서로 다른 형태의 배열 간 연산 자동 확장

### Feature
하나의 데이터 샘플(행, row)은 여러 속성(열, column)을 가짐

-> 이 속성 하나하나를 feature(특성) 라고 부름

- 연속형 변수 (Continuous Variable)
  - 실수형으로 측정되는 연속적인 수치형 데이터
  - ex. 키(cm), 몸무게(kg), 가격(USD), 온도(°C), 시간(초)
  - 특징
    - 값의 스펙트럼이 연속적
    - 수치 연산 및 스케일 조정 필요
  - 전처리 방법
    - 표준화(Standardization): 평균 0, 표준편차 1
    - 정규화(Normalization): `[0,1]` 범위로 조정

- 범주형 변수 (Categorical Variable)
  - 한정된 범주나 등급으로 구분되는 비연속형 데이터
  - 종류
    | 구분                | 설명        | 예시          | 전처리 방식  |
    | ----------------- | --------- | ----------- | ------- |
    | **명목형 (Nominal)** | 순서가 없는 범주 | 성별, 색상, 혈액형 | 원-핫 인코딩 |
    | **순서형 (Ordinal)** | 순서가 있는 범주 | 만족도, 학위 수준  | 순서형 인코딩 |
  - 머신러닝 모델은 숫자형 데이터만 다루므로, 범주형 변수는 반드시 인코딩 과정을 거쳐야 함

- pandas를 이용한 변수 분류
  - `pandas.DataFrame.select_dtypes()`를 사용하면 데이터 타입 기준으로 쉽게 컬럼을 분리 가능
    | 함수                                       | 설명              | 예시                           |
    | ---------------------------------------- | --------------- | ---------------------------- |
    | `df.select_dtypes(include=['number'])`   | 숫자형(연속형) 컬럼 선택  | `continuous_cols`            |
    | `df.select_dtypes(include=['category'])` | 범주형 컬럼 선택       | `categorical_cols`           |
    | `.columns.tolist()`                      | 컬럼명을 리스트 형태로 반환 | `cols = df.columns.tolist()` |
  
  - 예시
    ```python
    from typing import List

    categorical_cols: List[str] = df.select_dtypes(include=['category']).columns.tolist()
    continuous_cols: List[str] = df.select_dtypes(include=['number']).columns.tolist()
    ```

- 상수항(bias term) 추가
  - 모델 학습 시 절편(intercept) 항을 고려하기 위해 입력 데이터에 1을 추가함
    ```python
    X_b = np.c_[np.ones((len(X_norm), 1)), X_norm]
    ```
    - `np.ones((n, 1))` : n개의 행에 1 추가
    - `np.c_[A, B]` : 두 배열을 열 단위로 결합

### 표준화 (Standardization)
데이터의 분포를 평균 0, 표준편차 1로 조정하는 전처리 방법

- 수식: $ Z = X_{\text{norm}}  = \frac{X - \mu}{\sigma}$  
  - 평균(μ) : 각 feature 값들의 평균 
  - 표준편차(σ) : 데이터의 흩어진 정도를 나타내는 척도  
  - $Z = X_{\text{norm}}$ : 표준화된 데이터
  - $X$ : 원본 데이터
  - 목적: 스케일 차이를 제거해 모델 왜곡 방지 및 학습 안정화

- 표준화가 필요한 이유
  1. 스케일 차이 보정
      - feature 간 단위 차이로 인한 가중치 왜곡 방지
      - 만약 서로 다른 단위와 분포를 가진 특성을 그대로 두면, 일부 특성의 값 범위가 너무 커서 학습 과정에서 다른 특성들이 무시될 수 있음

  2. 경사 하강법 가속
      - 표준화로 손실함수의 곡률이 균등해져 빠르게 수렴
      - 만약 경사하강법 수렴 가속 손실 함수 표면의 곡률이 표준화되지 않은 입력으로 인해 왜곡되면,
        - 경사하강법이 어느 방향으로 얼마나 이동해야 할지 예측하기 어려움

  3. 이상치 탐지 용이
      - Z-점수를 통해 평균으로부터의 거리 측정 가능

  4. 거리 기반 알고리즘 안정화
      - KNN, K-means 등에서 공정한 거리 계산 보장
      - 만약 표준화 하지 않아서 특성의 단위나 범위가 다르면 특정 특성이 지나치게 큰 가중치를 가짐

- NumPy를 이용한 표준화 단계
  1. 연속형 데이터 추출
      ```python
      X_raw = df[continuous_cols].values
      ```
  2. 평균과 표준편차 계산
      ```python
      mu = np.mean(X_raw, axis=0)
      std = np.std(X_raw, axis=0)
      ```
  3. 예외 처리
      ```python
      std = np.where(std == 0, 1.0, std)
      ```
  4. 브로드캐스팅을 활용한 표준화
      ```python
      X_norm = (X_raw - mu) / std
      ```
  5. 상수항 추가
      ```python
      X_b = np.c_[np.ones((len(X_norm), 1)), X_norm]
      ```

- 관련 함수
  | 함수                     | 설명           | 예시                                   |
  | ---------------------- | ------------ | ------------------------------------ |
  | `np.mean(X, axis=0)`   | 각 열의 평균 계산   | `mu = np.mean(X_raw, axis=0)`        |
  | `np.std(X, axis=0)`    | 각 열의 표준편차 계산 | `sigma = np.std(X_raw, axis=0)`      |
  | `np.where(cond, a, b)` | 조건에 따라 값 대체  | `std = np.where(std == 0, 1.0, std)` |

- 결과 변수
  - `mu`: 각 열의 평균
  - `std`: 각 열의 표준편차
  - `X_norm`: 표준화된 입력 데이터

### 브로드캐스팅 (Broadcasting)
서로 모양이 다른 배열끼리 연산할 때 자동으로 차원을 확장하는 NumPy의 기능

- 예시
  ```python
  X_norm = (X_raw - mu) / std
  ```
  - `X_raw`: (n, m) 형태의 2D 배열
  - `mu`, `std`: (m,) 형태의 1D 배열
  - → NumPy가 자동으로 차원을 확장하여 각 열에 연산 수행

- 장점
  - 코드 단순화: for문 없이 벡터 연산 가능
  - 속도 향상: 내부적으로 C 기반 연산으로 빠름
  - 메모리 효율: 불필요한 복제 없이 처리


## 🔹 선형대수를 이용한 해법 - 정규방정식(Normal Equation)
### 선형대수 기반 해법의 목적
- 역행렬 계산의 수치적 불안정성을 이해하고,
- `np.linalg.lstsq()`와 SVD 기반 유사역행렬(pseudo-inverse) 로 더 안정적인 해법을 익히는 것

### 정규방정식 (Normal Equation)
정규방정식이란, 회귀 문제에서 모델의 예측값과 실제값 사이의 차이를 측정하는 함수를 말함

-> 회귀 문제의 최적 파라미터(가중치, 회귀계수) $\theta$는 아래와 같이 구할 수 있음

- 수식: $\theta = (X_b^T X_b)^{-1} X_b^T y$
  - $X_b$ : 상수항(1)을 포함한 설계 행렬
  - $Y$ : 실제 타깃값 벡터
  - $\theta$ : 절편과 가중치 벡터

- 역행렬 계산 시 조건수(condition number) 가 큰 행렬에서는 수치적 불안정성이 발생함
  - 역행렬이 존재하는 행렬 A에 대해, X와 X⁻¹의 곱은 단위행렬(I) 
  - 이 역행렬 구하는 것이 어려운 상황들이 많음

- 일반 역행렬 대신 **최소제곱법** 또는 S**VD 기반 유사역행렬**을 이용하는 것이 안전함
  | 방법        | 설명                  | NumPy 함수            |
  | --------- | ------------------- | ------------------- |
  | 정규방정식     | 행렬 역연산으로 θ 계산       | `np.linalg.inv()`   |
  | 최소제곱법     | 잔차 제곱합 최소화로 직접 해 계산 | `np.linalg.lstsq()` |
  | SVD 유사역행렬 | 특이값 역수 기반의 안정적 해    | `np.linalg.svd()`   |

### 선형회귀
각 샘플 $(i)$에 대해 연속형 특성 벡터 $\mathbf{x}^{(i)} \in \mathbb{R}^n$와 목표값 $y^{(i)}$가 있을 때, 선형 회귀는 아래와 같이 표현됨

- 수식: $\hat y^{(i)} = \theta_0 + \theta_1\,x^{(i)}_1 + \cdots + \theta_n\,x^{(i)}_n \quad\Longleftrightarrow\quad \hat y = \mathbf{X}_b\,\boldsymbol{\theta}$
  - $\mathbf{X}_b \in \mathbb{R}^{m \times (n+1)}$ : 각 행이 $[1,\,x_1,\dots,x_n]$인 설계 행렬
    - 즉, 상수항이 포함된 입력 행렬
  - $\boldsymbol{\theta} \in \mathbb{R}^{n+1}$ : 절편 $\theta_0$와 가중치 $\theta_{1:n}$의 벡터
    - 즉, 절편 및 회귀계수 벡터
  - $m$: 샘플 수
  - $n$: 특성 수

- 예시
  - 연속형 변수들에 어떤 선형계수를 곱해서 합하면 `price`를 예측할 수 있는가? 를 보는 것

### 비용 함수 (Cost Function)
모델의 예측값과 실제값의 차이를 측정하는 지표로, 회귀에서는 평균제곱오차(MSE) 를 사용함

- 수식: $J(\boldsymbol{\theta}) = \frac{1}{2m}\sum_{i=1}^m \bigl(\hat y^{(i)} - y^{(i)}\bigr)^2 = \frac{1}{2m}(\mathbf{X}_b\boldsymbol{\theta} - \mathbf{y})^\mathsf{T}(\mathbf{X}_b\boldsymbol{\theta}-\mathbf{y})$

- 올바른 선형계수를 찾기 위해서는, 현재 선형계수가 얼마나 모델을 잘 설명하는가 나타내는 '지표'가 필요함
  - 비용 함수 (Cost function, $J$)를 통해 얼마나 현재 선형계수가 `price`를 잘 예측하는지 확인 가능
- 비용 함수가 0에 가까울수록 현재 선형계수들이 `price`를 잘 예측한다고 볼 수 있음
- 즉, **비용함수를 최소화하는 $\theta$** 를 찾는 것이 목표

### 최적화 조건: 정규방정식 유도
미분이 0이되는 지점은 해당 함수의 값이 변하지 않는 지점으로, 조건에 따라 함수의 최대값 또는 최소값을 나타냄

- 현재 우리의 비용 함수는 변수 $\theta$에 대한 2차 함수이기 때문에 
  - 미분을 0으로 만드는 선형계수를 찾으면 우리는 비용 함수를 최소화하는(즉, `price`를 가장 잘 예측하는) 선형계수를 찾았다고 할 수 있음
  - 비용함수를 $\theta$에 대해 미분하고 0으로 두면
    - $\frac{\partial J}{\partial \boldsymbol{\theta}} = \frac{1}{m}\,\mathbf{X}_b^\mathsf{T}(\mathbf{X}_b\boldsymbol{\theta} - \mathbf{y}) = 0$

    - 따라서, $\mathbf{X}_b^\mathsf{T}\mathbf{X}_b \,\boldsymbol{\theta} = \mathbf{X}_b^\mathsf{T}\mathbf{y}$

    - 이것을 풀면 최적의 파라미터는: $\boldsymbol{\theta} = \bigl(\mathbf{X}_b^\mathsf{T}\mathbf{X}_b\bigr)^{-1}\,\mathbf{X}_b^\mathsf{T}\mathbf{y}$

### 정규방정식 해석
- 장점
  - 단 한 번의 행렬 연산으로 최적 파라미터 계산할 수 있음

- 단점
  - $\mathbf{X}_b^\mathsf{T}\mathbf{X}_b$가 풀랭크(full rank)가 아닐 때 역행렬 존재 불가
    - (어떤 행이 다른 행들의 선형결합(linear combination) 으로 표현된다(독립적이지 못하다)면 그 행은 사실 새로운 정보를 주지 않는다. 즉, 완전히 독립적인 행만 남겼을 때의 개수가 **랭크(rank)**)
    - 즉, 역행렬 존재 조건 필요 (풀랭크)
  - 특성 개수 $n$이 크거나 샘플 수 $m$에 비해 크면 $O(n^3)$ 복잡도로 느림
    - 즉, 계산 복잡도 $O(n^3)$
  - 조건수(condition number)가 크면 작은 노이즈에도 민감함
    - 즉, 조건수에 따른 수치 불안정성
  
### 실습: 정규방정식 선형계수 찾기 
1. 데이터 준비
    ```python
    # 1. X_norm에서 feature인 X와 예측해야하는 대상인 `price` y를 분리
    X: np.ndarray = X_df.drop(labels="price", axis=1).values
    y: np.ndarray = X_df["price"].values

    # 2. 절편항을 추가
    # np.c_ (concat - 가로 방향(왼쪽에서 오른쪽으로)) 행 방향은 r_
    m, n = X_norm.shape
    X_b = np.c_[np.ones((m, 1)), X_norm]        # shape = (m, n+1)
    ```
    - `np.c_` : 열 방향 결합
    - 절편항을 고려하기 위해 1을 추가

2. 정규방정식 계산
    ```python
    # 3. 요소별로 계산을 진행
    # 3-1. 역행렬의 대상인 `X_b^T @ X_b`를 구해 XT_X에 할당
    XT_X  = X_b.T @ X_b                         # shape = (n+1, n+1)
    # 3-2. 역행렬과 곱해지는 X_b^T @ y`를 구해 XT_y에 할당
    XT_y  = X_b.T @ y                           # shape = (n+1, 1)

    # 4. 해 구하기
    theta = np.linalg.inv(XT_X) @ XT_y         # shape = (n+1, 1)
    ```
    - `np.linalg.inv(A)` : 행렬 A의 역행렬 계산
    - `@` : 행렬 곱 연산자

3. 예측 및 평가
    ```python
    # 5. 모델 평가
    # 얻어낸 `theta`와 데이터의 행렬곱을 통하여 예측값인 `y_pred`를 구하고
    # 예측값과 실제값 차이를 나타내는 MSE를 계산
    y_pred = X_b @ theta    # shape = (m, 1)
    mse = np.mean((y_pred - y)**2)

    print("파라미터 θ:\n", theta.flatten())
    print(f"MSE: {mse:.2f}")

    # 파라미터 θ:
    # [-1.78491918e-17  2.66453526e-14 -1.77635684e-15 -4.44089210e-16
    #   1.00000000e+00 -2.48689958e-14 -3.55271368e-15  7.99360578e-15]
    # MSE: 0.00
    ```
    - 실제로 연구를 진행할 때 MSE가 0이 나오는 완벽한 경우는 찾기가 어려움
    - 해당 데이터는 연습용 데이터이다보니 최적화가 잘되는 경우임
    - 보통 MSE만 보고 모델이 얼마나 잘하고 있는지 알 수 있지만, 어디까지나 평가 지표는 하나의 숫자로 모델의 성능을 보고있기 때문에 데이터 별 예측상황을 알기는 어려움
    - 따라서 모델 학습 중간, 후에 예측값에 대한 시각화를 진행해보는 것이 중요함

### 절편(intercept) 항에 1을 추가하는 이유
- 절편 $\theta_0$를 모델에 포함하려면 입력값과 항상 곱해지는 상수항이 필요함
  - 즉, 절편 $\theta_0$를 표현하려면 항상 1과 곱해지는 입력 특성이 필요

- 입력 행렬 첫 열을 모두 1로 채우면 식이 단순해짐(해석이 직관적)
  - $ \hat{y} =  \theta_0 \times 1 + \theta_1x_1 + \theta_nx_n$
  - 따라서 첫 열을 1로 두면, $ \theta_0 \times 1 = \theta_0 $가 되어 $\theta_0$가 순수 절편 역할을 함

- 만약 다른 상수 \(c\)를 쓰면 예측식이 $ \hat{y} = \theta_0 \times c + \sum_{j=1}^n \theta_j x_j $ 가 되고, 
  - $\theta_0$ 자체가 절편이 아니라 $\theta_0 \times c$가 절편이어서 해석이 복잡해짐
  - 관례적으로 1을 사용하면 수식도 간결해지고 수치 조건수(condition number) 악화 없이 안정적!

### 타깃(ex. price) 정규화에 대한 고찰
- 선형 회귀의 해(θ)는 타깃 $y$의 스케일에 선형 비례하므로, 정규화하지 않아도 수학적으로는 문제없음
  - 즉, 정규방정식이나 `np.linalg.lstsq`로 해를 구할 때는 타깃을 스케일링하지 않아도 수학적으로 전혀 문제가 없음

- 다만:
  - $y$의 값이 매우 크면 수치 오차가 커질 수 있음
    - 타깃이 매우 큰 값(예: 수백만 단위)일 경우 컴퓨터 숫자 표현 한계로 인해 드물게 수치 안정성이 조금 떨어질 수 있음
  - 경사하강법 기반 학습에서는 $y$도 표준화 시 수렴이 더 빠름(학습 속도와 수렴 품질 개선 가능)

- 실무에서는 예측 후 반드시 역변환(inverse transform) 을 적용해 원래 단위로 해석함


## 🔹 선형대수를 이용한 해법 - 최소제곱법(Least Squares)로 해 찾기: `np.linalg.lstsq`
### 정규방정식의 한계
- 역행렬 계산의 수치적 불안정성  
  - $X_b^\mathsf{T} X_b$가 거의 특이(singular)에 가까워지면 작은 수치 오차에도 해가 크게 요동침
    - 특이(singular)하다는 건 행렬의 열들이 겹쳐 있어서 역행렬을 만들 수 없는 상태
  - 이런 행렬은 아주 작은 계산 오차에도 결과가 크게 흔들려서, 수치적으로 매우 불안정
  - 열들이 겹쳐 있다는 건, 서로 다른 변수처럼 보여도 사실 같은 정보를 담고 있어서 독립적이지 않은 상태임
  - 고차원·상관관계가 높은(feature들 간에) 데이터일수록 $X_b^\mathsf{T} X_b$의 조건수가 커져 불안정성이 증가함

- 비가역 행렬의 문제
  - 행렬이 완전한 정칙(invertible, 가역)이 아닐 때 해를 구할 수 없음  
  - 피처가 많거나 중복되면 $X_b^\mathsf{T} X_b$가 비가역행렬이 됨(역행렬을 가질 수 없음)
  - 이러한 경우, 역행렬 계산이 불가능해 정규방적식의 해를 계산할 수 없음

### `np.linalg.lstsq`의 원리
SVD(특이값 분해) 기반으로 의사역행렬(pseudo-inverse) 를 계산함

- 정규방정식의 불안정한 역행렬 대신 안정적인 의사역행렬 $X_b^+$ 사용
  - 기존 정규방정식의 $\bigl(X_b^\mathsf{T} X_b\bigr)^{-1} X_b^\mathsf{T}$를 대체
  - $ \theta = X_b^+\,y $
  - 내부적으로 $X_b = UΣV^T$ 형태로 분해
  - $Σ^+$를 이용해 최소제곱 해를 계산함
    
- 수치 안정성이 높아 극단적인 피처 스케일 차이, 상관관계 과다 문제를 완화함

- 특징
  - 안정성: 특이하거나 거의 특이한 행렬에서도 작동
  - 효율성: 역행렬 직접 계산 없이 최소제곱 해 도출
  - 추가정보: 잔차(residual), 랭크(rank), 특이값(singular values) 반환

- 예시 코드
  ```python
  #  `lstsq`를 이용하여 `theta_lstsq` 구하기
  theta_lstsq, residuals, rank, singular_vals = np.linalg.lstsq(X_b, y, rcond=None)

  y_pred = X_b @ theta_lstsq
  mse = np.mean((y_pred - y)**2)

  print("파라미터 θ:\n", theta_lstsq.flatten())
  print(f"MSE: {mse:.2f}")

  # 파라미터 θ:
  # [ 3.25060105e-17 -4.02455846e-16 -1.38777878e-17 -4.60026982e-16
  #   1.00000000e+00  8.13802151e-16 -5.07948718e-16 -8.89045781e-17]
  # MSE: 0.00
  ```
  - `np.linalg.lstsq(X_b, y)`
    - 최소제곱 해 계산
    - 반환값: 
      - `theta_lstsq`: 최소제곱법으로 얻은 가중치 벡터
      - `residuals`: 잔차 제곱합
      - `rank`: 행렬의 랭크(독립된 열 개수)
      - `singular_vals`: 특이값
  - `rcond`
    - 작은 특이값 절단 기준
    - 일반적으로 None

### 최소제곱법 장점
- $X_b^\mathsf{T} X_b$의 역행렬을 직접 계산하지 않고 최소제곱 해 도출 가능
  - 수치 안정성 향상
- 특이(Singular, 거의 특이)한 상황에서도 해를 산출함
  - 즉, 특이 행렬 상황에서도 유사역행렬로 해 구할 수 있음
- 잔차(residuals), 랭크(rank), 특이값(singular values)을 함께 반환하여 모델 진단이 가능


## 🔹 선형대수를 이용한 해법 - SVD를 이용한 최소제곱 해 구하기
### SVD (특잇값 분해, Singular Value Decomposition)
임의의 행렬 $A\in\mathbb{R}^{m\times n}$를 아래와 같이 분해하는 기법

-> SVD는 행렬을 ‘회전-늘리기-회전’ 세 단계로 분리하는 방법임

- $A = U\,\Sigma\,V^\mathsf{T}$
- 구성
  | 구성요소     | 형태                 | 설명                              |
  | -------- | ------------------ | ------------------------------- |
  | $U$      | $m \times m$ 직교 행렬 | 열벡터들은 서로 수직(orthogonal)         |
  | $V$      | $n \times n$ 직교 행렬 | 열벡터들은 서로 수직                     |
  | $\Sigma$ | $m \times n$ 대각 행렬 | 대각 성분이 **특이값(singular values)** |
  | $r$      | rank(A)            | 유효한 특이값의 개수                     |
  - $U\in\mathbb{R}^{m\times r}$와 $V\in\mathbb{R}^{n\times r}$는 직교 행렬 (orthogonal)
  - $\Sigma\in\mathbb{R}^{r\times r}$는 특이값을 대각으로 가진 대각 행렬
  - $r=\mathrm{rank}(A)$

- 특잇값(Singular Value)
  - 행렬이 공간을 얼마나 ‘늘이거나 줄이는지’ 나타내는 크기(스케일)
  - 즉, 어떤 행렬 $A$를 적용했을 때, 벡터의 방향은 바뀌더라도 길이가 얼마나 변하는지를 수치로 보여주는 값
    - ex. 행렬이 어떤 축으로는 데이터를 3배 늘리고, 다른 축으로는 0.5배 줄인다면, 그 “3”과 “0.5”가 바로 특잇값

### 최소제곱 해 구하기
SVD를 이용하면 최소제곱 해를 닫힌 형식으로 구할 수 있음

-> 특이값의 역수를 사용해 의사역행렬 $\Sigma^+$를 정의하면, 

-> 최소제곱 해는 아래와 같이 계산됨

- $x^* = A^+ b = V\,\Sigma^+\,U^\mathsf{T} b$ 로 최소제곱 해 $\min_x\|Ax - b\|_2^2$ 를 얻게됨

  - $\Sigma^+_{ii}=\frac{1}{\Sigma_{ii}}\quad(\Sigma_{ii}>0)$

  - $A^+ = V\,\Sigma^+U^\mathsf{T}$

### NumPy에서의 SVD 구현
- 예시 코드
  ```python
  # 1. SVD를 수행하여 U, S, V^T 구하기
  U, S, Vt = np.linalg.svd(X_b, full_matrices=False)

  # 2. Pseudo-inverse S_plus 구하기
  S_plus = np.diag(1.0 / S)

  # 3. 앞서 배운 수식을 참고하여 `theta_svd` 구하기
  theta_svd = Vt.T @ S_plus @ U.T @ y

  # 4. 모델 평가
  # 4.1 얻어낸 `theta_svd`와 데이터의 행렬곱을 통하여 예측값인 `y_pred`를 구하기
  y_pred = X_b @ theta_svd
  # 4.2 예측값과 실제값 차이를 나타내는 MSE 계산
  mse = np.mean((y_pred - y)**2)

  print("파라미터 θ:\n", theta_svd.flatten())
  print(f"MSE: {mse:.2f}")
  
  # 파라미터 θ:
  # [ 1.96023753e-16  8.55218674e-16 -3.29380620e-16 -2.45029691e-17
  #   1.00000000e+00 -1.64798730e-17 -1.84748050e-16  3.44776291e-17]
  # MSE: 0.00
  ```
  - `np.linalg.svd(A, full_matrices=False)`
    - SVD 수행
    - $U, S, V^\mathsf{T}$ 반환
  
  - `np.diag(1.0/S)`
    - 대각행렬의 역수로 의사역행렬 구성

### SVD 접근의 장점
`np.linalg.lstsq`와 달리 직접 SVD (`np.linalg.svd`)를 사용하면  

- 특이값 절단(threshold) 을 통해 노이즈 제어 가능
- 정규화(regularization)나 저차원 근사(rank-truncated) 해법 설계가 수월해짐
- $U, \Sigma, V^\mathsf{T}$ 를 활용해
  - 데이터 차원 축소 (PCA)
  - 랭크 근사(rank truncation)
  - 모델 안정성 진단 가능

- 비교 표
  | 비교     | 정규방정식          | lstsq / SVD  |
  | ------ | -------------- | ------------ |
  | 계산 안정성 | 낮음 (역행렬 직접 계산) | 높음 (SVD 사용)  |
  | 계산 복잡도 | (O(n^3))       | 효율적          |
  | 해석 가능성 | 단순             | 특이값 기반 분석 가능 |


## 🔹 경사 하강법과 손실 계산
### 경사 하강법을 사용해야 하는 이유
닫힌 해법(정규방정식, SVD 등)은 수학적으로 정확하고 한번에 해를 구한다는 장점아 있지만,

- $X_b^\mathsf{T}X_b$의 역행렬 계산( $O(n^3)$ ) 또는 전체 행렬 분해가 필요
- 특성 수($n$)나 데이터 수 ($m$)가 커지면 메모리와 연산량이 **급격히** 증가함
- 정칙성이 깨지면(상관 피처 많을 때) 수치적으로 불안정

- 경사 하강법
  - 정확한 해를 한 번에 구하기보단, 조금씩 내려가며 근사적인 최적의 해를 찾는 방식
  - 종류
    | 방식                      | 특징                     |
    | ----------------------- | ---------------------- |
    | **Batch GD**            | 전체 데이터를 한 번에 사용        |
    | **Mini-Batch GD**       | 일부 샘플(batch)만 사용       |
    | **SGD (Stochastic GD)** | 1개 샘플씩 갱신 (노이즈 크지만 빠름) |
  
  - 위 방식으로 데이터를 나누어 처리함으로써 **메모리 제약을 완화**하고 대규모 데이터에 유연하게 대응할 수 있음
  - 정규화 항이나 비선형 손실 함수에도 쉽게 확장 가능해 딥러닝 등 넓은 영역에서 핵심 기법으로 쓰임

### 손실 함수(MSE)
선형 회귀의 예측값은 $\hat y = X_b\,\theta$

-> 예측값과 실제 $y$ 사이의 평균 제곱 오차(MSE)는 정규방정식에서 다룬 손실함수와 동일함

-> $\mathrm{J(\theta)} = \frac{1}{m}\sum_{i=1}^{m}\bigl(\hat y_i - y_i\bigr)^2$

- MSE는 모델이 "실제값과 얼마나 차이 나는지"를 정량적으로 표현함
- 학습 반복마다 MSE를 계산해 `loss_history`에 저장하면  
  - **수렴 과정을 시각화**: 손실이 줄어드는 패턴 확인 가능
  - **학습률 스케줄링(조정)**: $α$ 가 너무 크면 진동, 너무 작으면 느림
  - **조기 종료(early stopping)**: 목표 MSE 이하 시 학습 중단
  - 으로 활용 가능  


### 그래디언트 계산 및 파라미터 업데이트
손실을 줄이려면 "θ를 얼마나, 어느 방향으로 바꿔야 할까?"를 알아야 함

-> 이를 알려주는 게 **그래디언트(gradient)**

- 선형 회귀 MSE의 파라미터 $\theta$에 대한 그래디언트
  - $\nabla_\theta = \frac{2}{m}\,X_b^\mathsf{T}\bigl(X_b\,\theta - y\bigr)$
  - $\theta \leftarrow \theta - \alpha\,\nabla_\theta$
    - 학습률 $\alpha$ 를 사용한 업데이트 식은 현재 θ에서 손실이 가장 빠르게 커지는 방향으로 보내기 위해
    - 기울기가 가리키는 손실 상승 방향의 반대로 이동하면서 손실을 점진적으로 줄여감
  
  - 기호 설명
    - $\alpha$: 학습률(learning rate) -> 한 번에 얼마나 이동할지
    - $\nabla_\theta$ = 기울기 벡터
    - $\theta \leftarrow$ ... = 새로운 가중치로 갱신
    - $- \alpha\,\nabla_\theta$ = 손실을 줄이는 방향으로 한 걸음 이동
  
### 코드 흐름 분석
1. 초기 파라미터 설정
    ```python
    from tqdm import tqdm

    theta        = np.zeros(n+1)   # 1️⃣ 파라미터 초기화
    alpha        = 0.01            # 2️⃣ 학습률 설정
    iterations   = 1000            # 3️⃣ 반복 횟수
    loss_history = []              # 손실 기록 리스트
    ```
    - $\theta$를 0으로 두는 이유
      - 선형회귀는 비대칭 손실이 없기 때문에 가능
    - 딥러닝 등에서는 일반적으로 무작위 초기화 사용

2. 경사하강법 반복 루프
    ```python
    for i in tqdm(range(iterations)):
        # 현재 매개변수 `theta`에 의한 예측값 `y_pred`를 계산 -> 예측값 계산
        y_pred = (X_b @ theta).flatten()

        # 예측해야하는 실제값 `y`와의 차 `error`를 계산 -> 예측 오차
        error  = y_pred - y            

        # `error`를 기반으로 `mse`를 계산 -> 손실 계산
        mse    = np.mean(error**2)        

        # 위에서 공부한 식을 토대로 `gradient`를 계산 -> 기울기 계산
        gradient = (2/m) * X_b.T @ error 

        # 얻어낸 `gradient`를 기반으로 `theta`를 업데이트 -> 파라미터 갱신
        theta -= alpha * gradient   
    
        loss_history.append(mse)
    ```
    - 순서 핵심: `예측 → 오차 → 손실 → 그래디언트 → 파라미터 업데이트`
    - `@` 연산자: 행렬 곱(dot product)

3. 학습 결과
    ```python
    print("최종 θ:", theta.flatten())
    print("최종 MSE:", loss_history[-1])
    plt.plot(loss_history)

    # 최종 θ: [-6.42699844e-17  1.37553160e-01 -4.91584831e-03 -5.51089520e-03
    #           9.32494793e-01 -1.70028629e-02 -2.69295815e-02 -2.96354905e-02]
    # 최종 MSE: 0.000854141507526023
    ```
    - 손실 감소 그래프를 통해 수렴 여부 확인

### 💭 고민해볼 점
| 질문                                | 해설                                                                                         |
| --------------------------------- | ------------------------------------------------------------------------------------------ |
| **초기값 θ, α, 반복 수는 이렇게 고정해도 되나요?** | 정답은 없습니다. 경험적으로 조정해야 합니다. 특히 α(학습률)는 너무 크면 발산, 너무 작으면 느리게 수렴합니다.                           |
| **iteration 전에 멈출 수도 있지 않나요?**    | 맞습니다. MSE가 충분히 작아졌다면 반복 중단(early stopping) 가능합니다.                                          |
| **1차 미분만으로 충분할까?**                | 단순 회귀에서는 전역 최적해 보장. 그러나 복잡한 손실함수에서는 모멘텀, Adam 등의 고급 옵티마이저가 필요합니다.                          |
| **실무에서 매번 이렇게 직접 짜나요?**           | 아니요. 실무에서는 `scikit-learn`, `PyTorch`, `TensorFlow`의 내장 옵티마이저 사용. 수학적 이해를 위해 수작업 구현을 연습합니다. |
| **데이터가 많을 때는?**                   | 전체 배치 연산은 부담이 큽니다. Mini-batch나 SGD 방식으로 일부 데이터만 사용해 갱신합니다.                                 |
| **새로운 데이터에서도 잘 될까?**              | 학습 데이터에만 맞춘다면 과적합 위험이 있습니다. Train/Validation 분리 후 일반화 성능을 검증해야 합니다.                        |

※ Iteration이나 학습률과 같이 '학습되지 않는' 값들을 **초매개변수 (Hyperparameters)** 라고 부름


### 실습: 경사 하강법 개선하기
고민해볼 점에 대한 해답을 반영하여 보완해보자

```python
def train_linear_regression_sequential(
    X_b, y,
    alpha=0.01,
    epochs=100,
    tol=None,
    batch_size=None,
    accumulate_steps=1,
):
    """
    Shuffling 없이 순차적으로 미니배치를 구성하는 선형 회귀 학습 함수
    X: (m, n) 입력 피처
    y: (m,) 타깃
    alpha: 학습률
    epochs: 전체 반복 횟수
    tol: MSE 기준 조기 종료 임계값
    batch_size: 미니배치 크기 (None이면 전체 배치)
    accumulate_steps: Gradient Accumulation할 배치 수
    """

    m, n = X_b.shape

    # 파라미터 초기화
    theta = np.zeros(n)
    history = []
    bs = batch_size or m   # batch_size 미지정 시 전체 배치
                           # 즉, 이렇게 설정하면 batch_size를 주지 않았을 때 전체 데이터 미분을 사용함

    for epoch in range(1, epochs + 1):
        grad_accum = np.zeros_like(theta)
        accum_count = 0

        # Mini-batch 단위 학습
        for start in range(0, m, bs):
            # Mini-batch 구성하기
            # 현재 batch 내의 index 시작 위치를 나타내는 `start` 변수를 기반으로 batch를 구성
            end = min(start + bs, m)
            X_batch = X_b[start:end]
            y_batch = y[start:end]

            # 예측, 오차, 그래디언트 계산
            y_pred = X_batch.dot(theta)     # 현재 batch의 prediction을 계산하기
            error  = y_pred - y_batch       # prediction과 ground truth 사이 오차 계산하기
            grad   = (2 / X_batch.shape[0]) * X_batch.T.dot(error)  # 오차를 기반으로 미분 계산하기

            grad_accum += grad
            accum_count += 1

            # 누적 기울기 업데이트 (Gradient Accumulation)
            if accum_count == accumulate_steps or end == m:
                theta -= alpha * (grad_accum / accum_count)
                grad_accum[:] = 0
                accum_count = 0

        # Epoch 단위 MSE 기록
          # batch 별로 MSE를 계산할 수 있지만, 이는 전체 데이터에 대한 것이 아니기 때문에
          # 누적된 전체 데이터의 MSE를 계산해줌
          # 이는 batch loop를 돌 때 MSE 결과를 저장하여 루프 후 평균을 내거나
          # 전체 데이터에 대해 다시 추론을 하는 방식으로 진행할 수 있음
          # `print`문을 통하여 중간중간 학습결과를 계속 확인하는 것이 중요
        mse = np.mean((X_b.dot(theta) - y) ** 2)
        history.append(mse)
        print(f"Epoch {epoch:3d}/{epochs}    MSE: {mse:.6f}")

        # Early Stopping 조건
        # 현재 MSE과 tol을 비교하여 학습을 진행하거나 Loop를 깸
        if tol is not None and mse < tol:
            print(f"-> Early stopping at epoch {epoch}, MSE={mse:.6f}")
            break

    return theta, history
```
```python
theta_est, loss_hist = train_linear_regression_sequential(
    X_b, y,
    alpha=0.01,
    epochs=100,
    tol=1e-4,
    batch_size=32,
    accumulate_steps=2
)
print("추정된 θ:", theta_est)

# Epoch   1/100    MSE: 0.011780
# Epoch   2/100    MSE: 0.001087
# Epoch   3/100    MSE: 0.000188
# Epoch   4/100    MSE: 0.000063
# -> Early stopping at epoch 4, MSE=0.000063
# 추정된 θ: [-2.01753996e-03 -5.94139703e-03  1.68627498e-03 -8.50616605e-04
#   9.89263082e-01  4.42230174e-02 -1.57787694e-02 -1.28863573e-02]
```
1. 초매개변수(Hyperparameters)
    - α, epochs, batch_size는 학습 중에 자동으로 최적화되지 않음 → 탐색이 필요
    - → 이후 Grid Search, Random Search, Bayesian Optimization 등으로 자동 조정 가능

2. Mini-batch Gradient Descent
    - 전체 데이터를 한 번에 학습하지 않고, 일부 샘플(batch) 단위로 나누어 학습하는 방식
    - 각 미니배치마다 기울기를 계산하고, 평균을 내어 파라미터를 업데이트
    - 계산 효율성과 수렴 안정성 사이의 균형을 맞춤
      | 방식            | 데이터 사용량           | 특징                 |
      | ------------- | ----------------- | ------------------ |
      | Batch GD      | 전체 데이터(m개 샘플)     | 정확하지만 느림, 메모리 부담 큼 |
      | Stochastic GD | 1개 샘플             | 빠르지만 노이즈 큼         |
      | Mini-batch GD | k개 샘플 (보통 32~256) | **속도와 안정성의 균형**    |

3. Early Stopping 조건
    - 손실이 plateau(정체 구간)에 들어가면 종료
      - MSE가 이미 0이거나 혹은 0에 가까운 값에 수렴했다면 굳이 학습 더 진행할 필요 없음
    - 너무 이르게 멈추면 underfitting, 너무 늦으면 overfitting

4. Mini-batch 크기
    - batch 크기가 작을수록 gradient 추정이 noisy하지만 빠름
    - batch 크기가 클수록 안정적이지만 느림

5. Gradient Accumulation
    - 메모리가 작은 환경에서 큰 batch 효과를 흉내내는 기법
      - 메모리나 하드웨어 제약으로 한 번에 큰 배치 크기를 사용할 수 없을 때,
      - 작은 미니배치들의 기울기(Gradient)를 모아서(Accumulation) 한 번에 파라미터를 갱신하는 기법
    - 분산 학습에서도 통신 효율을 높이는 데 사용

※ 대부분의 딥러닝 프레임워크(PyTorch, TensorFlow)는 이 모든 기능을 내장함

-> 하지만 “직접 구현” 경험은 학습률, 수렴, 안정성의 본질을 이해하게 해줌


# 로지스틱 회귀 구현
## 