# NumPy를 이용한 선형 회귀 모델 구현
## 🔹 데이터 정규화 및 준비
### 데이터 전처리의 중요성
모델 학습 이전 단계에서 데이터 스케일링(정규화·표준화) 은 필수적인 과정으로, 입력값의 크기 차이를 줄여 학습 안정성과 수렴 속도를 높임

- **표준화(Standardization)**: 평균을 0, 표준편차를 1로 맞추는 방식
- **정규화(Normalization)**: 특정 범위(보통 0~1)로 값을 조정
- **브로드캐스팅(Broadcasting)**: 서로 다른 형태의 배열 간 연산 자동 확장

### Feature
하나의 데이터 샘플(행, row)은 여러 속성(열, column)을 가짐

-> 이 속성 하나하나를 feature(특성) 라고 부름

- 연속형 변수 (Continuous Variable)
  - 실수형으로 측정되는 연속적인 수치형 데이터
  - ex. 키(cm), 몸무게(kg), 가격(USD), 온도(°C), 시간(초)
  - 특징
    - 값의 스펙트럼이 연속적
    - 수치 연산 및 스케일 조정 필요
  - 전처리 방법
    - 표준화(Standardization): 평균 0, 표준편차 1
    - 정규화(Normalization): `[0,1]` 범위로 조정

- 범주형 변수 (Categorical Variable)
  - 한정된 범주나 등급으로 구분되는 비연속형 데이터
  - 종류
    | 구분                | 설명        | 예시          | 전처리 방식  |
    | ----------------- | --------- | ----------- | ------- |
    | **명목형 (Nominal)** | 순서가 없는 범주 | 성별, 색상, 혈액형 | 원-핫 인코딩 |
    | **순서형 (Ordinal)** | 순서가 있는 범주 | 만족도, 학위 수준  | 순서형 인코딩 |
  - 머신러닝 모델은 숫자형 데이터만 다루므로, 범주형 변수는 반드시 인코딩 과정을 거쳐야 함

- pandas를 이용한 변수 분류
  - `pandas.DataFrame.select_dtypes()`를 사용하면 데이터 타입 기준으로 쉽게 컬럼을 분리 가능
    | 함수                                       | 설명              | 예시                           |
    | ---------------------------------------- | --------------- | ---------------------------- |
    | `df.select_dtypes(include=['number'])`   | 숫자형(연속형) 컬럼 선택  | `continuous_cols`            |
    | `df.select_dtypes(include=['category'])` | 범주형 컬럼 선택       | `categorical_cols`           |
    | `.columns.tolist()`                      | 컬럼명을 리스트 형태로 반환 | `cols = df.columns.tolist()` |
  
  - 예시
    ```python
    from typing import List

    categorical_cols: List[str] = df.select_dtypes(include=['category']).columns.tolist()
    continuous_cols: List[str] = df.select_dtypes(include=['number']).columns.tolist()
    ```

- 상수항(bias term) 추가
  - 모델 학습 시 절편(intercept) 항을 고려하기 위해 입력 데이터에 1을 추가함
    ```python
    X_b = np.c_[np.ones((len(X_norm), 1)), X_norm]
    ```
    - `np.ones((n, 1))` : n개의 행에 1 추가
    - `np.c_[A, B]` : 두 배열을 열 단위로 결합

### 표준화 (Standardization)
데이터의 분포를 평균 0, 표준편차 1로 조정하는 전처리 방법

- 수식: $ Z = X_{\text{norm}}  = \frac{X - \mu}{\sigma}$  
  - 평균(μ) : 각 feature 값들의 평균 
  - 표준편차(σ) : 데이터의 흩어진 정도를 나타내는 척도  
  - $Z = X_{\text{norm}}$ : 표준화된 데이터
  - $X$ : 원본 데이터
  - 목적: 스케일 차이를 제거해 모델 왜곡 방지 및 학습 안정화

- 표준화가 필요한 이유
  1. 스케일 차이 보정
      - feature 간 단위 차이로 인한 가중치 왜곡 방지
      - 만약 서로 다른 단위와 분포를 가진 특성을 그대로 두면, 일부 특성의 값 범위가 너무 커서 학습 과정에서 다른 특성들이 무시될 수 있음

  2. 경사 하강법 가속
      - 표준화로 손실함수의 곡률이 균등해져 빠르게 수렴
      - 만약 경사하강법 수렴 가속 손실 함수 표면의 곡률이 표준화되지 않은 입력으로 인해 왜곡되면,
        - 경사하강법이 어느 방향으로 얼마나 이동해야 할지 예측하기 어려움

  3. 이상치 탐지 용이
      - Z-점수를 통해 평균으로부터의 거리 측정 가능

  4. 거리 기반 알고리즘 안정화
      - KNN, K-means 등에서 공정한 거리 계산 보장
      - 만약 표준화 하지 않아서 특성의 단위나 범위가 다르면 특정 특성이 지나치게 큰 가중치를 가짐

- NumPy를 이용한 표준화 단계
  1. 연속형 데이터 추출
      ```python
      X_raw = df[continuous_cols].values
      ```
  2. 평균과 표준편차 계산
      ```python
      mu = np.mean(X_raw, axis=0)
      std = np.std(X_raw, axis=0)
      ```
  3. 예외 처리
      ```python
      std = np.where(std == 0, 1.0, std)
      ```
  4. 브로드캐스팅을 활용한 표준화
      ```python
      X_norm = (X_raw - mu) / std
      ```
  5. 상수항 추가
      ```python
      X_b = np.c_[np.ones((len(X_norm), 1)), X_norm]
      ```

- 관련 함수
  | 함수                     | 설명           | 예시                                   |
  | ---------------------- | ------------ | ------------------------------------ |
  | `np.mean(X, axis=0)`   | 각 열의 평균 계산   | `mu = np.mean(X_raw, axis=0)`        |
  | `np.std(X, axis=0)`    | 각 열의 표준편차 계산 | `sigma = np.std(X_raw, axis=0)`      |
  | `np.where(cond, a, b)` | 조건에 따라 값 대체  | `std = np.where(std == 0, 1.0, std)` |

- 결과 변수
  - `mu`: 각 열의 평균
  - `std`: 각 열의 표준편차
  - `X_norm`: 표준화된 입력 데이터

### 브로드캐스팅 (Broadcasting)
서로 모양이 다른 배열끼리 연산할 때 자동으로 차원을 확장하는 NumPy의 기능

- 예시
  ```python
  X_norm = (X_raw - mu) / std
  ```
  - `X_raw`: (n, m) 형태의 2D 배열
  - `mu`, `std`: (m,) 형태의 1D 배열
  - → NumPy가 자동으로 차원을 확장하여 각 열에 연산 수행

- 장점
  - 코드 단순화: for문 없이 벡터 연산 가능
  - 속도 향상: 내부적으로 C 기반 연산으로 빠름
  - 메모리 효율: 불필요한 복제 없이 처리


## 🔹 선형대수를 이용한 해법 - 정규방정식(Normal Equation)
### 선형대수 기반 해법의 목적
- 역행렬 계산의 수치적 불안정성을 이해하고,
- `np.linalg.lstsq()`와 SVD 기반 유사역행렬(pseudo-inverse) 로 더 안정적인 해법을 익히는 것

### 정규방정식 (Normal Equation)
정규방정식이란, 회귀 문제에서 모델의 예측값과 실제값 사이의 차이를 측정하는 함수를 말함

-> 회귀 문제의 최적 파라미터(가중치, 회귀계수) $\theta$는 아래와 같이 구할 수 있음

- 수식: $\theta = (X_b^T X_b)^{-1} X_b^T y$
  - $X_b$ : 상수항(1)을 포함한 설계 행렬
  - $Y$ : 실제 타깃값 벡터
  - $\theta$ : 절편과 가중치 벡터

- 역행렬 계산 시 조건수(condition number) 가 큰 행렬에서는 수치적 불안정성이 발생함
  - 역행렬이 존재하는 행렬 A에 대해, X와 X⁻¹의 곱은 단위행렬(I) 
  - 이 역행렬 구하는 것이 어려운 상황들이 많음

- 일반 역행렬 대신 **최소제곱법** 또는 S**VD 기반 유사역행렬**을 이용하는 것이 안전함
  | 방법        | 설명                  | NumPy 함수            |
  | --------- | ------------------- | ------------------- |
  | 정규방정식     | 행렬 역연산으로 θ 계산       | `np.linalg.inv()`   |
  | 최소제곱법     | 잔차 제곱합 최소화로 직접 해 계산 | `np.linalg.lstsq()` |
  | SVD 유사역행렬 | 특이값 역수 기반의 안정적 해    | `np.linalg.svd()`   |

### 선형회귀
각 샘플 $(i)$에 대해 연속형 특성 벡터 $\mathbf{x}^{(i)} \in \mathbb{R}^n$와 목표값 $y^{(i)}$가 있을 때, 선형 회귀는 아래와 같이 표현됨

- 수식: $\hat y^{(i)} = \theta_0 + \theta_1\,x^{(i)}_1 + \cdots + \theta_n\,x^{(i)}_n \quad\Longleftrightarrow\quad \hat y = \mathbf{X}_b\,\boldsymbol{\theta}$
  - $\mathbf{X}_b \in \mathbb{R}^{m \times (n+1)}$ : 각 행이 $[1,\,x_1,\dots,x_n]$인 설계 행렬
    - 즉, 상수항이 포함된 입력 행렬
  - $\boldsymbol{\theta} \in \mathbb{R}^{n+1}$ : 절편 $\theta_0$와 가중치 $\theta_{1:n}$의 벡터
    - 즉, 절편 및 회귀계수 벡터
  - $m$: 샘플 수
  - $n$: 특성 수

- 예시
  - 연속형 변수들에 어떤 선형계수를 곱해서 합하면 `price`를 예측할 수 있는가? 를 보는 것

### 비용 함수 (Cost Function)
모델의 예측값과 실제값의 차이를 측정하는 지표로, 회귀에서는 평균제곱오차(MSE) 를 사용함

- 수식: $J(\boldsymbol{\theta}) = \frac{1}{2m}\sum_{i=1}^m \bigl(\hat y^{(i)} - y^{(i)}\bigr)^2 = \frac{1}{2m}(\mathbf{X}_b\boldsymbol{\theta} - \mathbf{y})^\mathsf{T}(\mathbf{X}_b\boldsymbol{\theta}-\mathbf{y})$

- 올바른 선형계수를 찾기 위해서는, 현재 선형계수가 얼마나 모델을 잘 설명하는가 나타내는 '지표'가 필요함
  - 비용 함수 (Cost function, $J$)를 통해 얼마나 현재 선형계수가 `price`를 잘 예측하는지 확인 가능
- 비용 함수가 0에 가까울수록 현재 선형계수들이 `price`를 잘 예측한다고 볼 수 있음
- 즉, **비용함수를 최소화하는 $\theta$** 를 찾는 것이 목표

### 최적화 조건: 정규방정식 유도
미분이 0이되는 지점은 해당 함수의 값이 변하지 않는 지점으로, 조건에 따라 함수의 최대값 또는 최소값을 나타냄

- 현재 우리의 비용 함수는 변수 $\theta$에 대한 2차 함수이기 때문에 
  - 미분을 0으로 만드는 선형계수를 찾으면 우리는 비용 함수를 최소화하는(즉, `price`를 가장 잘 예측하는) 선형계수를 찾았다고 할 수 있음
  - 비용함수를 $\theta$에 대해 미분하고 0으로 두면
    - $\frac{\partial J}{\partial \boldsymbol{\theta}} = \frac{1}{m}\,\mathbf{X}_b^\mathsf{T}(\mathbf{X}_b\boldsymbol{\theta} - \mathbf{y}) = 0$

    - 따라서, $\mathbf{X}_b^\mathsf{T}\mathbf{X}_b \,\boldsymbol{\theta} = \mathbf{X}_b^\mathsf{T}\mathbf{y}$

    - 이것을 풀면 최적의 파라미터는: $\boldsymbol{\theta} = \bigl(\mathbf{X}_b^\mathsf{T}\mathbf{X}_b\bigr)^{-1}\,\mathbf{X}_b^\mathsf{T}\mathbf{y}$

### 정규방정식 해석
- 장점
  - 단 한 번의 행렬 연산으로 최적 파라미터 계산할 수 있음

- 단점
  - $\mathbf{X}_b^\mathsf{T}\mathbf{X}_b$가 풀랭크(full rank)가 아닐 때 역행렬 존재 불가
    - (어떤 행이 다른 행들의 선형결합(linear combination) 으로 표현된다(독립적이지 못하다)면 그 행은 사실 새로운 정보를 주지 않는다. 즉, 완전히 독립적인 행만 남겼을 때의 개수가 **랭크(rank)**)
    - 즉, 역행렬 존재 조건 필요 (풀랭크)
  - 특성 개수 $n$이 크거나 샘플 수 $m$에 비해 크면 $O(n^3)$ 복잡도로 느림
    - 즉, 계산 복잡도 $O(n^3)$
  - 조건수(condition number)가 크면 작은 노이즈에도 민감함
    - 즉, 조건수에 따른 수치 불안정성
  
### 실습: 정규방정식 선형계수 찾기 
1. 데이터 준비
    ```python
    # 1. X_norm에서 feature인 X와 예측해야하는 대상인 `price` y를 분리
    X: np.ndarray = X_df.drop(labels="price", axis=1).values
    y: np.ndarray = X_df["price"].values

    # 2. 절편항을 추가
    # np.c_ (concat - 가로 방향(왼쪽에서 오른쪽으로)) 행 방향은 r_
    m, n = X_norm.shape
    X_b = np.c_[np.ones((m, 1)), X_norm]        # shape = (m, n+1)
    ```
    - `np.c_` : 열 방향 결합
    - 절편항을 고려하기 위해 1을 추가

2. 정규방정식 계산
    ```python
    # 3. 요소별로 계산을 진행
    # 3-1. 역행렬의 대상인 `X_b^T @ X_b`를 구해 XT_X에 할당
    XT_X  = X_b.T @ X_b                         # shape = (n+1, n+1)
    # 3-2. 역행렬과 곱해지는 X_b^T @ y`를 구해 XT_y에 할당
    XT_y  = X_b.T @ y                           # shape = (n+1, 1)

    # 4. 해 구하기
    theta = np.linalg.inv(XT_X) @ XT_y         # shape = (n+1, 1)
    ```
    - `np.linalg.inv(A)` : 행렬 A의 역행렬 계산
    - `@` : 행렬 곱 연산자

3. 예측 및 평가
    ```python
    # 5. 모델 평가
    # 얻어낸 `theta`와 데이터의 행렬곱을 통하여 예측값인 `y_pred`를 구하고
    # 예측값과 실제값 차이를 나타내는 MSE를 계산
    y_pred = X_b @ theta    # shape = (m, 1)
    mse = np.mean((y_pred - y)**2)

    print("파라미터 θ:\n", theta.flatten())
    print(f"MSE: {mse:.2f}")

    # 파라미터 θ:
    # [-1.78491918e-17  2.66453526e-14 -1.77635684e-15 -4.44089210e-16
    #   1.00000000e+00 -2.48689958e-14 -3.55271368e-15  7.99360578e-15]
    # MSE: 0.00
    ```
    - 실제로 연구를 진행할 때 MSE가 0이 나오는 완벽한 경우는 찾기가 어려움
    - 해당 데이터는 연습용 데이터이다보니 최적화가 잘되는 경우임
    - 보통 MSE만 보고 모델이 얼마나 잘하고 있는지 알 수 있지만, 어디까지나 평가 지표는 하나의 숫자로 모델의 성능을 보고있기 때문에 데이터 별 예측상황을 알기는 어려움
    - 따라서 모델 학습 중간, 후에 예측값에 대한 시각화를 진행해보는 것이 중요함

### 절편(intercept) 항에 1을 추가하는 이유
- 절편 $\theta_0$를 모델에 포함하려면 입력값과 항상 곱해지는 상수항이 필요함
  - 즉, 절편 $\theta_0$를 표현하려면 항상 1과 곱해지는 입력 특성이 필요

- 입력 행렬 첫 열을 모두 1로 채우면 식이 단순해짐(해석이 직관적)
  - $ \hat{y} =  \theta_0 \times 1 + \theta_1x_1 + \theta_nx_n$
  - 따라서 첫 열을 1로 두면, $ \theta_0 \times 1 = \theta_0 $가 되어 $\theta_0$가 순수 절편 역할을 함

- 만약 다른 상수 \(c\)를 쓰면 예측식이 $ \hat{y} = \theta_0 \times c + \sum_{j=1}^n \theta_j x_j $ 가 되고, 
  - $\theta_0$ 자체가 절편이 아니라 $\theta_0 \times c$가 절편이어서 해석이 복잡해짐
  - 관례적으로 1을 사용하면 수식도 간결해지고 수치 조건수(condition number) 악화 없이 안정적!

### 타깃(ex. price) 정규화에 대한 고찰
- 선형 회귀의 해(θ)는 타깃 $y$의 스케일에 선형 비례하므로, 정규화하지 않아도 수학적으로는 문제없음
  - 즉, 정규방정식이나 `np.linalg.lstsq`로 해를 구할 때는 타깃을 스케일링하지 않아도 수학적으로 전혀 문제가 없음

- 다만:
  - $y$의 값이 매우 크면 수치 오차가 커질 수 있음
    - 타깃이 매우 큰 값(예: 수백만 단위)일 경우 컴퓨터 숫자 표현 한계로 인해 드물게 수치 안정성이 조금 떨어질 수 있음
  - 경사하강법 기반 학습에서는 $y$도 표준화 시 수렴이 더 빠름(학습 속도와 수렴 품질 개선 가능)

- 실무에서는 예측 후 반드시 역변환(inverse transform) 을 적용해 원래 단위로 해석함


## 🔹 선형대수를 이용한 해법 - 최소제곱법(Least Squares)로 해 찾기: `np.linalg.lstsq`
### 정규방정식의 한계
- 역행렬 계산의 수치적 불안정성  
  - $X_b^\mathsf{T} X_b$가 거의 특이(singular)에 가까워지면 작은 수치 오차에도 해가 크게 요동침
    - 특이(singular)하다는 건 행렬의 열들이 겹쳐 있어서 역행렬을 만들 수 없는 상태
  - 이런 행렬은 아주 작은 계산 오차에도 결과가 크게 흔들려서, 수치적으로 매우 불안정
  - 열들이 겹쳐 있다는 건, 서로 다른 변수처럼 보여도 사실 같은 정보를 담고 있어서 독립적이지 않은 상태임
  - 고차원·상관관계가 높은(feature들 간에) 데이터일수록 $X_b^\mathsf{T} X_b$의 조건수가 커져 불안정성이 증가함

- 비가역 행렬의 문제
  - 행렬이 완전한 정칙(invertible, 가역)이 아닐 때 해를 구할 수 없음  
  - 피처가 많거나 중복되면 $X_b^\mathsf{T} X_b$가 비가역행렬이 됨(역행렬을 가질 수 없음)
  - 이러한 경우, 역행렬 계산이 불가능해 정규방적식의 해를 계산할 수 없음

### `np.linalg.lstsq`의 원리
SVD(특이값 분해) 기반으로 의사역행렬(pseudo-inverse) 를 계산함

- 정규방정식의 불안정한 역행렬 대신 안정적인 의사역행렬 $X_b^+$ 사용
  - 기존 정규방정식의 $\bigl(X_b^\mathsf{T} X_b\bigr)^{-1} X_b^\mathsf{T}$를 대체
  - $ \theta = X_b^+\,y $
  - 내부적으로 $X_b = UΣV^T$ 형태로 분해
  - $Σ^+$를 이용해 최소제곱 해를 계산함
    
- 수치 안정성이 높아 극단적인 피처 스케일 차이, 상관관계 과다 문제를 완화함

- 특징
  - 안정성: 특이하거나 거의 특이한 행렬에서도 작동
  - 효율성: 역행렬 직접 계산 없이 최소제곱 해 도출
  - 추가정보: 잔차(residual), 랭크(rank), 특이값(singular values) 반환

- 예시 코드
  ```python
  #  `lstsq`를 이용하여 `theta_lstsq` 구하기
  theta_lstsq, residuals, rank, singular_vals = np.linalg.lstsq(X_b, y, rcond=None)

  y_pred = X_b @ theta_lstsq
  mse = np.mean((y_pred - y)**2)

  print("파라미터 θ:\n", theta_lstsq.flatten())
  print(f"MSE: {mse:.2f}")

  # 파라미터 θ:
  # [ 3.25060105e-17 -4.02455846e-16 -1.38777878e-17 -4.60026982e-16
  #   1.00000000e+00  8.13802151e-16 -5.07948718e-16 -8.89045781e-17]
  # MSE: 0.00
  ```
  - `np.linalg.lstsq(X_b, y)`
    - 최소제곱 해 계산
    - 반환값: 
      - `theta_lstsq`: 최소제곱법으로 얻은 가중치 벡터
      - `residuals`: 잔차 제곱합
      - `rank`: 행렬의 랭크(독립된 열 개수)
      - `singular_vals`: 특이값
  - `rcond`
    - 작은 특이값 절단 기준
    - 일반적으로 None

### 최소제곱법 장점
- $X_b^\mathsf{T} X_b$의 역행렬을 직접 계산하지 않고 최소제곱 해 도출 가능
  - 수치 안정성 향상
- 특이(Singular, 거의 특이)한 상황에서도 해를 산출함
  - 즉, 특이 행렬 상황에서도 유사역행렬로 해 구할 수 있음
- 잔차(residuals), 랭크(rank), 특이값(singular values)을 함께 반환하여 모델 진단이 가능


## 🔹 선형대수를 이용한 해법 - SVD를 이용한 최소제곱 해 구하기
### SVD (특잇값 분해, Singular Value Decomposition)
임의의 행렬 $A\in\mathbb{R}^{m\times n}$를 아래와 같이 분해하는 기법

-> SVD는 행렬을 ‘회전-늘리기-회전’ 세 단계로 분리하는 방법임

- $A = U\,\Sigma\,V^\mathsf{T}$
- 구성
  | 구성요소     | 형태                 | 설명                              |
  | -------- | ------------------ | ------------------------------- |
  | $U$      | $m \times m$ 직교 행렬 | 열벡터들은 서로 수직(orthogonal)         |
  | $V$      | $n \times n$ 직교 행렬 | 열벡터들은 서로 수직                     |
  | $\Sigma$ | $m \times n$ 대각 행렬 | 대각 성분이 **특이값(singular values)** |
  | $r$      | rank(A)            | 유효한 특이값의 개수                     |
  - $U\in\mathbb{R}^{m\times r}$와 $V\in\mathbb{R}^{n\times r}$는 직교 행렬 (orthogonal)
  - $\Sigma\in\mathbb{R}^{r\times r}$는 특이값을 대각으로 가진 대각 행렬
  - $r=\mathrm{rank}(A)$

- 특잇값(Singular Value)
  - 행렬이 공간을 얼마나 ‘늘이거나 줄이는지’ 나타내는 크기(스케일)
  - 즉, 어떤 행렬 $A$를 적용했을 때, 벡터의 방향은 바뀌더라도 길이가 얼마나 변하는지를 수치로 보여주는 값
    - ex. 행렬이 어떤 축으로는 데이터를 3배 늘리고, 다른 축으로는 0.5배 줄인다면, 그 “3”과 “0.5”가 바로 특잇값

### 최소제곱 해 구하기
SVD를 이용하면 최소제곱 해를 닫힌 형식으로 구할 수 있음

-> pseudo inverse $\Sigma^+$를 정의하면  

$$
\Sigma^+_{ii} = \frac{1}{\Sigma_{ii}}\quad(\Sigma_{ii}>0)
$$


이고, 이때  


$$ x^* = A^+ b = V\,\Sigma^+\,U^\mathsf{T} b
$$


로 최소제곱 해 $\min_x\|Ax - b\|_2^2$를 얻게 됩니다.  