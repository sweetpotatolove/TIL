# 스트리밍 데이터 파이프라인 구축 (Kafka + Flink)
## 목표
- 이번 프로젝트의 목표는 (준)실시간 스트리밍 데이터를 효율적으로 처리하고 적절히 저장·분석하는 파이프라인을 구축하는 것입니다.
- 이를 위해 Kafka와 Flink를 이용하여 스트리밍 환경을 구성하고, 실시간으로 데이터를 수집·처리할 수 있는 토대를 마련합니다.
  - DB와 서버(벡엔드) 상호작용 가능하도록 데이터 적재
  - 프론트에서 입력을 받아 백엔드로 데이터 넘겨주면, 그 데이터 또한 DB에 저장할 수 있게(사용자 데이터)
  - 사용자 로그 등 장기저장소로 넘겨야 하는 데이터는 다시 Kafka로 넘겨주기
- 이전 주차에서 구축한 데이터베이스와 데이터를 적극 활용하여, 기존 RSS 피드에서 새로운 데이터를 지속적으로 가져와 분석 가능하도록 하는 과정을 포함합니다.
  - RSS는 실시간은 아니기 때문에, 여러 피드를 가져와서 준실시간 정도의 데이터 가져오기 작업을 하던지
  - 아니면 사용자 로그 등 대체 가능한 실시간 데이터 작업으로 보완
  - **이벤트가 들어왔을 때 바로 처리되는 구조!!**
- Kafka + Flink 구조를 이해하고 직접 구현해볼 수 있다.
- 이전 주차에서 획득한 데이터 처리 구조를 재활용하여 전체 파이프라인의 연속성을 확보한다.
- 실시간 데이터 처리 형태의 환경을 경험한다.
- 데이터를 분류, 키워드 추출, 임베딩 등 다양한 전처리/분석 기술을 적용할 수 있다.
  - flink
  - 본문, 제목에서 임베딩 추출하면 이를 기반으로 임베딩 연산을 통해 (유사도) 추천 기능 구현 가능
  - 지금 해놔야 나중에 사용하기 좋음
  - 키워드 추출 - LLM 사용해서 프롬프팅 잘 하기
  - 데이터 분류 
    - 신문사마다 카테고리가 다르기 때문에
    - 우리가 사용할 서비스에 맞게 카테고리 변형 필요
    - 이 또한 LLM을 통해 처리 가능


## 준비사항
### 사용 데이터
- PostgreSQL DB
- 이전까지 수집한 데이터

### 개발언어/프로그램
- `Python`: Kafka Producer, Flink Consumer(파이썬 API) 구현
  - Producer는 뭘 가져오냐?
    - RSS 피드에서 파싱한 데이터를 프로듀서에 넣음
- `Kafka`: 스트리밍 데이터 큐(브로커) 역할
  - 수집한 데이터를 토픽 단위로 분류해서 전송
  - 컨슈머가 구독하는 형태로 데이터 받게 함
- `Flink`: 실시간 스트리밍 데이터 처리 엔진
  - source -> transformation -> sink
  - flink -> DB 또는 flink -> kafka
- `PostgreSQL`: 처리된 데이터 저장(DB)
- `Requests`, `BeautifulSoup`: 웹 크롤링 및 파싱

※ 수집 데이터 외의 다른 데이터(사용자 로그 등)는 다른 클라이언트(producer, consumer) 만들어서 토픽 새로 만들어 사용자 데이터 쌓이는 공간을 따로 만들어 장기 저장소에 넣던지의 처리 가능


## 구현 방법
### 1) 데이터 수집 및 Kafka Producer 구현
- 실시간의 최신 데이터를 주기적으로 수집
- BeautifulSoup을 활용해 수집하는 데이터를 파싱
- 중복 데이터 처리를 위한 방식 활용
- **Kafka Producer로** 수집된 데이터를 JSON 형태로 **직렬화하여 특정 토픽(TOPIC)에 전송**
  - 직렬화는 Json, binary, str 등 뭘로 할지 생각해보기
  - 데이터 많으면 압축도 고려해보기

※ Kafka, flink 띄우기 위한 Docker-compose 파일: `관통프로젝트\03_DataEngineeringPJT\08_PJT\kafka&flink&docker 가이드\docker-compose.yml`

-> `docker compose up`


### 2) Flink를 통한 스트리밍 처리
- Flink Environment를 설정하고, Kafka Consumer를 사용하여 토픽의 실시간 메시지 수신
  - Kafka Consumer 역할을 하는 flink가 됨
- 수신된 JSON 데이터를 Pydantic 모델로 매핑하여 구조화
- 예시로 확인할 `transform_classify_category`, `transform_extract_keywords`, `transform_to_embedding` 등 본인의 태스크에 맞는 사용자 정의 처리 과정을 통해 내용 전처리 및 분석


### 3) DB 저장
- Flink의 sink 형태로 PostgreSQL에 데이터(ex. 제목, 작성자, 본문, 카테고리, 키워드, 임베딩 등) 저장
  - mapfunction 내에서 pg 관련 라이브러리 사용하여 처리하고 바로 적재 가능
  - sink에서 보내는게 flink 구조상 맞지만, sink 설정이 꽤 까다롭기 때문에 map이나 process funcion으로 객체 자체를 생성할 때 데이터를 처리, 바로 저장하는게 원할함
- 실시간으로 저장된 데이터를 기반으로 후속 분석 가능

- `03_DataEngineeringPJT\08_PJT\kafka&flink&docker 가이드\consumer\flink_kafka_consumer.py`

## 구현 방법
### 1) 데이터 수집

- 수집한 데이터 가공하고 취합하여 처리할 수 있는 Flink 코드 구성
- 크롤링을 통해 데이터 추가 확보
- Kafka Producer를 이용해 실시간 전송

### 2) 데이터 처리
- Flink로 스트리밍 데이터를 실시간 소비
- 구조화 및 전처리
- 카테고리 분류, 키워드 추출, 임베딩 처리

### 3) 데이터 처리 이후 저장
- 분류된 데이터를 PostgreSQL에 설계한 DB의 스키마에 맞게 저장
- 중복으로 삽입되는 형태가 발생하지 않도록 처리 (카프카에서 처리되긴 하지만, 처리를 최대한 줄이기 위함)

![alt text](image.png)

## 요구사항
### Kafka + Flink를 통한 실시간 데이터 파이프라인 구축 (기본 기능)
- 카프카 토픽 생성 및 실시간으로 Producer가 데이터 전송
- Flink Consumer가 메시지를 수신하여 DB에 저장
- DB에는 이전에 구상한 동일한 스키마(또는 확장)를 사용하여 데이터 적재

- 요청조건
  - 이전 과정의 **데이터 및 DB** 사용을 하되, **kafka와 flink**를 거쳐 적재되는 형태로 구성
  - 적재 과정에서 키워드 및 카테고리 분류 등의 데이터 특성에 맞는 전처리 과정 포함

### AI 활용을 위한 데이터 가공 (임베딩(Embedding) 및 심화 분석)
- 요청 조건
  - 키워드 추출, 임베딩 로직을 코드로 구현
  - 임베딩 결과를 DB에 json 문자열로 저장

- 결과
  - DB에 각 요소의 임베딩 필드가 추가로 저장되어야 함
  - 고급 분석(기사 간 유사도 분석 등) 가능

## 결과
- Kafka 토픽에 적재된 데이터
- Flink 환경에서의 Consumer 및 DB 적재 로그
- DB 테이블에 (준)실시간으로 쌓이는 데이터

## 산출물
- Gitlab에 올라온 de-project 코드 기반의 레포지토리 지속적 커밋  
- 이후 PJT도 해당 레포지토리에 이어 나가면서 개발  

## 정리
| 구현 기능 | 체크 포인트 |
|----------|-------------|
| 데이터 수집 | Kafka Producer/Consumer 구성<br>Flink 처리 |
| DB 적재 | DB 스키마 맞게 중복 처리<br>예외 처리 |
| AI 활용을 위한 데이터 가공 | 임베딩 기반 및 AI 활용 로직 처리를 위한 아이디어<br>유사도 분석 등을 위한 임베딩 적재 등 다양한 AI 활용 형태의 데이터 가공 |

